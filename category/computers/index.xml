<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Computers | Open Neuroscience</title><link>https://open-neuroscience.com/category/computers/</link><atom:link href="https://open-neuroscience.com/category/computers/index.xml" rel="self" type="application/rss+xml"/><description>Computers</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>CC BY SA 4.0</copyright><lastBuildDate>Thu, 16 Dec 2021 00:00:00 +0000</lastBuildDate><image><url>https://open-neuroscience.com/media/openneuroscience_logo_dark.svg</url><title>Computers</title><link>https://open-neuroscience.com/category/computers/</link></image><item><title>COGAIN Association (COmmunication by GAze INteraction)</title><link>https://open-neuroscience.com/post/cogain_association_communication_by_gaze_interaction_/</link><pubDate>Thu, 16 Dec 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/cogain_association_communication_by_gaze_interaction_/</guid><description>&lt;p>The COGAIN Association aims to promote research and development in the field of gaze-based interaction in computer-aided communication and control. Computer applications can be controlled by gazing at the computer screen. For people with disabilities, gaze controlled applications have been largely used for communication, entertainment, education, and many other purposes.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Bjarne Ersboell; John Paulin Hansen; Arantxa Villanueva; Carlos H. Morimoto; Paivi Majaranta; Marco Porta; Thies Pfeiffer&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.cogain.org/">https://www.cogain.org/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
anonymous&lt;/p>
&lt;hr></description></item><item><title>BrainGlobe</title><link>https://open-neuroscience.com/post/brainglobe/</link><pubDate>Thu, 25 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/brainglobe/</guid><description>&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/Ndsssf_gHns" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>BrainGlobe is a suite of Python-based computational neuroanatomy software tools. We provide software packages for the analysis and visualisation of neuroanatomical data, particularly from whole-brain microscopy. In addition, we provide tools for working with brain atlases, to simplify development of new tools and aid collaboration and cooperation by adopting common standards.&lt;/p>
&lt;p>Our tools include:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://open-neuroscience.com/post/brainglobe_atlas_api/" target="_blank" rel="noopener">BrainGlobe Atlas API&lt;/a> - A lightweight python module to interact with atlases for systems neuroscience&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://open-neuroscience.com/post/cellfinder/" target="_blank" rel="noopener">Cellfinder&lt;/a> - Automated 3D cell detection and registration of whole-brain microscopy images&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://open-neuroscience.com/post/brainrender/" target="_blank" rel="noopener">Brainrender&lt;/a> - A Python based software for visualization of neuroanatomical and morphological data.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Project Author(s): Adam Tyson; Federico Claudi; Luigi Petrucco&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/brainglobe">https://github.com/brainglobe&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://brainglobe.info/">https://brainglobe.info/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Adam Tyson&lt;/p>
&lt;hr></description></item><item><title>Computer-controlled dog treat dispenser</title><link>https://open-neuroscience.com/post/computer_controlled_dog_treat_dispenser/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/computer_controlled_dog_treat_dispenser/</guid><description>&lt;p>When performing canine operant conditioning studies, the delivery of the reward can be a limiting factor of the study. While there are a few commercially available options for automatically delivering rewards, they generally require manual input, such as using a remote control, in accordance with the experiment script. This means that human reaction times and transmission distances can cause interruptions to the flow of the experiment. The potential for development of non-supervised conditioning studies is limited by this same factor. To remedy this, we retrofitted an off-the-shelf treat dispenser with new electronics that allow it to be remotely controllable as well as act as an experiment computation, data storage, and networking center. We present a fully integrated dispenser driver board with a complementary Raspberry Pi. With rather simple modifications, the commercial treat dispenser can be modified into a computer-controlled dispenser for canine cognition experiments or for other forms of canine training or games.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Jeffrey R. Stevens; Walker Arce&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/unl-cchil/canine_treat_dispenser">https://github.com/unl-cchil/canine_treat_dispenser&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=veKvqE5ipu4">https://www.youtube.com/watch?v=veKvqE5ipu4&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Jeffrey R. Stevens&lt;/p>
&lt;hr></description></item><item><title>lab.js</title><link>https://open-neuroscience.com/post/lab_js/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/lab_js/</guid><description>&lt;p>lab.js makes online experimentation easy. It&amp;rsquo;s a free, open, online study builder and data collection framework for the behavioral, neuro- and cognitive sciences. Studies built with lab.js run in the laboratory as well as online, across a wide variety of data collection platforms, and provide excellent performance for all modes of stimulus presentation.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Felix Henninger; Yury Shevchenko; Ulf Mertens; Pascal Kieslich; Benjamin E. Hilbig&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://lab.js.org">https://lab.js.org&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Felix Henninger&lt;/p>
&lt;hr></description></item><item><title>openEyeTrack - An open source high-speed eyetracker</title><link>https://open-neuroscience.com/post/openeyetrack_an_open_source_high_speed_eyetracker/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/openeyetrack_an_open_source_high_speed_eyetracker/</guid><description>&lt;p>Vision is one of the primary senses, and tracking eye gaze can offer insight into the cues that affect decision-making behavior. Thus, to study decision-making and other cognitive processes, it is fundamentally necessary to track eye position accurately. However, commercial eye trackers are 1) often very expensive, and 2) incorporate proprietary software to detect the movement of the eye. Closed source solutions limit the researcher’s ability to be fully informed regarding the algorithms used to track the eye and to incorporate modifications tailored to their needs. Here, we present our software solution, openEyeTrack, a low-cost, high-speed, low-latency, open-source video-based eye tracker. Video-based eye trackers can perform nearly as well as classical scleral search coil methods and are suitable for most applications.&lt;/p>
&lt;p>openEyeTrack is a video-based eye-tracker that takes advantage of OpenCV, a low-cost, high-speed infrared camera and GigE-V APIs for Linux provided by Teledyne DALSA, the graphical user interface toolkit QT5 and cvui, the OpenCV based GUI. All of the software components are freely available. The only costs are from the hardware components such as the camera (Genie Nano M640 NIR, Teledyne DALSA, ~$450, ~730 frames per second) and infrared light source, an articulated arm to position the camera (Manfrotto: $130), a computer with one or more gigabit network interface cards, and a power over ethernet switch to power and receive data from the camera.&lt;/p>
&lt;p>By using the GigE-V Framework to capture the frames from the DALSA camera and the OpenCV simple blob detector, openEyeTrack can accurately estimate the position and area of the pupil. We include pupil size calculations because of its putative link to arousal levels and emotions of the subject.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Chandramouli Chandrasekaran; Jorge Paolo Casas&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/chand-lab/openEyeTrack">https://github.com/chand-lab/openEyeTrack&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Chandramouli Chandrasekaran&lt;/p>
&lt;hr></description></item><item><title>Pirecorder: Controlled and automated image and video recording with the raspberry pi</title><link>https://open-neuroscience.com/post/pirecorder_controlled_and_automated_image_and_video_recording_with_the_raspberry_pi/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pirecorder_controlled_and_automated_image_and_video_recording_with_the_raspberry_pi/</guid><description>&lt;p>pirecorder is a Python package, built on the picamera and OpenCV libraries, that provides a flexible solution for the collection of consistent image and video data with the raspberry pi. It was developed to overcome the need for a complete solution to help researchers, especially those with limited coding skills, to easily set up and configure their raspberry pi to run large numbers of controlled and automated image and video recordings using optimal settings.&lt;/p>
&lt;p>Pirecorder consists of a number of interconnected modules to facilitate key aspects of media recording: 1) setting-up and configuring the camera, 2) recording images, videos, time-lapses, and standardised video sequences with automatic file-naming, 3) easy scheduling of future recordings, and 4) converting of recorded media with resize, timestamp, and monitoring options. All functionalities are designed to make it very straightforward, even for users with limited coding experience, to configure, initiate, schedule, and convert recordings. In particular, pirecorder offers interactive streaming functionalities to facilitate users in positioning and focusing the camera, selecting the desired white-balance and other image parameters using trackbars, and set the ideal camera shutter speed. Furthermore, pirecorder comes with a dedicated documentation website with detailed information and tutorials (jollejolles.github.io/pirecorder) as well as a set of annotated Jupyter Notebooks to help users integrate the raspberry pi and pirecorder in their work.&lt;/p>
&lt;p>Key Features:&lt;/p>
&lt;ul>
&lt;li>Controlled recording using custom, easy-to-edit configuration files&lt;/li>
&lt;li>Record single images and videos, timelapses, and sequences of videos&lt;/li>
&lt;li>Configure camera settings interactively via a live camera stream&lt;/li>
&lt;li>Dynamically draw the region of interest for your recordings&lt;/li>
&lt;li>Automatic naming of files and folders with relevant and custom labels&lt;/li>
&lt;li>Easy scheduling and automating recordings in the future&lt;/li>
&lt;li>Direct control of all modules via simple terminal commands&lt;/li>
&lt;li>Convert (folders of) images and videos with resize, monitor, and label options&lt;/li>
&lt;li>Dedicated documentation website with detailed guides and tutorials: &lt;a href="https://jollejolles.github.io/pirecorder/">https://jollejolles.github.io/pirecorder/&lt;/a>&lt;/li>
&lt;li>Jupyter notebook tutorial files&lt;/li>
&lt;/ul>
&lt;p>A paper accompanying the package is published in the Journal of Open Source Software: Jolles, J.W. (2020). pirecorder: controlled and automated image and video recording with the raspberry pi. J. Open Source Softw. 5, 2584. doi: 10.21105/joss.02584&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Jolle W. Jolles&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/JolleJolles/pirecorder">https://github.com/JolleJolles/pirecorder&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=pcVHpijd6wc">https://www.youtube.com/watch?v=pcVHpijd6wc&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Jolle W. Jolles&lt;/p>
&lt;hr></description></item><item><title>FastTrack</title><link>https://open-neuroscience.com/post/fasttrack/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/fasttrack/</guid><description>&lt;p>FastTrack is an open-source cross-platform tracking software. Easy to install and easy to use, it can track a large variety of systems from active particles to animals, with a known or unknown number of objects. It can process movies from any quality on low-end to high-end computers.&lt;/p>
&lt;p>Two main features are implemented in the software:
- A fast and automatic tracking algorithm that can detect and track objects, conserving the objects' identities across the video recording.
- A manual tool to review the tracking where errors can be corrected rapidly and easily to achieve 100% accuracy with a minimum of efforts.&lt;/p>
&lt;p>FastTrack do not require coding abilities to be used. A developer documentation is available for users who want to embed FastTrack tracking algorithm directly inside their projects.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Benjamin Gallois&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/FastTrackOrg/FastTrack">https://github.com/FastTrackOrg/FastTrack&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="http://www.fasttrack.sh/UserManual/docs/assets/example_vid.webm">http://www.fasttrack.sh/UserManual/docs/assets/example_vid.webm&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Benjamin Gallois&lt;/p>
&lt;hr></description></item><item><title>NeuroFedora</title><link>https://open-neuroscience.com/post/neurofedora/</link><pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/neurofedora/</guid><description>&lt;p>NeuroFedora is an initiative to provide a ready to use Fedora Linux based Free/Open source software platform for neuroscience. We believe that similar to Free software, science should be free for all to use, share, modify, and study. The use of Free software also aids reproducibility, data sharing, and collaboration in the research community. By making the tools used in the scientific process easier to use, NeuroFedora aims to take a step to enable this ideal.&lt;/p>
&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/2xAK1tY0qro" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>NeuroFedora volunteers @ the Fedora project&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://neuro.fedoraproject.org">https://neuro.fedoraproject.org&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Ankur Sinha (NeuroFedora SIG member)&lt;/p>
&lt;hr></description></item><item><title>BrainGlobe atlas API</title><link>https://open-neuroscience.com/post/brainglobe_atlas_api/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/brainglobe_atlas_api/</guid><description>&lt;p>Many excellent brain atlases exist for different species. Some of them have an API (application programming interface) to allow users to interact with the data programmatically (e.g. the excellent Allen Mouse Brain Atlas), but many do not, and there is no consistent way to process data from multiple sources.&lt;/p>
&lt;p>The brainglobe atlas API (BG-AtlasAPI) deals with this problem by providing a common interface for programmers to download and process atlas data from multiple sources.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Adam Tyson; Federico Claudi; Luigi Petrucco&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/brainglobe/bg-atlasapi">https://github.com/brainglobe/bg-atlasapi&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Adam Tyson&lt;/p>
&lt;hr></description></item><item><title>Neuroimaging Informatics Tools and Resources Collaboratory (NITRC)</title><link>https://open-neuroscience.com/post/neuroimaging_informatics_tools_and_resources_collaboratory_nitrc/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/neuroimaging_informatics_tools_and_resources_collaboratory_nitrc/</guid><description>&lt;p>NeuroImaging Tools &amp;amp; Resources Collaboratory is an award-winning free web-based resource that offers comprehensive information on an ever expanding scope of neuroinformatics software and data. Since debuting in 2007, NITRC has helped the neuroscience community make further discoveries using software and data produced from research that used to end up lost or disregarded.&lt;/p>
&lt;p>NITRC also provides free access to data and enables pay-per-use cloud-based access to unlimited computing power, enabling worldwide scientific collaboration with minimal startup and cost. NITRC’s scientific focus includes: MR, PET/SPECT, CT, EEG/MEG, optical imaging, clinical neuroimaging, computational neuroscience, and imaging genomics software tools, data, and computational resources.&lt;/p>
&lt;p>With NITRC and its components—the Resources Registry (NITRC-R), Image Repository (NITRC-IR), and Computational Environment (NITRC-CE)—a researcher can obtain pilot or proof-of-concept data to validate a hypothesis for just a few dollars.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>NITRC Development Team&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://www.nitrc.org">http://www.nitrc.org&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
David Kennedy&lt;/p>
&lt;hr></description></item><item><title>OpenDrop Digital Microfluidics Platform</title><link>https://open-neuroscience.com/post/opendrop_digital_microfluidics_platform/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/opendrop_digital_microfluidics_platform/</guid><description>&lt;p>OpenDrop a modular, open source digital microfludics platform for research purposes. The device uses recent electro-wetting technology to control small droplets of liquids. Potential applications are lab on a chip devices for automating processes of digital biology.&lt;/p>
&lt;p>The OpenDrop V4 is modular electrowetting controller. The driver board is equipped with a connector that can host a circuit board cartridge with a 14×8 electrode array and 4 reservoirs. The liquids stay on a thin, hydrophobic foil laminated to the circuit board . The device is powered from USB though an included USB-C cable. All the voltage level are generated on the device and can be set with the built in soft menu from 150-300 Volts, DC or AC.&lt;/p>
&lt;p>OpenDrop Cartridges
The modular concepts of the OpenDrop V4 allows different configurations of cartridges: A gold coated electrode array board that can be coated with any dielectric layer and hydrophobic coating to make cartridges for topless digital microfluidic applications using readily available materials.The OpenDrop V4 Cartridge is a close-cell cartridge capable of “move, mix, split and reservoir dispensing”. The 4×8 electrode array and 4 reservoirs are laminated with a 15um ETFE foil, hydrophobic coating and ITO top cover.&lt;/p>
&lt;p>Programming
The OpenDrop V4 can be operated standalone and droplets can be moved through the built in joystick. A control software to program sequences of patterns from a computer is available as a free download. The board is also compatible with Adafruit Feather M0 controller boards and can be reprogrammed through the free Arduino IDE for custom specific applications. A sample code with the instruction to activate electrodes can be found on the OpenDrop GitHub.&lt;/p>
&lt;p>Features:&lt;/p>
&lt;ul>
&lt;li>Modular Cartridge System
- Connector to connect electrode board with up to 128 channels
- Gold coated 14×8 electrodes array, 2.75 mm x 2.75 mm in size, 4mil gaps&lt;/li>
&lt;li>Reservoirs – the new electrode array features 4 CT-type reservoirs&lt;/li>
&lt;li>AC and DC voltage generated on the device form USB power. True AC voltage driving capability (up to 300VAC).&lt;/li>
&lt;li>32bit AVR SAMD21G18 microprocessor with plenty of memory and power
- Electronic settings for voltage level, frequency and AC/DC selection
- Electronic reading of actual voltage level
- One connector for communication and powering (USB-C)
- Optical isolation of the high-voltage electronics trough opto-couplers and PhotoMOS
- New polyphonic audio amplifier and speaker (it’s a synth!)
- Cartridge presence detection
- Feedback amplifier
- Super flat OLED Display
- Nice joystick and 2 buttons, 3 LEDs
- Reset button
- All files open source, designed on KiCAD&lt;/li>
&lt;/ul>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>MSc Urs Gaudenz&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://www.gaudi.ch/OpenDrop/">http://www.gaudi.ch/OpenDrop/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=TY97QfWY6J4">https://www.youtube.com/watch?v=TY97QfWY6J4&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Anonymous&lt;/p>
&lt;hr></description></item><item><title>Ethoscopes</title><link>https://open-neuroscience.com/post/ethoscopes/</link><pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/ethoscopes/</guid><description>&lt;p>Ethoscopes are machines for high-throughput analysis of behavior in Drosophila and other animals.&lt;/p>
&lt;p>Ethoscopes provide a software and hardware solution that is reproducible and easily scalable.&lt;/p>
&lt;p>They perform, in real-time, tracking and profiling of behavior by using a supervised machine learning algorithm, are able to deliver behaviorally triggered stimuli to flies in a feedback-loop mode, and are highly customizable and open source.&lt;/p>
&lt;p>Ethoscopes can be built easily by using 3D printing technology and rely on Raspberry Pi microcomputers and Arduino boards to provide affordable and flexible hardware.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Quentin Geissmann; Luis Garcia; Giorgio Gilestro&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://lab.gilest.ro/ethoscope">http://lab.gilest.ro/ethoscope&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=5oWGBUMJON8&amp;amp;feature=emb_title">https://www.youtube.com/watch?v=5oWGBUMJON8&amp;amp;feature=emb_title&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Giorgio Gilestro&lt;/p>
&lt;hr></description></item><item><title>Red Pitaya</title><link>https://open-neuroscience.com/post/red-pitaya/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/red-pitaya/</guid><description>&lt;p>&lt;a href="https://www.redpitaya.com/?skip_intro=yes" target="_blank" rel="noopener">Red Pitaya&lt;/a> is an computer+FPGA that has digital input and outputs and really fast analog inputs and outputs. It allows connection over ethernet and programming of custom routines. The system is powerful enough to have application in mostly all branches of neuroscience labs: oscilloscopes, signal generators and even a candidate for recording systems.&lt;/p></description></item></channel></rss>
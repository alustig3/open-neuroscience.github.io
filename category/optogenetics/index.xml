<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Optogenetics | Open Neuroscience</title><link>https://open-neuroscience.com/category/optogenetics/</link><atom:link href="https://open-neuroscience.com/category/optogenetics/index.xml" rel="self" type="application/rss+xml"/><description>Optogenetics</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>CC BY SA 4.0</copyright><lastBuildDate>Tue, 06 Apr 2021 00:00:00 +0000</lastBuildDate><image><url>https://open-neuroscience.com/media/openneuroscience_logo_dark.svg</url><title>Optogenetics</title><link>https://open-neuroscience.com/category/optogenetics/</link></image><item><title>Building a Simple and Versatile Illumination System for Optogenetic Experiments</title><link>https://open-neuroscience.com/post/building_a_simple_and_versatile_illumination_system_for_optogenetic_experiments/</link><pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/building_a_simple_and_versatile_illumination_system_for_optogenetic_experiments/</guid><description>&lt;p>Controlling biological processes using light has increased the accuracy and speed with which researchers can manipulate many biological processes. Optical control allows for an unprecedented ability to dissect function and holds the potential for enabling novel genetic therapies. However, optogenetic experiments require adequate light sources with spatial, temporal, or intensity control, often a bottleneck for researchers. Here we detail how to build a low-cost and versatile LED illumination system that is easily customizable for different available optogenetic tools. This system is configurable for manual or computer control with adjustable LED intensity. We provide an illustrated step-by-step guide for building the circuit, making it computer-controlled, and constructing the LEDs. To facilitate the assembly of this device, we also discuss some basic soldering techniques and explain the circuitry used to control the LEDs. Using our open-source user interface, users can automate precise timing and pulsing of light on a personal computer (PC) or an inexpensive tablet. This automation makes the system useful for experiments that use LEDs to control genes, signaling pathways, and other cellular activities that span large time scales. For this protocol, no prior expertise in electronics is required to build all the parts needed or to use the illumination system to perform optogenetic experiments.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Phillip Kyriakakis1, Lourdes Fernandez de Cossio2, Patrick Wade Howard1, Sivleng Kouv1, Marianne Catanho1, Vincent J. Hu3, Robert Kyriakakis1, Molly E. Allen1, Yunhan Ma4, Marcelo Aguilar-Rivera1, Todd P. Coleman1&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/BreakLiquid/LED-Control-User-Interfaces">https://github.com/BreakLiquid/LED-Control-User-Interfaces&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Phillip Kyriakakis&lt;/p>
&lt;hr></description></item><item><title>Addgene's AAV Data Hub</title><link>https://open-neuroscience.com/post/addgene_s_aav_data_hub/</link><pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/addgene_s_aav_data_hub/</guid><description>&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/U_P2CD746pI" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;br>
&lt;p>AAV are versatile tools used by neuroscientists for expression and manipulation of neurons. Many scientists have benefited from the high-quality, ready-to-use AAV prep service from Addgene, a nonprofit plasmid repository. However, it can be challenging to determine which AAV tool and techniques are best to use for an experiment. Scientists also may have questions about how much virus to inject or which serotype or promoter should be used to target the desired neuron or brain region. To help scientists answer these questions, Addgene launched an open platform called the AAV Data Hub (&lt;a href="https://datahub.addgene.org/aav/">https://datahub.addgene.org/aav/&lt;/a>) which allows researchers to easily share practical experimental details with the scientific community (AAV used, in vivo model used, injection site, injection volumes, etc.). The goal of this platform is to help scientists find the best AAV tool for their experiments by reviewing combined data from a broad range of research labs. The AAV Data Hub launched in late 2019 and over 100 experiments have since been contributed to this project. The dataset includes details and images from experiments conducted in six different species and several different expression sites.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Addgene&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://datahub.addgene.org/aav/">https://datahub.addgene.org/aav/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/ZPKdr1RdtGI" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;hr>
&lt;p>This post was automatically generated by
Angela Abitua&lt;/p>
&lt;hr></description></item><item><title>LED Zappelin'</title><link>https://open-neuroscience.com/post/led_zappelin_/</link><pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/led_zappelin_/</guid><description>&lt;p>Two-photon (2P) microscopy is a cornerstone technique in neuroscience research. However, combining 2P imaging with spectrally arbitrary light stimulation can be challenging due to crosstalk between stimulation light and fluorescence detection. To overcome this limitation, we present a simple and low-cost electronic solution based on an ESP32 microcontroller and a TLC5947 LED driver to rapidly time-interleave stimulation and detection epochs during scans. Implemented for less than $100, our design can independently drive up to 24 arbitrary spectrum LEDs to meet user requirements. We demonstrate the utility of our stimulator for colour vision experiments on the in vivo tetrachromatic zebrafish retina and for optogenetic circuit mapping in Drosophila.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Maxime Zimmermann; Andre Maia Chagas; Philipp Bartel; Sinzi Pop, Lucia Pierto Godino; Tom Baden&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/BadenLab/LED-Zappelin">https://github.com/BadenLab/LED-Zappelin&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Maxime Zimmermann&lt;/p>
&lt;hr></description></item><item><title>Efficient training of mice on the 5-choice serial reaction time task in an automated rodent training system</title><link>https://open-neuroscience.com/post/efficient_training_of_mice_on_the_5_choice_serial_reaction_time_task_in_an_automated_rodent_training_system/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/efficient_training_of_mice_on_the_5_choice_serial_reaction_time_task_in_an_automated_rodent_training_system/</guid><description>&lt;p>Experiments aiming to understand sensory-motor systems, cognition and behavior necessitate training animals to perform complex tasks. Traditional training protocols require lab personnel to move the animals between home cages and training chambers, to start and end training sessions, and in some cases, to hand-control each training trial. Human labor not only limits the amount of training per day, but also introduces several sources of variability and may increase animal stress. Here we present an automated training system for the 5-choice serial reaction time task (5CSRTT), a classic rodent task often used to test sensory detection, sustained attention and impulsivity. We found that full automation without human intervention allowed rapid, cost-efficient training, and decreased stress as measured by corticosterone levels. Training breaks introduced only a transient drop in performance, and mice readily generalized across training systems when transferred from automated to manual protocols. We further validated our automated training system with wireless optogenetics and pharmacology experiments, expanding the breadth of experimental needs our system may fulfill. Our automated 5CSRTT system can serve as a prototype for fully automated behavioral training, with methods and principles transferrable to a range of rodent tasks.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Eszter Birtalan; Anita Bánhidi; Joshua I Sanders; Diána Balázsfi; Balázs Hangya;&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/hangyabalazs/ATS">https://github.com/hangyabalazs/ATS&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Balazs Hangya&lt;/p>
&lt;hr></description></item><item><title>OPETH: Open Source Solution for Real-Time Peri-Event Time Histogram Based on Open Ephys</title><link>https://open-neuroscience.com/post/opeth_open_source_solution_for_real_time_peri_event_time_histogram_based_on_open_ephys/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/opeth_open_source_solution_for_real_time_peri_event_time_histogram_based_on_open_ephys/</guid><description>&lt;p>Single cell electrophysiology remains one of the most widely used approaches of systems neuroscience. Decisions made by the experimenter during electrophysiology recording largely determine recording quality, duration of the project and value of the collected data. Therefore, online feedback aiding these decisions can lower monetary and time investment, and substantially speed up projects as well as allow novel studies otherwise not possible due to prohibitively low throughput. Real-time feedback is especially important in studies that involve optogenetic cell type identification by enabling a systematic search for neurons of interest. However, such tools are scarce and limited to costly commercial systems with high degree of specialization, which hitherto prevented wide-ranging benefits for the community. To address this, we present an open-source tool that enables online feedback during electrophysiology experiments and provides a Python interface for the widely used Open Ephys open source data acquisition system. Specifically, our software allows flexible online visualization of spike alignment to external events, called the online peri-event time histogram (OPETH). These external events, conveyed by digital logic signals, may indicate photostimulation time stamps for in vivo optogenetic cell type identification or the times of behaviorally relevant events during in vivo behavioral neurophysiology experiments. Therefore, OPETH allows real-time identification of genetically defined neuron types or behaviorally responsive populations. By allowing &amp;ldquo;hunting&amp;rdquo; for neurons of interest, OPETH significantly reduces experiment time and thus increases the efficiency of experiments that combine in vivo electrophysiology with behavior or optogenetic tagging of neurons.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>András Széll; Sergio Martínez-Bellver; Panna Hegedüs; Balázs Hangya&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/hangyabalazs/opeth">https://github.com/hangyabalazs/opeth&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Balazs Hangya&lt;/p>
&lt;hr></description></item><item><title>SignalBuddy</title><link>https://open-neuroscience.com/post/signalbuddy/</link><pubDate>Tue, 23 Feb 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/signalbuddy/</guid><description>&lt;p>SignalBuddy is an easy-to-make, easy-to-use signal generator for scientific applications. Making friends is hard, but making SignalBuddy is easy. All you need is an Arduino Uno! SignalBuddy replaces more complicated and (much) more expensive signal generators in laboratory settings where one millisecond resolution is sufficient.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Richard Warren&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://hackaday.io/project/167649-signalbuddy">https://hackaday.io/project/167649-signalbuddy&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://github.com/richard-warren/SignalBuddy/raw/master/images/SignalBuddy3D.gif">https://github.com/richard-warren/SignalBuddy/raw/master/images/SignalBuddy3D.gif&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Richard Warren&lt;/p>
&lt;hr></description></item><item><title>Open Source Tools for Temporally Controlled Rodent Behavior Suitable for Electrophysiology and Optogenetic Manipulations</title><link>https://open-neuroscience.com/post/open_source_tools_for_temporally_controlled_rodent_behavior_suitable_for_electrophysiology_and_optogenetic_manipulations/</link><pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open_source_tools_for_temporally_controlled_rodent_behavior_suitable_for_electrophysiology_and_optogenetic_manipulations/</guid><description>&lt;p>Understanding how the brain controls behavior requires observing and manipulating neural activity in awake behaving animals. Neuronal firing is timed at millisecond precision. Therefore, to decipher temporal coding, it is necessary to monitor and control animal behavior at the same level of temporal accuracy. However, it is technically challenging to deliver sensory stimuli and reinforcers as well as to read the behavioral responses they elicit with millisecond precision. Presently available commercial systems often excel in specific aspects of behavior control, but they do not provide a customizable environment allowing flexible experimental design while maintaining high standards for temporal control necessary for interpreting neuronal activity. Moreover, delay measurements of stimulus and reinforcement delivery are largely unavailable. We combined microcontroller-based behavior control with a sound delivery system for playing complex acoustic stimuli, fast solenoid valves for precisely timed reinforcement delivery and a custom-built sound attenuated chamber using high-end industrial insulation materials. Together this setup provides a physical environment to train head-fixed animals, enables calibrated sound stimuli and precisely timed fluid and air puff presentation as reinforcers. We provide latency measurements for stimulus and reinforcement delivery and an algorithm to perform such measurements on other behavior control systems. Combined with electrophysiology and optogenetic manipulations, the millisecond timing accuracy will help interpret temporally precise neural signals and behavioral changes. Additionally, since software and hardware provided here can be readily customized to achieve a large variety of paradigms, these solutions enable an unusually flexible design of rodent behavioral experiments.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Nicola Solari; Katalin Sviatkó; Tamás Laszlovszky; Panna Hegedüs; Balázs Hangya&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/hangyabalazs/Rodent_behavior_setup">https://github.com/hangyabalazs/Rodent_behavior_setup&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Balazs Hangya&lt;/p>
&lt;hr></description></item><item><title>PiVR</title><link>https://open-neuroscience.com/post/pivr/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pivr/</guid><description>&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/uG898FL421U" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;br>
PiVR is a system that allows experimenters to immerse small animals into virtual realities. The system tracks the position of the animal and presents light stimulation according to predefined rules, thus creating a virtual landscape in which the animal can behave. By using optogenetics, we have used PiVR to present fruit fly larvae with virtual olfactory realities, adult fruit flies with a virtual gustatory reality and zebrafish larvae with a virtual light gradient.
&lt;p>PiVR operates at high temporal resolution (70Hz) with low latencies (&amp;lt;30 milliseconds) while being affordable (&amp;lt;US$500) and easy to build (&amp;lt;6 hours). Through extensive documentation (&lt;a href="http://www.PiVR.org">www.PiVR.org&lt;/a>), this tool was designed to be accessible to a wide public, from high school students to professional researchers studying systems neuroscience in academia.&lt;/p>
&lt;p>The project is open source (BSD-3) and the documented code written in the freely available programming language Python. We hope that PiVR will be adapted by advanced users for their particular needs, for example to create closed-loop experiments involving other sensory modalities (e.g., sound/vibration) through the use of PWM controllable devices. We envision PiVR to be used as the central module when creating virtual realities for a variety of sensory modalities. This ‘PiVR module’ takes care of detecting the animal and presenting the appropriate PWM signal that is then picked up by the PWM controllable device installed by the user, for example to produce a sound whenever an animal enters a pre-defined region.&lt;/p>
&lt;p>In short, PiVR is a powerful and affordable experimental platform allowing experimenters to create a wide array of virtual reality experiments. Our hope is that PiVR will be adapted by several labs to democratize closed-loop experiments and, by standardizing image quality and the animal detection algorithm, increase reproducibility.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>David Tadres; Matthieu Louis&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://www.PiVR.org">http://www.PiVR.org&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/w5tIG6B6FWo" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;hr>
&lt;p>This post was automatically generated by
David Tadres&lt;/p>
&lt;hr></description></item><item><title>Pulse Pal</title><link>https://open-neuroscience.com/post/pulse-pal/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pulse-pal/</guid><description>&lt;blockquote>
&lt;p>Pulse Pal is an open and inexpensive (~$210) alternative to pulse generators used in neurophysiology research, and is most often used to create precisely timed light trains in optogenetics assays. Pulse Pal generates four channels of configurable square pulse trains ranging in voltage from +10 to -10V using a bipolar DAC. Two digital trigger channels can be used to start and stop playback. APIs are available in C++, Python and MATLAB, and the hardware designs and firmware are fully open source.&lt;/p>
&lt;/blockquote>
&lt;p>Be sure to check the &lt;a href="http://journal.frontiersin.org/article/10.3389/fneng.2014.00043/abstract" target="_blank" rel="noopener">paper&lt;/a> about it and their &lt;a href="https://sites.google.com/site/pulsepalwiki/home" target="_blank" rel="noopener">wiki page.&lt;/a>&lt;/p></description></item></channel></rss>
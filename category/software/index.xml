<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Software | Open Neuroscience</title><link>https://open-neuroscience.com/category/software/</link><atom:link href="https://open-neuroscience.com/category/software/index.xml" rel="self" type="application/rss+xml"/><description>Software</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>CC BY SA 4.0</copyright><lastBuildDate>Tue, 06 Apr 2021 00:00:00 +0000</lastBuildDate><image><url>https://open-neuroscience.com/media/openneuroscience_logo_dark.svg</url><title>Software</title><link>https://open-neuroscience.com/category/software/</link></image><item><title>Building a Simple and Versatile Illumination System for Optogenetic Experiments</title><link>https://open-neuroscience.com/post/building_a_simple_and_versatile_illumination_system_for_optogenetic_experiments/</link><pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/building_a_simple_and_versatile_illumination_system_for_optogenetic_experiments/</guid><description>&lt;p>Controlling biological processes using light has increased the accuracy and speed with which researchers can manipulate many biological processes. Optical control allows for an unprecedented ability to dissect function and holds the potential for enabling novel genetic therapies. However, optogenetic experiments require adequate light sources with spatial, temporal, or intensity control, often a bottleneck for researchers. Here we detail how to build a low-cost and versatile LED illumination system that is easily customizable for different available optogenetic tools. This system is configurable for manual or computer control with adjustable LED intensity. We provide an illustrated step-by-step guide for building the circuit, making it computer-controlled, and constructing the LEDs. To facilitate the assembly of this device, we also discuss some basic soldering techniques and explain the circuitry used to control the LEDs. Using our open-source user interface, users can automate precise timing and pulsing of light on a personal computer (PC) or an inexpensive tablet. This automation makes the system useful for experiments that use LEDs to control genes, signaling pathways, and other cellular activities that span large time scales. For this protocol, no prior expertise in electronics is required to build all the parts needed or to use the illumination system to perform optogenetic experiments.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Phillip Kyriakakis1, Lourdes Fernandez de Cossio2, Patrick Wade Howard1, Sivleng Kouv1, Marianne Catanho1, Vincent J. Hu3, Robert Kyriakakis1, Molly E. Allen1, Yunhan Ma4, Marcelo Aguilar-Rivera1, Todd P. Coleman1&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/BreakLiquid/LED-Control-User-Interfaces">https://github.com/BreakLiquid/LED-Control-User-Interfaces&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Phillip Kyriakakis&lt;/p>
&lt;hr></description></item><item><title>CellExplorer - Framework for analyzing single cells</title><link>https://open-neuroscience.com/post/cellexplorer_framework_for_analyzing_single_cells/</link><pubDate>Sun, 28 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/cellexplorer_framework_for_analyzing_single_cells/</guid><description>&lt;p>The large diversity of cell-types of the brain, provides the means by which circuits perform complex operations. Understanding such diversity is one of the key challenges of modern neuroscience. Neurons have many unique electrophysiological and behavioral features from which parallel cell-type classification can be inferred.&lt;/p>
&lt;p>To address this, we built CellExplorer, a framework for analyzing and characterizing single cells recorded using extracellular electrodes. It can be separated into three components: a standardized yet flexible data structure, a single yet extensive processing module, and a powerful graphical interface. Through the processing module, a high dimensional representation is built from electrophysiological and functional features including the spike waveform, spiking statistics, monosynaptic connections, and behavioral spiking dynamics. The user-friendly interactive graphical interface allows for classification and exploration of those features, through a rich set of built-in plots, interaction modes, cell grouping, and filters. Powerful figures can be created for publications. Opto-tagged cells and public access to reference data have been incorporated to help you characterize your data better. The framework is built entirely in MATLAB making it fast and intuitive to implement and incorporate CellExplorer into your pipelines and analysis scripts. You can expand it with your metrics, plots, and opto-tagged data.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Peter C. Petersen, Joshua H. Siegle, Nicholas A. Steinmetz, Sara Mahallati, György Buzsáki&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://cellexplorer.org/">https://cellexplorer.org/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=GR1glNhcGIY">https://www.youtube.com/watch?v=GR1glNhcGIY&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Peter C. Petersen&lt;/p>
&lt;hr></description></item><item><title>BrainGlobe</title><link>https://open-neuroscience.com/post/brainglobe/</link><pubDate>Thu, 25 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/brainglobe/</guid><description>&lt;p>BrainGlobe is a suite of Python-based computational neuroanatomy software tools. We provide software packages for the analysis and visualisation of neuroanatomical data, particularly from whole-brain microscopy. In addition, we provide tools for working with brain atlases, to simplify development of new tools and aid collaboration and cooperation by adopting common standards.&lt;/p>
&lt;p>Our tools include:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://open-neuroscience.com/post/brainglobe_atlas_api/" target="_blank" rel="noopener">BrainGlobe Atlas API&lt;/a> - A lightweight python module to interact with atlases for systems neuroscience&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://open-neuroscience.com/post/cellfinder/" target="_blank" rel="noopener">Cellfinder&lt;/a> - Automated 3D cell detection and registration of whole-brain microscopy images&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://open-neuroscience.com/post/brainrender/" target="_blank" rel="noopener">Brainrender&lt;/a> - A Python based software for visualization of neuroanatomical and morphological data.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Project Author(s): Adam Tyson; Federico Claudi; Luigi Petrucco&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/brainglobe">https://github.com/brainglobe&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Adam Tyson&lt;/p>
&lt;hr></description></item><item><title>Kilosort</title><link>https://open-neuroscience.com/post/kilosort/</link><pubDate>Sun, 21 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/kilosort/</guid><description>&lt;p>Kilosort is a software package for identifying neurons and their spikes in extracellular electrophysiology, a process known as &amp;ldquo;spike sorting&amp;rdquo;. Kilosort has been primarily developed and tested on the Neuropixels 1.0 probes, but is now used on data acquired with a wide variety of ephys methods, from Utah arrays to tetrodes and to the latest Neuropixels 2.0 probes.&lt;/p>
&lt;p>Spike sorting consists of several largely-independent steps: data preprocessing, drift correction, spike clustering, template matching, and results postprocessing. There have been several versions of Kilosort, improving on various aspects of these steps, and we are currently on version v3. In many cases, and especially for Neuropixels probes, the automated output of Kilosort3 requires minimal manual curation. The main change from v2.5 is a completely new and much more sophisticated clustering algorithm. To learn about Kilosort2.5, the primary reference is the Neuropixels 2.0 paper. Kilosort2.5 improves on Kilosort2 primarily in the type of drift correction we use. Where Kilosort2 modified templates as a function of time/drift (a drift tracking approach), Kilosort2.5 corrects the raw data directly via a sub-pixel registration process (a drift correction approach).&lt;/p>
&lt;p>All versions of Kilosort so far have been developed and released in Matlab. However, we are advancing towards Python releases of the codebase, including of older version like Kilosort 2 and 2.5 (available here: &lt;a href="https://github.com/MouseLand/pykilosort/tree/master/pykilosort)">https://github.com/MouseLand/pykilosort/tree/master/pykilosort)&lt;/a>.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Marius Pachitariu&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/MouseLand/Kilosort">https://github.com/MouseLand/Kilosort&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://youtu.be/clq50N7V_wA">https://youtu.be/clq50N7V_wA&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Marius Pachitariu&lt;/p>
&lt;hr></description></item><item><title>An Open-source Anthropomorphic Robot Hand System: HRI Hand</title><link>https://open-neuroscience.com/post/an_open_source_anthropomorphic_robot_hand_system_hri_hand/</link><pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/an_open_source_anthropomorphic_robot_hand_system_hri_hand/</guid><description>&lt;p>We present an open-source anthropomorphic robot hand system called HRI hand. Our robot hand system was developed with a focus on the end-effector role of the collaborative robot manipulator. HRI hand is a research platform that can be built at a lower price (approximately $500, using only 3D printing) than commercial end-effectors. Moreover, it was designed as a two four-bar linkage for the under-actuated mechanism and provides pre-shaping motion similar to the human hand prior to touching an object. A URDF, python node, and rviz package is also provided to support the Robot Operating System (ROS). All hardware CAD design files and software source codes have been released and can be easily assembled and modified. The system proposed in this paper is developed with a five-finger structure, but each finger is modularized, so it can be developed with end-effectors of various shapes depending on the shape of the palm.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Hyeonjun Park; Donghan Kim&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/MrLacuqer/HRI-hand-firmware.git">https://github.com/MrLacuqer/HRI-hand-firmware.git&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://youtu.be/c5Ry3tl9FVw">https://youtu.be/c5Ry3tl9FVw&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Andre M Chagas&lt;/p>
&lt;hr></description></item><item><title>Suite2P</title><link>https://open-neuroscience.com/post/suite2p/</link><pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/suite2p/</guid><description>&lt;p>Suite2P is a very modular imaging processing pipeline written in Python which allows you to perform registration of raw data movies, automatic cell detection, extraction of calcium traces and infers spike times. It is a very fast and accurate tool and can work on standard workstations. It also includes a visualization graphical user interface (GUI) that facilitates analysis and manual curation of the cell detection algorithm.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Carsen Stringer and Marius Pachitariu&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://mouseland.github.io/suite2p/_build/html/index.html">https://mouseland.github.io/suite2p/_build/html/index.html&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>BigPint Bioconductor package that makes BIG (RNA seq) data pint sized</title><link>https://open-neuroscience.com/post/bigpint_bioconductor_package_that_makes_big_rna_seq_data_pint_sized/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/bigpint_bioconductor_package_that_makes_big_rna_seq_data_pint_sized/</guid><description>&lt;p>The BigPint package can help examine any large multivariate dataset. However, we note that the example datasets and example code in this package consider RNA-sequencing datasets. If you are using this software for RNA-sequencing data, then it can help you confirm that the variability between your treatment groups is larger than that between your replicates and determine how various normalization techniques in popular RNA-sequencing analysis packages (such as edgeR, DESeq2, and limma) affect your dataset. Moreover, you can easily superimpose lists of differentially expressed genes (DEGs) onto your dataset to check that they show the expected patterns (large variability between treatment groups and small variability between replicates).&lt;/p>
&lt;ol>
&lt;li>
&lt;p>BigPint software website: &lt;a href="https://lindsayrutter.github.io/bigPint/">https://lindsayrutter.github.io/bigPint/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Article explaining the BigPint methodology: &lt;a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2968-1">https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2968-1&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Research article showcasing the BigPint software:
&lt;a href="https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-5767-1">https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-5767-1&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Article explaining the BigPint software: &lt;a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007912">https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007912&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Lindsay Rutter; Dianne Cook&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/lindsayrutter/bigPint">https://github.com/lindsayrutter/bigPint&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Anonymous&lt;/p>
&lt;hr></description></item><item><title>Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC)</title><link>https://open-neuroscience.com/post/collaborative_informatics_and_neuroimaging_suite_toolkit_for_anonymous_computation_coinstac_/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/collaborative_informatics_and_neuroimaging_suite_toolkit_for_anonymous_computation_coinstac_/</guid><description>&lt;p>COINSTAC provides a platform to analyze data stored locally across multiple organizations without the need for pooling the data at any point during the analysis. It is intended to be an ultimate one-stop shop by which researchers can build any statistical or machine learning model collaboratively in a decentralized fashion. This framework implements a message passing infrastructure that will allow large scale analysis of decentralized data with results on par with those that would have been obtained if the data were in one place.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Sergey M. Plis; Anand D. Sarwate; Dylan Wood; Christopher Dieringer; Drew Landis; Cory Reed; Sandeep R. Panta; Jessica A. Turner; Jody M. Shoemaker; Kim W. Carter; Paul Thompson; Kent Hutchison; Vince D. Calhoun&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/trendscenter/coinstac">https://github.com/trendscenter/coinstac&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Kelly Rootes-Murdy&lt;/p>
&lt;hr></description></item><item><title>Computer-controlled dog treat dispenser</title><link>https://open-neuroscience.com/post/computer_controlled_dog_treat_dispenser/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/computer_controlled_dog_treat_dispenser/</guid><description>&lt;p>When performing canine operant conditioning studies, the delivery of the reward can be a limiting factor of the study. While there are a few commercially available options for automatically delivering rewards, they generally require manual input, such as using a remote control, in accordance with the experiment script. This means that human reaction times and transmission distances can cause interruptions to the flow of the experiment. The potential for development of non-supervised conditioning studies is limited by this same factor. To remedy this, we retrofitted an off-the-shelf treat dispenser with new electronics that allow it to be remotely controllable as well as act as an experiment computation, data storage, and networking center. We present a fully integrated dispenser driver board with a complementary Raspberry Pi. With rather simple modifications, the commercial treat dispenser can be modified into a computer-controlled dispenser for canine cognition experiments or for other forms of canine training or games.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Jeffrey R. Stevens; Walker Arce&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/unl-cchil/canine_treat_dispenser">https://github.com/unl-cchil/canine_treat_dispenser&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=veKvqE5ipu4">https://www.youtube.com/watch?v=veKvqE5ipu4&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Jeffrey R. Stevens&lt;/p>
&lt;hr></description></item><item><title>EASI-FISH</title><link>https://open-neuroscience.com/post/easi_fish/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/easi_fish/</guid><description>&lt;p>This workflow is used to analyze large-scale, multi-round, high-resolution image data acquired using EASI-FISH (Expansion-Assisted Iterative Fluorescence In Situ Hybridization). It takes advantage of the n5 filesystem to allow for rapid and parallel data reading and writing. It performs automated image stitching, distributed and highly accurate multi-round image registration, 3D cell segmentation, and distributed spot detection. We also envision this workflow being adapted for analysis of other image-based spatial transcriptomic data.&lt;/p>
&lt;p>Check the pre-print here &lt;a href="https://www.biorxiv.org/content/10.1101/2021.03.08.434304v1">https://www.biorxiv.org/content/10.1101/2021.03.08.434304v1&lt;/a>&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Yuhan Wang; Konrad Rokicki; Avatar Cristian Goina&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/multiFISH/EASI-FISH">https://github.com/multiFISH/EASI-FISH&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Efficient training of mice on the 5-choice serial reaction time task in an automated rodent training system</title><link>https://open-neuroscience.com/post/efficient_training_of_mice_on_the_5_choice_serial_reaction_time_task_in_an_automated_rodent_training_system/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/efficient_training_of_mice_on_the_5_choice_serial_reaction_time_task_in_an_automated_rodent_training_system/</guid><description>&lt;p>Experiments aiming to understand sensory-motor systems, cognition and behavior necessitate training animals to perform complex tasks. Traditional training protocols require lab personnel to move the animals between home cages and training chambers, to start and end training sessions, and in some cases, to hand-control each training trial. Human labor not only limits the amount of training per day, but also introduces several sources of variability and may increase animal stress. Here we present an automated training system for the 5-choice serial reaction time task (5CSRTT), a classic rodent task often used to test sensory detection, sustained attention and impulsivity. We found that full automation without human intervention allowed rapid, cost-efficient training, and decreased stress as measured by corticosterone levels. Training breaks introduced only a transient drop in performance, and mice readily generalized across training systems when transferred from automated to manual protocols. We further validated our automated training system with wireless optogenetics and pharmacology experiments, expanding the breadth of experimental needs our system may fulfill. Our automated 5CSRTT system can serve as a prototype for fully automated behavioral training, with methods and principles transferrable to a range of rodent tasks.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Eszter Birtalan; Anita Bánhidi; Joshua I Sanders; Diána Balázsfi; Balázs Hangya;&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/hangyabalazs/ATS">https://github.com/hangyabalazs/ATS&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Balazs Hangya&lt;/p>
&lt;hr></description></item><item><title>Heuristic Spike Sorting Tuner (HSST)</title><link>https://open-neuroscience.com/post/heuristic_spike_sorting_tuner_hsst_/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/heuristic_spike_sorting_tuner_hsst_/</guid><description>&lt;p>Extracellular microelectrodes frequently record neural activity from more than one neuron in the vicinity of the electrode. The process of labeling each recorded spike waveform with the identity of its source neuron is called spike sorting and is often approached from an abstracted statistical perspective. However, these approaches do not consider neurophysiological realities and may ignore important features that could improve the accuracy of these methods. Further, standard algorithms typically require selection of at least one free parameter, which can have significant effects on the quality of the output. We describe a Heuristic Spike Sorting Tuner (HSST) that determines the optimal choice of the free parameters for a given spike sorting algorithm based on the neurophysiological qualification of unit isolation and signal discrimination. A set of heuristic metrics are used to score the output of a spike sorting algorithm over a range of free parameters resulting in optimal sorting quality. We demonstrate that these metrics can be used to tune parameters in several spike sorting algorithms. The HSST algorithm shows robustness to variations in signal to noise ratio, number and relative size of units per channel. Moreover, the HSST algorithm is computationally efficient, operates unsupervised, and is parallelizable for batch processing.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>David A. Bjanes; Lee B. Fisher; Robert A. Gaunt; Douglas J. Weber&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/davidbjanes/hsst">https://github.com/davidbjanes/hsst&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
David Bjanes&lt;/p>
&lt;hr></description></item><item><title>lab.js</title><link>https://open-neuroscience.com/post/lab_js/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/lab_js/</guid><description>&lt;p>lab.js makes online experimentation easy. It&amp;rsquo;s a free, open, online study builder and data collection framework for the behavioral, neuro- and cognitive sciences. Studies built with lab.js run in the laboratory as well as online, across a wide variety of data collection platforms, and provide excellent performance for all modes of stimulus presentation.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Felix Henninger; Yury Shevchenko; Ulf Mertens; Pascal Kieslich; Benjamin E. Hilbig&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://lab.js.org">https://lab.js.org&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Felix Henninger&lt;/p>
&lt;hr></description></item><item><title>MNE-Python</title><link>https://open-neuroscience.com/post/mne_python/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/mne_python/</guid><description>&lt;p>MNE-Python is an open-source Python module for neuroscience data analysis. It implements many neuroscience-specific algorithms and statistical tools; has rich visualization capabilities that are both interactive and fully scriptable (for reproducibility); and integrates easily with general-purpose libraries like SciPy, Scikit-learn, and TensorFlow.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Eric Larson; Alexandre Gramfort; Denis A. Engemann; Jaakko Leppakangas; Christian Brodbeck; Mainak Jas; Teon Brooks; Jona Sassenhagen; Martin Luessi; Jean-Remi King; Roman Goj; Daniel McCloy; Mark Wronkiewicz; Marijn van Vliet; Guillaume Favelier; Clemens Brunner; Chris Holdgraf; Joan Massich; Yousra Bekhti; Richard Höchenberger; Alan Leggitt; Andrew Dykstra; Romain Trachel; Lorenzo De Santis; Asish Panda; Stefan Appelhoff; Mikołaj Magnuski; Martin Billinger; Britta Westner; Dan G. Wakeman; Daniel Strohmeier; Robert Luke; Hari Bharadwaj; Tal Linzen; Alexandre Barachant; Emily Ruzich; Christopher J. Bailey; Clément Moutard; Luke Bloy; Fede Raimondo; Jussi Nurminen; Jair Montoya; Marmaduke Woodman; Ingoo Lee; Nick Foti; Cathy Nangini; José C. García Alanis; Roan LaPlante; Ross Maddox; Christoph Dinh; Olaf Hauk; Adam Li; Guillaume Dumas; Paul Pasler; Stefan Repplinger; Thomas Hartmann; Alexander Rudiuk; Brad Buran; Mathurin Massias; Matti Hämäläinen; Praveen Sripad; Christopher Mullins; Félix Raimundo; Phillip Alday; Simon Kornblith; Yaroslav Halchenko; Yu-Han Luo; Johannes Kasper; Keith Doelling; Mads Jensen; Tanay Gahlot; Adonay Nunes; Dirk Gütlin; kjs; Alejandro Weinstein; Camilo Lamus; Cristóbal Moënne-Loccoz; Natalie Klein; Alex Rockhill; Antti Rantala; Burkhard Maess; Erkka Heinila; Henrich Kolkhorst; Jeff Hanna; Jon Houck; Saket Choudhary; Christian O&amp;rsquo;Reilly; Fu-Te Wong; Hubert Banville; Ivana Kojcic; Jesper Duemose Nielsen; Kaisu Lankinen; Kambiz Tabavi; Kostiantyn Maksymenko; Louis Thibault; Nathalie Gayraud; Nick Ward; Antoine Gauthier; Basile Pinsard; Emily Stephen; Erik Hornberger; Evgenii Kalenkovich; Fahimeh Mamashli; Hafeza Anevar; Johann Benerradi; Larry Eisenman; Lorenz Esch; Nicolas Barascud; Nicolas Legrand; Samuel Deslauriers-Gauthier; Simon Kern; Victor Férat; Alexander Kovrig; Annalisa Pascarella; Dominik Krzemiński; Ezequiel Mikulan; Jean-Baptiste Schiratti; Jen Evans; Kyle Mathewson; Laura Gwilliams; Lenny Varghese; Lx37; Martin Schulz; Matt Boggess; Mohamed Sherif; Nataliia Kozhemiako; Niklas Wilming; Oleh Kozynets; Pierre Ablin; Quentin Bertrand; Rodrigo Hübner; Sara Sommariva; Sheraz Khan; Sophie Herbst; Thomas Jochmann; Tod Flak; Tom Dupré la Tour; Tristan Stenner; akshay0724; sviter; Abram Hindle; Achilleas Koutsou; Aniket Pradhan; Anne-Sophie Dubarry; Anton Nikolas Waniek; Ariel Rokem; Austin Hurst; Bruno Nicenboim; Carlos de la Torre; Christian Clauss; Chun-Hui Li; Claire Braboszcz; David Haslacher; David Sabbagh; Demetres Kostas; Desislava Petkova; Dmitrii Altukhov; Dominik Welke; Eberhard Eich; Eduard Ort; Elizabeth DuPre; Ellen Lau; Emanuele Olivetti; Evan Hathaway; Geoff Brookshire; Hermann Sonntag; Hongjiang Ye; Jakub Kaczmarzyk; Jasper J.F. van den Bosch; Jeff Stout; Jeroen Van Der Donckt; Johan van der Meer; Johannes Niediek; Joshua J Bear; Juergen Dammers; Katarina Slama; Katrin Leinweber; Laetitia Grabot; Lau Møller Andersen; Leonardo S. Barbosa; Liberty Hamilton; Lorenzo Alfine; Lukáš Hejtmánek; Manfred Kitzbichler; Manoj Kumar; Manu Sutela; Marcin Koculak; Marian Dovgialo; Martin van Harmelen; MartinBaBer; Matt Tucker; Matteo Visconti di Oleggio Castello; Michael Krause; Milan Rybář; Mohammad Daneshzand; Nicole Proulx; Nikolas Chalas; Padma Sundaram; Paul Roujansky; Pedro Silva; Peter J. Molfese; Quanliang Li; Rahul Nadkarni; Ramiro Gatti; Ramonapariciog Apariciogarcia; Richard Koehler; Robert Oostenveld; Robert Seymour; Robin Tibor Schirrmeister; Sagun Pai; Sam Perry; Sebastian Major; Sebastián Castaño; Sergey Antopolskiy; Simeon Wong; Simon-Shlomo Poil; Sourav Singh; Stanislas Chambon; Steve Matindi; Steven Bethard; Steven Bierer; Steven M. Gutstein; Svea Marie Meyer; Theodore Papadopoulo; Thomas Donoghue; Thomas Radman; Tommy Clausner; ZHANG Zhi; apadee; buildqa; chapochn; enricovara; mshader&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://mne.tools">https://mne.tools&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Daniel McCloy&lt;/p>
&lt;hr></description></item><item><title>napari</title><link>https://open-neuroscience.com/post/napari/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/napari/</guid><description>&lt;p>Napari is a fast, interactive, multi-dimensional image viewer for Python. It’s designed for browsing, annotating, and analyzing large multi-dimensional images. It’s built on top of Qt (for the GUI), vispy (for performant GPU-based rendering), and the scientific Python stack (numpy, scipy).&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>napari team&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://napari.org/">https://napari.org/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>openEyeTrack - An open source high-speed eyetracker</title><link>https://open-neuroscience.com/post/openeyetrack_an_open_source_high_speed_eyetracker/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/openeyetrack_an_open_source_high_speed_eyetracker/</guid><description>&lt;p>Vision is one of the primary senses, and tracking eye gaze can offer insight into the cues that affect decision-making behavior. Thus, to study decision-making and other cognitive processes, it is fundamentally necessary to track eye position accurately. However, commercial eye trackers are 1) often very expensive, and 2) incorporate proprietary software to detect the movement of the eye. Closed source solutions limit the researcher’s ability to be fully informed regarding the algorithms used to track the eye and to incorporate modifications tailored to their needs. Here, we present our software solution, openEyeTrack, a low-cost, high-speed, low-latency, open-source video-based eye tracker. Video-based eye trackers can perform nearly as well as classical scleral search coil methods and are suitable for most applications.&lt;/p>
&lt;p>openEyeTrack is a video-based eye-tracker that takes advantage of OpenCV, a low-cost, high-speed infrared camera and GigE-V APIs for Linux provided by Teledyne DALSA, the graphical user interface toolkit QT5 and cvui, the OpenCV based GUI. All of the software components are freely available. The only costs are from the hardware components such as the camera (Genie Nano M640 NIR, Teledyne DALSA, ~$450, ~730 frames per second) and infrared light source, an articulated arm to position the camera (Manfrotto: $130), a computer with one or more gigabit network interface cards, and a power over ethernet switch to power and receive data from the camera.&lt;/p>
&lt;p>By using the GigE-V Framework to capture the frames from the DALSA camera and the OpenCV simple blob detector, openEyeTrack can accurately estimate the position and area of the pupil. We include pupil size calculations because of its putative link to arousal levels and emotions of the subject.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Chandramouli Chandrasekaran; Jorge Paolo Casas&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/chand-lab/openEyeTrack">https://github.com/chand-lab/openEyeTrack&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Chandramouli Chandrasekaran&lt;/p>
&lt;hr></description></item><item><title>OPETH: Open Source Solution for Real-Time Peri-Event Time Histogram Based on Open Ephys</title><link>https://open-neuroscience.com/post/opeth_open_source_solution_for_real_time_peri_event_time_histogram_based_on_open_ephys/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/opeth_open_source_solution_for_real_time_peri_event_time_histogram_based_on_open_ephys/</guid><description>&lt;p>Single cell electrophysiology remains one of the most widely used approaches of systems neuroscience. Decisions made by the experimenter during electrophysiology recording largely determine recording quality, duration of the project and value of the collected data. Therefore, online feedback aiding these decisions can lower monetary and time investment, and substantially speed up projects as well as allow novel studies otherwise not possible due to prohibitively low throughput. Real-time feedback is especially important in studies that involve optogenetic cell type identification by enabling a systematic search for neurons of interest. However, such tools are scarce and limited to costly commercial systems with high degree of specialization, which hitherto prevented wide-ranging benefits for the community. To address this, we present an open-source tool that enables online feedback during electrophysiology experiments and provides a Python interface for the widely used Open Ephys open source data acquisition system. Specifically, our software allows flexible online visualization of spike alignment to external events, called the online peri-event time histogram (OPETH). These external events, conveyed by digital logic signals, may indicate photostimulation time stamps for in vivo optogenetic cell type identification or the times of behaviorally relevant events during in vivo behavioral neurophysiology experiments. Therefore, OPETH allows real-time identification of genetically defined neuron types or behaviorally responsive populations. By allowing &amp;ldquo;hunting&amp;rdquo; for neurons of interest, OPETH significantly reduces experiment time and thus increases the efficiency of experiments that combine in vivo electrophysiology with behavior or optogenetic tagging of neurons.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>András Széll; Sergio Martínez-Bellver; Panna Hegedüs; Balázs Hangya&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/hangyabalazs/opeth">https://github.com/hangyabalazs/opeth&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Balazs Hangya&lt;/p>
&lt;hr></description></item><item><title>Pirecorder: Controlled and automated image and video recording with the raspberry pi</title><link>https://open-neuroscience.com/post/pirecorder_controlled_and_automated_image_and_video_recording_with_the_raspberry_pi/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pirecorder_controlled_and_automated_image_and_video_recording_with_the_raspberry_pi/</guid><description>&lt;p>pirecorder is a Python package, built on the picamera and OpenCV libraries, that provides a flexible solution for the collection of consistent image and video data with the raspberry pi. It was developed to overcome the need for a complete solution to help researchers, especially those with limited coding skills, to easily set up and configure their raspberry pi to run large numbers of controlled and automated image and video recordings using optimal settings.&lt;/p>
&lt;p>Pirecorder consists of a number of interconnected modules to facilitate key aspects of media recording: 1) setting-up and configuring the camera, 2) recording images, videos, time-lapses, and standardised video sequences with automatic file-naming, 3) easy scheduling of future recordings, and 4) converting of recorded media with resize, timestamp, and monitoring options. All functionalities are designed to make it very straightforward, even for users with limited coding experience, to configure, initiate, schedule, and convert recordings. In particular, pirecorder offers interactive streaming functionalities to facilitate users in positioning and focusing the camera, selecting the desired white-balance and other image parameters using trackbars, and set the ideal camera shutter speed. Furthermore, pirecorder comes with a dedicated documentation website with detailed information and tutorials (jollejolles.github.io/pirecorder) as well as a set of annotated Jupyter Notebooks to help users integrate the raspberry pi and pirecorder in their work.&lt;/p>
&lt;p>Key Features:&lt;/p>
&lt;ul>
&lt;li>Controlled recording using custom, easy-to-edit configuration files&lt;/li>
&lt;li>Record single images and videos, timelapses, and sequences of videos&lt;/li>
&lt;li>Configure camera settings interactively via a live camera stream&lt;/li>
&lt;li>Dynamically draw the region of interest for your recordings&lt;/li>
&lt;li>Automatic naming of files and folders with relevant and custom labels&lt;/li>
&lt;li>Easy scheduling and automating recordings in the future&lt;/li>
&lt;li>Direct control of all modules via simple terminal commands&lt;/li>
&lt;li>Convert (folders of) images and videos with resize, monitor, and label options&lt;/li>
&lt;li>Dedicated documentation website with detailed guides and tutorials: &lt;a href="https://jollejolles.github.io/pirecorder/">https://jollejolles.github.io/pirecorder/&lt;/a>&lt;/li>
&lt;li>Jupyter notebook tutorial files&lt;/li>
&lt;/ul>
&lt;p>A paper accompanying the package is published in the Journal of Open Source Software: Jolles, J.W. (2020). pirecorder: controlled and automated image and video recording with the raspberry pi. J. Open Source Softw. 5, 2584. doi: 10.21105/joss.02584&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Jolle W. Jolles&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/JolleJolles/pirecorder">https://github.com/JolleJolles/pirecorder&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=pcVHpijd6wc">https://www.youtube.com/watch?v=pcVHpijd6wc&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Jolle W. Jolles&lt;/p>
&lt;hr></description></item><item><title>PocketPCR-Pocket size USB powered PCR Thermo Cycler</title><link>https://open-neuroscience.com/post/pocketpcr_pocket_size_usb_powered_pcr_thermo_cycler/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pocketpcr_pocket_size_usb_powered_pcr_thermo_cycler/</guid><description>&lt;p>The PocketPCR is a so called thermocycler used to activate biological reactions. To do so the device raises and lowers the temperature of the liquid in the small tubes. The polymerase chain reaction (PCR) is a method widely used in molecular biology to make copies of a specific DNA segment. Applications of the technique include DNA cloning for sequencing, analysis of genetic fingerprints, amplification of ancient DNA and gene cloning.&lt;/p>
&lt;p>Simpler, smaller and more affordable. This ultra portable and compact thermocycler was designed with the goal to bring the cost down to affordable price for anyone who wants to do some Do-It-Your-Self biology with a DNA starter kit in his kitchen. The PocketPCR can be run from a simple USB power adapter. The device can be operated stand alone and all parameters can be set without the need of a computer or smartphone.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Urs Gaudenz; Yanwu Guo&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://gaudi.ch/PocketPCR/">http://gaudi.ch/PocketPCR/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://youtu.be/0tXwAAMCetI">https://youtu.be/0tXwAAMCetI&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Urs Gaudenz&lt;/p>
&lt;hr></description></item><item><title>CLIJ2</title><link>https://open-neuroscience.com/post/clij2/</link><pubDate>Tue, 23 Feb 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/clij2/</guid><description>&lt;p>CLIJ2 is a GPU-accelerated image processing library for ImageJ/Fiji, Icy, Matlab and Java based on OpenCL. It comes with hundreds of operations for filtering, binarizing, labeling, measuring in images, projections, transformations and mathematical operations for images.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Robert Haase&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://clij.github.io/">https://clij.github.io/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>morphologica</title><link>https://open-neuroscience.com/post/morphologica/</link><pubDate>Tue, 23 Feb 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/morphologica/</guid><description>&lt;p>morphologica is a header-only C++ library which provides simulation support facilities for simulations of dynamical systems.&lt;/p>
&lt;p>It helps with:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Configuration: morphologica allows you to easily set up a simulation parameter configuration system, using the JSON reading and writing abilities of morph::Config.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Saving data from your simulation. morphologica provides a set of easy-to-use convenience wrappers (morph::HdfData) around the HDF5 C API. By saving data in a standard format, it is easy to access simulation data in python, MATLAB or Octave for analysis and graphing.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Visualizing your model while it runs. A modern OpenGL visualization scheme called morph::Visual provides the ability to visualise hex &amp;amp; Cartesian grids, surfaces, scatter plots and quiver plots with minimal processing overhead.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Seb James; Stuart Wilson&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/ABRG-Models/morphologica">https://github.com/ABRG-Models/morphologica&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://youtu.be/UejRHe-6LoM">https://youtu.be/UejRHe-6LoM&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Seb James&lt;/p>
&lt;hr></description></item><item><title>SignalBuddy</title><link>https://open-neuroscience.com/post/signalbuddy/</link><pubDate>Tue, 23 Feb 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/signalbuddy/</guid><description>&lt;p>SignalBuddy is an easy-to-make, easy-to-use signal generator for scientific applications. Making friends is hard, but making SignalBuddy is easy. All you need is an Arduino Uno! SignalBuddy replaces more complicated and (much) more expensive signal generators in laboratory settings where one millisecond resolution is sufficient.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Richard Warren&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://hackaday.io/project/167649-signalbuddy">https://hackaday.io/project/167649-signalbuddy&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://github.com/richard-warren/SignalBuddy/raw/master/images/SignalBuddy3D.gif">https://github.com/richard-warren/SignalBuddy/raw/master/images/SignalBuddy3D.gif&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Richard Warren&lt;/p>
&lt;hr></description></item><item><title>jsPsych</title><link>https://open-neuroscience.com/post/jspsych/</link><pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/jspsych/</guid><description>&lt;p>jsPsych is a JavaScript library for running behavioral experiments in a web browser. The library provides a flexible framework for building a wide range of laboratory-like experiments that can be run online. To use jsPsych, you provide a description of the experiment in the form of a timeline. jsPsych handles things like determining which trial to run next, storing data, and randomization. jsPsych uses plugins to define what to do at each point on the timeline. Plugins are ready-made templates for simple experimental tasks like displaying instructions or displaying a stimulus and collecting a keyboard response. Plugins are very flexible to support a wide variety of experiments. It is easy to create your own plugin if you have experience with JavaScript programming.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Josh de Leeuw&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.jspsych.org">https://www.jspsych.org&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
anonymous&lt;/p>
&lt;hr></description></item><item><title>NetPyNE</title><link>https://open-neuroscience.com/post/netpyne/</link><pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/netpyne/</guid><description>&lt;p>NetPyNE (Networks using Python and NEURON) is a Python package to facilitate the development, simulation, parallelization, analysis, and optimization of biophysical neuronal networks using the NEURON simulator.&lt;/p>
&lt;p>For more details, installation instructions, documentation, tutorials, forums, videos and more, please visit: &lt;a href="http://www.netpyne.org">www.netpyne.org&lt;/a>&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Salvador Dura-Bernal&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/Neurosim-lab/netpyne">https://github.com/Neurosim-lab/netpyne&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://youtu.be/TV7ipuDHBd0">https://youtu.be/TV7ipuDHBd0&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Joe Graham&lt;/p>
&lt;hr></description></item><item><title>Psygo</title><link>https://open-neuroscience.com/post/psygo/</link><pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/psygo/</guid><description>&lt;p>The easiest way to get started creating online behavioural experiments! psygo is a CLI tool that streamlines the development of custom jsPsych plugins, allowing you to create custom behavioural experiments that can be run online. psygo also helps you test your experiment locally. All the hard work is done for you, from setting up a project, to preparing it for administration.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Henry Burgess&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/henry-burgess/psygo">https://github.com/henry-burgess/psygo&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Henry Burgess&lt;/p>
&lt;hr></description></item><item><title>Greedy</title><link>https://open-neuroscience.com/post/greedy/</link><pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/greedy/</guid><description>&lt;p>Greedy is a tool for fast medical image registration. It is a fast CPU-based deformable image registration tool that can be used in applications where many images have to be registered in parallel. It can perform affine and rigid image (2D and 3D data) registration. You can also supply masks for restricting registration to specific regions.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Paul Yushkevich&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://greedy.readthedocs.io/en/latest/index.html">https://greedy.readthedocs.io/en/latest/index.html&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>ERPLAB</title><link>https://open-neuroscience.com/post/erplab/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/erplab/</guid><description>&lt;p>ERPLAB Toolbox is a free, open-source Matlab package for analyzing ERP data. It is tightly integrated with EEGLAB Toolbox, extending EEGLAB’s capabilities to provide robust, industrial-strength tools for ERP processing, visualization, and analysis. A graphical user interface makes it easy for beginners to learn, and Matlab scripting provides enormous power for intermediate and advanced users.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Andrew X Stewart; Steve Luck&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/lucklab/erplab">https://github.com/lucklab/erplab&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Andrew X Stewart&lt;/p>
&lt;hr></description></item><item><title>TGAC Browser</title><link>https://open-neuroscience.com/post/tgac_browser/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/tgac_browser/</guid><description>&lt;p>The TGAC Browser is a Genomic Browser with novel rendering and annotation capabilities designed to overcome some shortcomings in available approaches. It was developed to visualize genome annotations from Ensembl Database Schema.&lt;/p>
&lt;p>The TGAC Browser provides the following features:&lt;/p>
&lt;p>Responsiveness: Client-side rendering and caching, based on JSON fragments generated by server logic, helps decrease the server load and improves user experience.&lt;/p>
&lt;p>User-friendly browser Interaction: Live data searching, track modification, and drag and drop selection; actions that are seamlessly powered by modern web browsers.&lt;/p>
&lt;p>Analysis Integration: The ability to carry out heavyweight analysis tasks, using tools such as BLAST, via a dedicated extensible daemon service.&lt;/p>
&lt;p>User Annotation: Users can edit annotations which can be persisted on the server, reloaded, and shared at a later date.&lt;/p>
&lt;p>Off-the-shelf Installation: The only prerequisites are a web application container, such as Jetty or Tomcat, and a standard Ensembl database to host sequence features.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Anil S. Thanki; Xingdong Bian; Robert P. Davey&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://browser.tgac.ac.uk/">http://browser.tgac.ac.uk/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>neurolib</title><link>https://open-neuroscience.com/post/neurolib/</link><pubDate>Tue, 10 Nov 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/neurolib/</guid><description>&lt;p>Easy whole-brain modeling for computational neuroscientists 👩🏿‍🔬💻🧠&lt;/p>
&lt;p>In its essence, neurolib is a computational framework for simulating coupled neural mass models written in Python. It helps you to easily load structural brain scan data to construct brain networks where each node is a neural mass representing a single brain area. This network model can be used to simulate whole-brain dynamics.&lt;/p>
&lt;p>neurolib provides a simulation and optimization framework which allows you to easily implement your own neural mass model, simulate fMRI BOLD activity, analyse the results and fit your model to empirical data.&lt;/p>
&lt;p>With neurolib, our goal is to create a hackable framework for coders and focus on the simulation and optimization machinery. In this sense, neurolib is primarily a modern research tool and our main goal is to provide an accessible research framework. However, it is built with people in mind who are new to the field and just want to get going. We have made it as easy as possible to setup a simulation or to implement your own model and run your experiments.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Caglar Cakan; Nikola Jajcay&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/neurolib-dev/neurolib">https://github.com/neurolib-dev/neurolib&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Caglar Cakan&lt;/p>
&lt;hr></description></item><item><title>YAPiC</title><link>https://open-neuroscience.com/post/yapic/</link><pubDate>Tue, 10 Nov 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/yapic/</guid><description>&lt;p>With YAPiC you can make your own customized filter (also called model or classifier) to enhance a certain structure of your choice with a simple Python based command line interface, installable with pip. We have used YAPiC so far for analyzing various microscopy image data. Our experiments are mainly related to neurobiology, cell biology, histopathology and drug discovery (high content screening). However, YAPiC is a very generally applicable tool and can be applied to very different domains. It could be used for detecting e.g. forest regions in satellite images, clouds in landscape photographs or fried eggs in food photography.
Pixel classification in YAPiC is based on deep learning wit fully convolutional neural networks. Development of YAPiC started in 2015, when Ronneberger et al. presented a U-shaped fully convolutional neural network that was capable of solving highly challenging pixel classification tasks in bio images, such as tumor classification in histological slides or cell segmentation in brightfield DIC images.&lt;/p>
&lt;p>YAPiC was designed to make this new kind of AI powered pixel classification simply applicable, i.e feasible to use for a PhD student in his/her imaging project.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Christoph Moehl; Manuel Schoelling&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://yapic.github.io/yapic/">https://yapic.github.io/yapic/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Christoph Moehl&lt;/p>
&lt;hr></description></item><item><title>Computational Cognitive Neuroscience 4th Ed</title><link>https://open-neuroscience.com/post/computational_cognitive_neuroscience_4th_ed/</link><pubDate>Wed, 04 Nov 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/computational_cognitive_neuroscience_4th_ed/</guid><description>&lt;p>This is the 4th edition of the online, freely available textbook, providing a complete, self-contained introduction to the field of Computational Cognitive Neuroscience, where computer models of the brain are used to understand a wide range of cognitive functions, including perception, attention, motor control, learning, memory, language, and executive function.&lt;/p>
&lt;p>The first part of this textbook develops a coherent set of computational and neural principles that capture the behavior of networks of interconnected neurons, and the second part applies these principles to understand the above-listed cognitive functions.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Randall C. O&amp;rsquo;Reilly; Yuko Munakata; Michael J. Frank; Thomas E. Hazy&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://compcogneuro.org">https://compcogneuro.org&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Randall C. O&amp;rsquo;Reilly&lt;/p>
&lt;hr></description></item><item><title>Emergent Neural Network Simulation Software</title><link>https://open-neuroscience.com/post/emergent_neural_network_simulation_software/</link><pubDate>Wed, 04 Nov 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/emergent_neural_network_simulation_software/</guid><description>&lt;p>Neural network simulation software written in Go and Python, for developing biologically-based but also computationally functional neural models. Features an interactive 3D interface for visualizing networks and data, and has many implemented models of a wide range of cognitive phenomena.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Randall C. O&amp;rsquo;Reilly&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/emer/emergent">https://github.com/emer/emergent&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Randall C. O&amp;rsquo;Reilly&lt;/p>
&lt;hr></description></item><item><title>Uncertainpy A python toolbox for uncertainty quantification and sensitivity analysis tailored towards computational neuroscience</title><link>https://open-neuroscience.com/post/uncertainpy_a_python_toolbox_for_uncertainty_quantification_and_sensitivity_analysis_tailored_towards_computational_neuroscience/</link><pubDate>Wed, 04 Nov 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/uncertainpy_a_python_toolbox_for_uncertainty_quantification_and_sensitivity_analysis_tailored_towards_computational_neuroscience/</guid><description>&lt;p>Uncertainpy is a python toolbox for uncertainty quantification and sensitivity analysis tailored towards computational neuroscience.&lt;/p>
&lt;p>Uncertainpy is model independent and treats the model as a black box where the model can be left unchanged. Uncertainpy implements both quasi-Monte Carlo methods and polynomial chaos expansions using either point collocation or the pseudo-spectral method. Both of the polynomial chaos expansion methods have support for the rosenblatt transformation to handle dependent input parameters.&lt;/p>
&lt;p>Uncertainpy is feature based, i.e., if applicable, it recognizes and calculates the uncertainty in features of the model, as well as the model itself. Examples of features in neuroscience can be spike timing and the action potential shape.&lt;/p>
&lt;p>Uncertainpy is tailored towards neuroscience models, and comes with several common neuroscience models and features built in, but new models and features can easily be implemented. It should be noted that while Uncertainpy is tailored towards neuroscience, the implemented methods are general, and Uncertainpy can be used for many other types of models and features within other fields.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Simen Tennøe; Geir Halnes; Gaute Einevoll&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/simetenn/uncertainpy">https://github.com/simetenn/uncertainpy&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Simen Tennøe&lt;/p>
&lt;hr></description></item><item><title>EmotiBit</title><link>https://open-neuroscience.com/post/emotibit/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/emotibit/</guid><description>&lt;p>EmotiBit is a wearable sensor to capture high-quality emotional, physiological, and movement data from just about anywhere on the body.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Sean Montgomery&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.emotibit.com/">https://www.emotibit.com/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=jbcL2jzyWj4&amp;amp;ab_channel=EmotiBit">https://www.youtube.com/watch?v=jbcL2jzyWj4&amp;amp;ab_channel=EmotiBit&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Sean Montgomery&lt;/p>
&lt;hr></description></item><item><title>BrainSMASH</title><link>https://open-neuroscience.com/post/brainsmash/</link><pubDate>Tue, 06 Oct 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/brainsmash/</guid><description>&lt;p>BrainSMASH is a Python-based framework for quantifying the significance of a brain map’s spatial topography in studies of large-scale brain organization. BrainSMASH was designed to generate synthetic brain maps with spatial autocorrelation (SA) matched to the SA of a target brain map.&lt;/p>
&lt;p>SA is a prominent and ubiquitous property of brain maps that violates the assumptions of independence/exchangeability that underlie many conventional statistical tests. Controlling for this property is therefore necessary to disambiguate meaningful topographic relationships from chance associations. To this end, BrainSMASH instantiates a generative null model for simulating surrogate brain maps, constrained by empirical data, that preserve the SA of cortical (surface-based), subcortical (volumetric), parcellated, and dense brain maps.&lt;/p>
&lt;p>BrainSMASH requires only two inputs: a brain map of interest, and a matrix of pairwise distances between elements of the brain map. How these inputs are derived is left to user discretion, though additional support has been provided for investigators working with HCP-compliant neuroimaging files. Specifically, BrainSMASH includes routines to generate two-dimensional Euclidean and geodesic distance matrices from surface geometry (GIFTI) files, and subcortical Euclidean distance matrices from CIFTI-format neuroimaging files.&lt;/p>
&lt;p>Detailed documentation for BrainSMASH can be found at &lt;a href="https://brainsmash.readthedocs.io/">https://brainsmash.readthedocs.io/&lt;/a>.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Joshua B. Burt; Markus Helmer; Maxwell Shinn; Alan Anticevic; John D. Murray&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/murraylab/brainsmash">https://github.com/murraylab/brainsmash&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Joshua B. Burt&lt;/p>
&lt;hr></description></item><item><title>PsychRNN: An Accessible and Flexible Python Package for Training Recurrent Neural Network Models on Cognitive Tasks</title><link>https://open-neuroscience.com/post/psychrnn_an_accessible_and_flexible_python_package_for_training_recurrent_neural_network_models_on_cognitive_tasks/</link><pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/psychrnn_an_accessible_and_flexible_python_package_for_training_recurrent_neural_network_models_on_cognitive_tasks/</guid><description>&lt;p>PsychRNN is designed for neuroscientists and psychologists who are interested in RNNs as models of cognitive function in the brain.&lt;/p>
&lt;p>Despite growing interest in RNNs as models of brain function, this approach poses relatively high barriers to entry to researchers, due to the technical know-how required for specialized deep learning software (e.g. TensorFlow or PyTorch) to train artificial neural network models.&lt;/p>
&lt;p>We designed PsychRNN with accessibility and flexibility as important goals.&lt;/p>
&lt;p>The frontend for users to define tasks and train RNNs uses only Python &amp;amp; NumPy, with no requirement for deep learning software.&lt;/p>
&lt;p>The backend, based on TensorFlow for model training, is readily extensible. This design allows for accessible high-level specification and parameterization of tasks and models, using only a few lines of Python.&lt;/p>
&lt;p>Modularity is central to PsychRNN&amp;rsquo;s design, to achieve flexibility in defining and parameterizing tasks and networks. This facilitates investigation of how task features (e.g. timing or input/output channels) shape the network solutions learned by the models.&lt;/p>
&lt;p>PsychRNN also provides support for implementation of neurobiologically motivated constraints on synaptic connectivity, such as: no autapses, structured connectivity (e.g. for multi-region RNNs), Dale&amp;rsquo;s principle (separate excitatory &amp;amp; inhibitory cells), and fixed nonplastic subset of synapses. Modularity enables implementation of curriculum learning, or task shaping. RNNs can be trained in closed-loop, with tasks progressively adjusted as behavioral performance improves. This is more similar to animal training, for investigation of how shaping impacts neural solutions.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Daniel B Ehrlich; Jasmine T Stone; David Brandfonbrener; Alexander Atanasov; John D Murray&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/murraylab/PsychRNN">https://github.com/murraylab/PsychRNN&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Jasmine Stone&lt;/p>
&lt;hr></description></item><item><title>FastTrack</title><link>https://open-neuroscience.com/post/fasttrack/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/fasttrack/</guid><description>&lt;p>FastTrack is an open-source cross-platform tracking software. Easy to install and easy to use, it can track a large variety of systems from active particles to animals, with a known or unknown number of objects. It can process movies from any quality on low-end to high-end computers.&lt;/p>
&lt;p>Two main features are implemented in the software:
- A fast and automatic tracking algorithm that can detect and track objects, conserving the objects' identities across the video recording.
- A manual tool to review the tracking where errors can be corrected rapidly and easily to achieve 100% accuracy with a minimum of efforts.&lt;/p>
&lt;p>FastTrack do not require coding abilities to be used. A developer documentation is available for users who want to embed FastTrack tracking algorithm directly inside their projects.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Benjamin Gallois&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/FastTrackOrg/FastTrack">https://github.com/FastTrackOrg/FastTrack&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="http://www.fasttrack.sh/UserManual/docs/assets/example_vid.webm">http://www.fasttrack.sh/UserManual/docs/assets/example_vid.webm&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Benjamin Gallois&lt;/p>
&lt;hr></description></item><item><title>MorphoPy: A python package for feature extraction of neural morphologies</title><link>https://open-neuroscience.com/post/morphopy_a_python_package_for_feature_extraction_of_neural_morphologies/</link><pubDate>Mon, 28 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/morphopy_a_python_package_for_feature_extraction_of_neural_morphologies/</guid><description>&lt;p>MorphoPy is an open software package written in Python3 that allows for visualization and processing of morphological reconstructions of neural data. It has been created to facilitate the translation from morphology graphs into descriptive features like density maps, morphometric statistics, and persistence diagrams for down-stream exploration and statistical analysis.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Sophie Laturnus; Adam von Daranyi; Ziwei Huang; Philipp Berens&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/berenslab/MorphoPy">https://github.com/berenslab/MorphoPy&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Sophie Laturnus&lt;/p>
&lt;hr></description></item><item><title>DIPY</title><link>https://open-neuroscience.com/post/dipy/</link><pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/dipy/</guid><description>&lt;p>DIPY is the paragon 3D/4D+ imaging library in Python. Contains generic methods for spatial normalization, signal processing, machine learning, statistical analysis and visualization of medical images. Additionally, it contains specialized methods for computational anatomy including diffusion, perfusion and structural imaging.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>&lt;a href="https://github.com/dipy/dipy/graphs/contributors">https://github.com/dipy/dipy/graphs/contributors&lt;/a>&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://dipy.org">https://dipy.org&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/channel/UCHnEuCRDGFOR5cfEo0nD3pw">https://www.youtube.com/channel/UCHnEuCRDGFOR5cfEo0nD3pw&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
anonymous&lt;/p>
&lt;hr></description></item><item><title>NeuroFedora</title><link>https://open-neuroscience.com/post/neurofedora/</link><pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/neurofedora/</guid><description>&lt;p>NeuroFedora is an initiative to provide a ready to use Fedora Linux based Free/Open source software platform for neuroscience. We believe that similar to Free software, science should be free for all to use, share, modify, and study. The use of Free software also aids reproducibility, data sharing, and collaboration in the research community. By making the tools used in the scientific process easier to use, NeuroFedora aims to take a step to enable this ideal.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>NeuroFedora volunteers @ the Fedora project&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://neuro.fedoraproject.org">https://neuro.fedoraproject.org&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Ankur Sinha (NeuroFedora SIG member)&lt;/p>
&lt;hr></description></item><item><title>BrainGlobe atlas API</title><link>https://open-neuroscience.com/post/brainglobe_atlas_api/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/brainglobe_atlas_api/</guid><description>&lt;p>Many excellent brain atlases exist for different species. Some of them have an API (application programming interface) to allow users to interact with the data programmatically (e.g. the excellent Allen Mouse Brain Atlas), but many do not, and there is no consistent way to process data from multiple sources.&lt;/p>
&lt;p>The brainglobe atlas API (BG-AtlasAPI) deals with this problem by providing a common interface for programmers to download and process atlas data from multiple sources.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Adam Tyson; Federico Claudi; Luigi Petrucco&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/brainglobe/bg-atlasapi">https://github.com/brainglobe/bg-atlasapi&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Adam Tyson&lt;/p>
&lt;hr></description></item><item><title>culture_shock</title><link>https://open-neuroscience.com/post/culture_shock/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/culture_shock/</guid><description>&lt;p>Culture Shock is an open-source electroporator that was developed
through internet based collaboration, starting on the DIYbio Google
Group. It is an evolution on the traditional capacitive discharge
circuit topology, instead using pulsed induction to enable a
programmable waveform as well as reduce the size, weight, and cost of
the equipment. With all these benefits, we hope to reduce the burden
of laboratory consumables for DNA transformation and electrofusion
procedures, where chemical supplies are currently relied on. The added
benefit of programmability allows many cell types to be manipulated by
altering the voltage level, or even giving the voltage profile a
particular shape.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>John Griessen; Nathan McCorkle; Bryan Bishop&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/kanzure/culture_shock">https://github.com/kanzure/culture_shock&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Nathan McCorkle&lt;/p>
&lt;hr></description></item><item><title>brainrender</title><link>https://open-neuroscience.com/post/brainrender/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/brainrender/</guid><description>&lt;p>brainrender is a python package for the visualization of three dimensional neuro-anatomical data. It can be used to render data from publicly available data set (e.g. Allen Brain atlas) as well as user generated experimental data. The goal of brainrender is to facilitate the exploration and dissemination of neuro-anatomical data by providing a user-friendly platform to create high-quality 3D renderings.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Federico Claudi&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/brainglobe/brainrender">https://github.com/brainglobe/brainrender&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Federico Claudi&lt;/p>
&lt;hr></description></item><item><title>JASP</title><link>https://open-neuroscience.com/post/jasp/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/jasp/</guid><description>&lt;p>JASP is a cross-platform statistical software program with a state-of-the-art graphical user interface. The JASP interface allows you to conduct statistical analyses in seconds, and without having to learn programming or risking a programming mistake. JASP is open-source and free of charge, and we provide it as a service to the community. JASP is statistically inclusive as it offers both frequentist and Bayesian analysis methods.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>The JASP Team&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://jasp-stats.org/">https://jasp-stats.org/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=HxqB7CUA-XI">https://www.youtube.com/watch?v=HxqB7CUA-XI&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
EJ Wagenmakers&lt;/p>
&lt;hr></description></item><item><title>pyControl</title><link>https://open-neuroscience.com/post/pycontrol/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pycontrol/</guid><description>&lt;p>pyControl is a system of open source hardware and software for controlling behavioural experiments, built around the Micropython microcontroller.&lt;/p>
&lt;p>pyControl makes it easy to program complex behavioural tasks using a clean, intuitive, and flexible syntax for specifying tasks as state machines. User created task definition files, written in Python, run directly on the microcontroller, supported by pyControl framework code. This gives users the power and simplicity of Python for specifying task behaviour, while allowing advanced users low-level access to the microcontroller hardware.&lt;/p>
&lt;p>pyControl hardware consists of a breakout board and a set of devices such as nose-pokes, audio boards, LED drivers, rotary encoders and stepper motor controllers that are connected to the breakout board to create behavioural setups.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Thomas Akam&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://pycontrol.readthedocs.io">https://pycontrol.readthedocs.io&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Thomas Akam&lt;/p>
&lt;hr></description></item><item><title>pyPhotometry</title><link>https://open-neuroscience.com/post/pyphotometry/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pyphotometry/</guid><description>&lt;p>pyPhotometry is system of open source, Python based, hardware and software for neuroscience fiber photometry data acquisition, consisting of an acquisition board and graphical user interface.&lt;/p>
&lt;p>pyPhotometry supports data aquisition from two analog and two digital inputs, and control of two LEDs via built in LED drivers with an adjustable 0-100mA output. The system supports time-division multiplexed illumination which allows fluoresence evoked by different excitation wavelengths to be independenly readout from a single photoreciever signal.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Thomas Akam&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://pyphotometry.readthedocs.io">https://pyphotometry.readthedocs.io&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Thomas Akam&lt;/p>
&lt;hr></description></item><item><title>SLEAP</title><link>https://open-neuroscience.com/post/sleap/</link><pubDate>Sun, 26 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/sleap/</guid><description>&lt;p>SLEAP (Social LEAP Estimates Animal Poses) is a multi-animal pose tracker based on deep learning. It is the successor of LEAP (Pereira et al., Nature Methods, 2019) and was designed to deal with the problem of tracking body landmarks of multiple freely interacting animals.&lt;/p>
&lt;p>Using deep learning, SLEAP trains neural network models from few user annotations to enable highly accurate body part localization, grouping and tracking. It supports multiple neural network architectures, including pretrained state-of-the-art models and lightweight customizable architectures. SLEAP has been used successfully to track mice, fruit flies, bees and other species of animals under a variety of experimental and imaging conditions.&lt;/p>
&lt;p>The software was designed to make it easy for users with no experience with deep learning through a fully featured GUI, as well as providing a rich functionality for advanced users seeking to develop a custom solution for their project. Tutorials and guides are available on our website (&lt;a href="https://sleap.ai">https://sleap.ai&lt;/a>) detailing steps for easy installation (Windows/Mac/Linux), labeling a new project, training on the locally or on the cloud, and tracking new data.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Talmo Pereira; Joshua Shaevitz; Mala Murthy&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://sleap.ai">https://sleap.ai&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=zwCf1pGnBUw">https://www.youtube.com/watch?v=zwCf1pGnBUw&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Talmo Pereira&lt;/p>
&lt;hr></description></item><item><title>Neuroimaging Informatics Tools and Resources Collaboratory (NITRC)</title><link>https://open-neuroscience.com/post/neuroimaging_informatics_tools_and_resources_collaboratory_nitrc/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/neuroimaging_informatics_tools_and_resources_collaboratory_nitrc/</guid><description>&lt;p>NeuroImaging Tools &amp;amp; Resources Collaboratory is an award-winning free web-based resource that offers comprehensive information on an ever expanding scope of neuroinformatics software and data. Since debuting in 2007, NITRC has helped the neuroscience community make further discoveries using software and data produced from research that used to end up lost or disregarded.&lt;/p>
&lt;p>NITRC also provides free access to data and enables pay-per-use cloud-based access to unlimited computing power, enabling worldwide scientific collaboration with minimal startup and cost. NITRC’s scientific focus includes: MR, PET/SPECT, CT, EEG/MEG, optical imaging, clinical neuroimaging, computational neuroscience, and imaging genomics software tools, data, and computational resources.&lt;/p>
&lt;p>With NITRC and its components—the Resources Registry (NITRC-R), Image Repository (NITRC-IR), and Computational Environment (NITRC-CE)—a researcher can obtain pilot or proof-of-concept data to validate a hypothesis for just a few dollars.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>NITRC Development Team&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://www.nitrc.org">http://www.nitrc.org&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
David Kennedy&lt;/p>
&lt;hr></description></item><item><title>PiVR</title><link>https://open-neuroscience.com/post/pivr/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pivr/</guid><description>&lt;p>PiVR is a system that allows experimenters to immerse small animals into virtual realities. The system tracks the position of the animal and presents light stimulation according to predefined rules, thus creating a virtual landscape in which the animal can behave. By using optogenetics, we have used PiVR to present fruit fly larvae with virtual olfactory realities, adult fruit flies with a virtual gustatory reality and zebrafish larvae with a virtual light gradient.&lt;/p>
&lt;p>PiVR operates at high temporal resolution (70Hz) with low latencies (&amp;lt;30 milliseconds) while being affordable (&amp;lt;US$500) and easy to build (&amp;lt;6 hours). Through extensive documentation (&lt;a href="http://www.PiVR.org">www.PiVR.org&lt;/a>), this tool was designed to be accessible to a wide public, from high school students to professional researchers studying systems neuroscience in academia.&lt;/p>
&lt;p>The project is open source (BSD-3) and the documented code written in the freely available programming language Python. We hope that PiVR will be adapted by advanced users for their particular needs, for example to create closed-loop experiments involving other sensory modalities (e.g., sound/vibration) through the use of PWM controllable devices. We envision PiVR to be used as the central module when creating virtual realities for a variety of sensory modalities. This ‘PiVR module’ takes care of detecting the animal and presenting the appropriate PWM signal that is then picked up by the PWM controllable device installed by the user, for example to produce a sound whenever an animal enters a pre-defined region.&lt;/p>
&lt;p>In short, PiVR is a powerful and affordable experimental platform allowing experimenters to create a wide array of virtual reality experiments. Our hope is that PiVR will be adapted by several labs to democratize closed-loop experiments and, by standardizing image quality and the animal detection algorithm, increase reproducibility.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>David Tadres; Matthieu Louis&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://www.PiVR.org">http://www.PiVR.org&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=w5tIG6B6FWo">https://www.youtube.com/watch?v=w5tIG6B6FWo&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
David Tadres&lt;/p>
&lt;hr></description></item><item><title>ReproNim: A Center for Reproducible Neuroimaging Computation</title><link>https://open-neuroscience.com/post/repronim_a_center_for_reproducible_neuroimaging_computation/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/repronim_a_center_for_reproducible_neuroimaging_computation/</guid><description>&lt;p>ReproNim&amp;rsquo;s goal is to improve the reproducibility of neuroimaging science and extend the value of our national investment in neuroimaging research, while making the process easier and more efficient for investigators.&lt;/p>
&lt;p>ReproNim delivers a reproducible analysis framework comprised of components that include: 1) data and software discovery; 2) implementation of standardized description of data, results and workflows; 3) development of execution options that facilitates operation in all computational environments; 4)
provision of training and education to the community.&lt;/p>
&lt;p>All components of the framework are intended to foster continued use and development of the reproducible and generalizable framework in neuroimaging research.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>The ReproNim Development Team&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/ReproNim">https://github.com/ReproNim&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
David Kennedy&lt;/p>
&lt;hr></description></item><item><title>Simple Behavioral Analysis (SimBA)</title><link>https://open-neuroscience.com/post/simple_behavioral_analysis_simba/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/simple_behavioral_analysis_simba/</guid><description>&lt;p>Several excellent computational frameworks exist that enable high-throughput and consistent tracking of freely moving unmarked animals. SimBA introduce and distribute a plug-and play pipeline that enables users to use these pose-estimation approaches in combination with behavioral annotation for the generation of supervised machine-learning behavioral predictive classifiers.&lt;/p>
&lt;p>SimBA was developed for the analysis of complex social behaviors, but includes the flexibility for users to generate predictive classifiers across other behavioral modalities with minimal effort and no specialized computational background.&lt;/p>
&lt;p>SimBA has a variety of extended functions for large scale batch video pre-processing, generating descriptive statistics from movement features, and interactive modules for user-defined regions of interest and visualizing classification probabilities and movement patterns.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Simon Nilsson: Jia Jie Chhong; Sophia Hwang; Nastacia Goodwin; Sam A Golden&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/sgoldenlab/simba">https://github.com/sgoldenlab/simba&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=Frq6mMcaHBc&amp;amp;list=PLi5Vwf0hhy1R6NDQJ3U28MOUJPfl2YWYl&amp;amp;index=2&amp;amp;t=0s">https://www.youtube.com/watch?v=Frq6mMcaHBc&amp;amp;list=PLi5Vwf0hhy1R6NDQJ3U28MOUJPfl2YWYl&amp;amp;index=2&amp;amp;t=0s&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Simon Nilsson&lt;/p>
&lt;hr></description></item><item><title>Neurodata Without Borders</title><link>https://open-neuroscience.com/post/neurodata_without_borders/</link><pubDate>Sun, 19 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/neurodata_without_borders/</guid><description>&lt;p>Neurodata Without Borders is a data standard for neurophysiology, providing neuroscientists with a common standard to share, archive, use, and build analysis tools for neurophysiology data. NWB is designed to store a variety of neurophysiology data, including data from intracellular and extracellular electrophysiology experiments, data from optical physiology experiments, and tracking and stimulus data.&lt;/p>
&lt;p>The NWB team consists of neuroscientists and software developers who recognize that adoption of a unified data format is an important step toward breaking down the barriers to data sharing in neuroscience.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Andrew Tritt; Ryan Ly; Ben Dichter; Oliver Ruebel&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.nwb.org/">https://www.nwb.org/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://youtu.be/vfQsMyl0HQI">https://youtu.be/vfQsMyl0HQI&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Ben Dichter&lt;/p>
&lt;hr></description></item><item><title>OpenDrop Digital Microfluidics Platform</title><link>https://open-neuroscience.com/post/opendrop_digital_microfluidics_platform/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/opendrop_digital_microfluidics_platform/</guid><description>&lt;p>OpenDrop a modular, open source digital microfludics platform for research purposes. The device uses recent electro-wetting technology to control small droplets of liquids. Potential applications are lab on a chip devices for automating processes of digital biology.&lt;/p>
&lt;p>The OpenDrop V4 is modular electrowetting controller. The driver board is equipped with a connector that can host a circuit board cartridge with a 14×8 electrode array and 4 reservoirs. The liquids stay on a thin, hydrophobic foil laminated to the circuit board . The device is powered from USB though an included USB-C cable. All the voltage level are generated on the device and can be set with the built in soft menu from 150-300 Volts, DC or AC.&lt;/p>
&lt;p>OpenDrop Cartridges
The modular concepts of the OpenDrop V4 allows different configurations of cartridges: A gold coated electrode array board that can be coated with any dielectric layer and hydrophobic coating to make cartridges for topless digital microfluidic applications using readily available materials.The OpenDrop V4 Cartridge is a close-cell cartridge capable of “move, mix, split and reservoir dispensing”. The 4×8 electrode array and 4 reservoirs are laminated with a 15um ETFE foil, hydrophobic coating and ITO top cover.&lt;/p>
&lt;p>Programming
The OpenDrop V4 can be operated standalone and droplets can be moved through the built in joystick. A control software to program sequences of patterns from a computer is available as a free download. The board is also compatible with Adafruit Feather M0 controller boards and can be reprogrammed through the free Arduino IDE for custom specific applications. A sample code with the instruction to activate electrodes can be found on the OpenDrop GitHub.&lt;/p>
&lt;p>Features:&lt;/p>
&lt;ul>
&lt;li>Modular Cartridge System
- Connector to connect electrode board with up to 128 channels
- Gold coated 14×8 electrodes array, 2.75 mm x 2.75 mm in size, 4mil gaps&lt;/li>
&lt;li>Reservoirs – the new electrode array features 4 CT-type reservoirs&lt;/li>
&lt;li>AC and DC voltage generated on the device form USB power. True AC voltage driving capability (up to 300VAC).&lt;/li>
&lt;li>32bit AVR SAMD21G18 microprocessor with plenty of memory and power
- Electronic settings for voltage level, frequency and AC/DC selection
- Electronic reading of actual voltage level
- One connector for communication and powering (USB-C)
- Optical isolation of the high-voltage electronics trough opto-couplers and PhotoMOS
- New polyphonic audio amplifier and speaker (it’s a synth!)
- Cartridge presence detection
- Feedback amplifier
- Super flat OLED Display
- Nice joystick and 2 buttons, 3 LEDs
- Reset button
- All files open source, designed on KiCAD&lt;/li>
&lt;/ul>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>MSc Urs Gaudenz&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://www.gaudi.ch/OpenDrop/">http://www.gaudi.ch/OpenDrop/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=TY97QfWY6J4">https://www.youtube.com/watch?v=TY97QfWY6J4&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Anonymous&lt;/p>
&lt;hr></description></item><item><title>Stytra</title><link>https://open-neuroscience.com/post/stytra/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/stytra/</guid><description>&lt;p>Stytra, a flexible, open-source software package, written in Python and designed to cover all the general requirements involved in larval zebrafish behavioral experiments.&lt;/p>
&lt;p>It provides timed stimulus presentation, interfacing with external devices and simultaneous real-time tracking of behavioral parameters such as position, orientation, tail and eye motion in both freely-swimming and head-restrained preparations.&lt;/p>
&lt;p>Stytra logs all recorded quantities, metadata, and code version in standardized formats to allow full provenance tracking, from data acquisition through analysis to publication.&lt;/p>
&lt;p>The package is modular and expandable for different experimental protocols and setups. We also provide complete documentation with examples for extending the package to new stimuli and hardware, as well as a schema and parts list for behavioural setups.&lt;/p>
&lt;p>The software can be used in the context of calcium imaging experiments by interfacing with other acquisition devices.&lt;/p>
&lt;p>Our aims are to enable more laboratories to easily implement behavioral experiments, as well as to provide a platform for sharing stimulus protocols that permits easy reproduction of experiments and straightforward validation.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Vilim Stih; Luigi Petrucco&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/portugueslab/stytra">https://github.com/portugueslab/stytra&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Anonymous&lt;/p>
&lt;hr></description></item><item><title>cellfinder</title><link>https://open-neuroscience.com/post/cellfinder/</link><pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/cellfinder/</guid><description>&lt;p>cellfinder is software from the Margrie Lab at the Sainsbury Wellcome Centre for automated 3D cell detection and registration of whole-brain images (e.g. serial two-photon or lightsheet imaging).&lt;/p>
&lt;p>It’s a work in progress, but cellfinder can:&lt;/p>
&lt;ul>
&lt;li>Detect labelled cells in 3D in whole-brain images (many hundreds of GB)&lt;/li>
&lt;li>Register the image to an atlas (such as the Allen Mouse Brain Atlas)&lt;/li>
&lt;li>Segment the brain based on the reference atlas&lt;/li>
&lt;li>Calculate the volume of each brain area, and the number of labelled cells within it&lt;/li>
&lt;li>Transform everything into standard space for analysis and visualisation&lt;/li>
&lt;/ul>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Adam Tyson&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/brainglobe/cellfinder">https://github.com/brainglobe/cellfinder&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Adam Tyson&lt;/p>
&lt;hr></description></item><item><title>DeepLabCut</title><link>https://open-neuroscience.com/post/deeplabcut/</link><pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/deeplabcut/</guid><description>&lt;p>DeepLabCut™ is an efficient method for 3D markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results (i.e. you can match human labeling accuracy) with minimal training data (typically 50-200 frames). We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors.&lt;/p>
&lt;p>The package is open source, fast, robust, and can be used to compute 3D pose estimates. Please see the original paper and the latest work below. This package is collaboratively developed by the Mathis Group &amp;amp; Mathis Lab at EPFL/Harvard.&lt;/p>
&lt;p>The code is freely available and easy to install in a few clicks with Anaconda (and pypi). Please see instructions on deeplabcut.org. We also provide a very easy to use GUI interface, and a step-by-step user guide!&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Mackenzie Mathis, Alexander Mathis &amp;amp; contributors&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://deeplabcut.org/">http://deeplabcut.org/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/channel/UC2HEbWpC_1v6i9RnDMy-dfA">https://www.youtube.com/channel/UC2HEbWpC_1v6i9RnDMy-dfA&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Mackenzie Mathis&lt;/p>
&lt;hr></description></item><item><title>Nilearn</title><link>https://open-neuroscience.com/post/nilearn/</link><pubDate>Sat, 11 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/nilearn/</guid><description>&lt;p>Nilearn is a Python module for fast and easy statistical learning on NeuroImaging data. It leverages the scikit-learn Python toolbox for multivariate statistics with applications such as predictive modelling, classification, decoding, or connectivity analysis.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>&lt;a href="https://github.com/orgs/nilearn/people">https://github.com/orgs/nilearn/people&lt;/a>&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://nilearn.github.io/">http://nilearn.github.io/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Anonymous&lt;/p>
&lt;hr></description></item><item><title>A Cartesian Coordinate Robot for Dispensing Fruit Fly Food</title><link>https://open-neuroscience.com/post/a_cartesian_coordinate_robot_for_dispensing_fruit_fly_food/</link><pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/a_cartesian_coordinate_robot_for_dispensing_fruit_fly_food/</guid><description>&lt;p>The fruit fly, Drosophila melanogaster, continues to be one of the most widely used model organisms in biomedical research.&lt;/p>
&lt;p>Though chosen for its ease of husbandry, maintaining large numbers of stocks of fruit flies, as done by many laboratories, is labour-intensive.&lt;/p>
&lt;p>One task which lends itself to automation is the production of the vials of food in which the flies are reared. Fly facilities typically have to generate several thousand vials of fly food each week to sustain their fly stocks.&lt;/p>
&lt;p>The system presented here combines a cartesian coordinate robot with a peristaltic pump. The design of the robot is based on an open hardware CNC (computer numerical control) machine, and uses belt and pulley actuators for the X and Y axes, and a leadscrew actuator for the Z axis.&lt;/p>
&lt;p>CNC motion and operation of the peristaltic pump are controlled by grbl (&lt;a href="https://github.com/gnea/grbl),">https://github.com/gnea/grbl),&lt;/a> an open source, embedded, G-code parser. Grbl is written in optimized C and runs directly on an Arduino. A Raspberry Pi is used to generate and stream G-code instructions to Grbl.&lt;/p>
&lt;p>A touch screen on the Raspberry Pi provides a graphical user interface to the system. Whilst the robot was built for the express purpose of filling vials of fly food, it could potentially be used for other liquid handling tasks in the laboratory.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Matt Wayland; Matthias Landgraf&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/WaylandM/fly-food-robot">https://github.com/WaylandM/fly-food-robot&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://doi.org/10.6084/m9.figshare.5175223.v1">https://doi.org/10.6084/m9.figshare.5175223.v1&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matt Wayland&lt;/p>
&lt;hr></description></item><item><title>Bonsai</title><link>https://open-neuroscience.com/post/bonsai/</link><pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/bonsai/</guid><description>&lt;p>Bonsai is a high-performance, easy to use, and flexible visual programming language for designing closed-loop neuroscience experiments combining physiology and behaviour data.&lt;/p>
&lt;p>Bonsai has allowed scientists with no previous programming experience to quickly develop their own experimental rigs and is also being increasingly used as a platform to integrate new open-source hardware and software from the experimental neuroscience community.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Gonçalo Lopes&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://bonsai-rx.org/">https://bonsai-rx.org/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Gonçalo Lopes&lt;/p>
&lt;hr></description></item><item><title>Ethoscopes</title><link>https://open-neuroscience.com/post/ethoscopes/</link><pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/ethoscopes/</guid><description>&lt;p>Ethoscopes are machines for high-throughput analysis of behavior in Drosophila and other animals.&lt;/p>
&lt;p>Ethoscopes provide a software and hardware solution that is reproducible and easily scalable.&lt;/p>
&lt;p>They perform, in real-time, tracking and profiling of behavior by using a supervised machine learning algorithm, are able to deliver behaviorally triggered stimuli to flies in a feedback-loop mode, and are highly customizable and open source.&lt;/p>
&lt;p>Ethoscopes can be built easily by using 3D printing technology and rely on Raspberry Pi microcomputers and Arduino boards to provide affordable and flexible hardware.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Quentin Geissmann; Luis Garcia; Giorgio Gilestro&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://lab.gilest.ro/ethoscope">http://lab.gilest.ro/ethoscope&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=5oWGBUMJON8&amp;amp;feature=emb_title">https://www.youtube.com/watch?v=5oWGBUMJON8&amp;amp;feature=emb_title&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Giorgio Gilestro&lt;/p>
&lt;hr></description></item><item><title>Bonvision</title><link>https://open-neuroscience.com/post/bonvision/</link><pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/bonvision/</guid><description>&lt;p>BonVision is an open-source closed-loop visual environment generator developed by the Saleem Lab and Solomon Lab at the UCL Institute of Behavioural Neuroscience in collaboration with NeuroGEARS.&lt;/p>
&lt;p>BonVision’s key features include:&lt;/p>
&lt;pre>&lt;code>Naturally closed-loop system based on reactive coding of the Bonsai framework
Handles 2D and 3D stimuli with equal ease
Visual environment generated independent of display configuration
Graphical programming language of the Bonsai framework
Can be used for Augmented Reality, Virtual Reality or 2D visual stimuli
Does not require the observer to be in a fixed position
&lt;/code>&lt;/pre>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Bonvision&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://bonvision.github.io">http://bonvision.github.io&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Poseidon</title><link>https://open-neuroscience.com/post/poseidon/</link><pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/poseidon/</guid><description>&lt;p>The Poseidon is an open-source syringe pump and microscope system. It uses 3D printed parts and common components that can be easily purchased. It can be used in microfluidics experiments or other applications. You can assemble it in a short-time for under $400. The system is modular and highly customizable. Examples of applications are: control the chemical environment of a bioreactor, purify proteins and precisely add reagents to chemical reactions over time.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Pachter Lab&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://pachterlab.github.io/poseidon">https://pachterlab.github.io/poseidon&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>DeepLabStream</title><link>https://open-neuroscience.com/post/deeplabstream/</link><pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/deeplabstream/</guid><description>&lt;p>DeepLabStream is a python based multi-purpose tool that enables the realtime tracking of animals and manipulation of experiments. Our toolbox is adapted from the previously published DeepLabCut (Mathis et al., 2018) and expands on its core capabilities. DeepLabStreams core feature is the real-time analysis using any type of camera-based video stream (incl. multiple streams). Building onto that, we designed a full experimental closed-loop toolkit. It enables running experimental protocols that are dependent on a constant stream of bodypart positions and feedback activation of several input/output devices. It&amp;rsquo;s capabilities range from simple region of interest (ROI) based triggers to headdirection or behavior dependent stimulation.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Schwarz Neurocon Lab&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/SchwarzNeuroconLab/DeepLabStream">https://github.com/SchwarzNeuroconLab/DeepLabStream&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>SpikeInterface</title><link>https://open-neuroscience.com/post/spikeinterface/</link><pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/spikeinterface/</guid><description>&lt;p>SpikeInterface is a unified Python framework for spike sorting. With its high-level API, it is designed to be accessible and easy to use, allowing users to build full analysis pipelines for spike sorting (reading-writing (IO) / preprocessing / spike sorting / postprocessing / validation / curation / comparison / visualization) with a few lines of code.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Alessio Buccino*; Cole Hurwitz*; Samuel Garcia; Jeremy Magland; Josh Siegle; Matthias Hennig&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/SpikeInterface/spikeinterface">https://github.com/SpikeInterface/spikeinterface&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=nWJGwFB7oII">https://www.youtube.com/watch?v=nWJGwFB7oII&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Alessio Buccino&lt;/p>
&lt;hr></description></item><item><title>neuTube</title><link>https://open-neuroscience.com/post/neutube/</link><pubDate>Mon, 22 Jun 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/neutube/</guid><description>&lt;p>neuTube is an open source software for reconstructing neurons from fluorescence microscope images. It is easy to use and improves the efficiency of reconstructing neuron structures accurately. The framework combines 2D/3D visualization, semi-automated tracing algorithms, and flexible editing options that simplify the task of neuron reconstruction.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Ting Zhao&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.neutracing.com/">https://www.neutracing.com/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>OpenTrons</title><link>https://open-neuroscience.com/post/opentrons/</link><pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/opentrons/</guid><description>&lt;p>Today, biologists spend too much time pipetting by hand. We think biologists should have robots to do pipetting for them. People doing science should be free of tedious benchwork and repetitive stress injuries. They should be able to spend their time designing experiments and analyzing data.&lt;/p>
&lt;p>That&amp;rsquo;s why we started Opentrons.&lt;/p>
&lt;p>We make robots for biologists. Our mission is to provide the scientific community with a common platform to easily share protocols and reproduce each other&amp;rsquo;s results. Our robots automate experiments that would otherwise be done by hand, allowing our community to spend more time pursuing answers to some of the 21st century’s most important questions&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Opentrons&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://opentrons.com/about">https://opentrons.com/about&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/channel/UCvMRmXIxnHs3AutkVhuqaQg">https://www.youtube.com/channel/UCvMRmXIxnHs3AutkVhuqaQg&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Deep Cinac</title><link>https://open-neuroscience.com/post/deep_cinac/</link><pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/deep_cinac/</guid><description>&lt;p>Two-photon calcium imaging is now widely used to infer neuronal dynamics from changes in fluorescence of an indicator. However, state of the art computational tools are not optimized for the reliable detection of fluorescence transients from highly synchronous neurons located in densely packed regions such as the CA1 pyramidal layer of the hippocampus during early postnatal stages of development. Indeed,the latest analytical tools often lack proper benchmark measurements. To meet this challenge, we first developed a graphical user interface allowing for a precise manual detection of all calcium transients from imaged neurons based on the visualization of the calcium imaging movie. Then, we analyzed the movies using a convolutional neural network with an attention process and a bidirectional long-short term memory network. This method is able to reach human performance and offers a better F1 score (harmonic mean of sensitivity and precision) than CaImAn to infer neural activity in the developingCA1 without any user intervention. It also enables automatically identifying activity originating from GABAergic neurons. Overall, DeepCINAC offers a simple, fast and flexible open-source toolbox for processing a wide variety of calcium imaging datasets while providing the tools to evaluate its performance.&lt;/p>
&lt;p>See full text at &lt;a href="https://www.biorxiv.org/content/10.1101/803726v2.full.pdf">https://www.biorxiv.org/content/10.1101/803726v2.full.pdf&lt;/a>&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Julien Denis&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://gitlab.com/cossartlab/deepcinac">https://gitlab.com/cossartlab/deepcinac&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>UC2</title><link>https://open-neuroscience.com/post/uc2/</link><pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/uc2/</guid><description>&lt;p>The open-source optical toolbox UC2 [YouSeeToo] simplifies the process of building optical setups, by combining 3D-printed cubes, each holding a specific component (e.g. lens, mirror) on a magnetic square-grid baseplate. The use of widely available consumables and 3D printing, together with documentation and software, offers an extremely low-cost and accessible alternative for both education and research areas. In order to reduce the entry barrier, we provide a fully comprehensive toolbox called TheBOX. A paper describing the scientific application in detail can be found &lt;a href="https://www.biorxiv.org/content/10.1101/2020.03.02.973073v1" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Benedict Diederich; René Lachmann; Barbora Marsikova&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://useetoo.org">https://useetoo.org&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=ey4uEFEG6MY">https://www.youtube.com/watch?v=ey4uEFEG6MY&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Barbora Marsikova&lt;/p>
&lt;hr></description></item><item><title>Automated Operant Conditioning</title><link>https://open-neuroscience.com/post/automated_operant_conditioning/</link><pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/automated_operant_conditioning/</guid><description>&lt;p>Operant conditioning (OC) is a classical paradigm and a standard technique used in experimental psychology in which animals learn to perform an action to achieve a reward. By using this paradigm, it is possible to extract learning curves and measure accurately reaction times (RTs). Both these measurements are proxy of cognitive capabilities and can be used to evaluate the effectiveness of therapeutic interventions in mouse models of disease. Here, we describe a fully 3D printable device that is able to perform OC on freely moving mice, while performing real-time tracking of the animal position. We successfully trained six mice, showing stereotyped learning curves that are highly reproducible across mice and reaching &amp;gt;70% of accuracy after 2 d of conditioning. Different products for OC are commercially available, though most of them do not provide customizable features and are relatively expensive. This data demonstrate that this system is a valuable alternative to available state-of-the-art commercial devices, representing a good balance between performance, cost, and versatility in its use.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Raffaele Mazziotti, Giulia Sagona, Leonardo Lupori, Virginia Martini and Tommaso Pizzorusso&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/raffaelemazziotti/oc_chamber">https://github.com/raffaelemazziotti/oc_chamber&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>VocalMat</title><link>https://open-neuroscience.com/post/vocalmat/</link><pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/vocalmat/</guid><description>&lt;p>Mice emit ultrasonic vocalizations (USV) to transmit socially-relevant information. To detect and classify these USVs, here we describe the development of VocalMat. VocalMat is a software that uses image-processing and differential geometry approaches to detect USVs in audio files, eliminating the need for user-defined parameter tuning. VocalMat also uses computational vision and machine learning methods to classify USVs into distinct categories. In a dataset of &amp;gt;4,000 USVs emitted by mice, VocalMat detected more than &amp;gt;98% of the USVs and accurately classified ≈86% of USVs when considering the most likely label out of 11 different USV types. We then used Diffusion Maps and Manifold Alignment to analyze the probability distribution of USV classification among different experimental groups, providing a robust method to quantify and qualify the vocal repertoire of mice. Thus, VocalMat allows accurate and highly quantitative analysis of USVs, opening the opportunity for detailed and high-throughput analysis of this behavior.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Antonio H. O. Fonseca, Gustavo M. Santana, Sergio Bampi, Marcelo O Dietrich&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.dietrich-lab.org/vocalmat">https://www.dietrich-lab.org/vocalmat&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>BossDB</title><link>https://open-neuroscience.com/post/bossdb/</link><pubDate>Tue, 26 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/bossdb/</guid><description>&lt;p>BossDB is a volumetric database that lives in the AWS cloud. Hundreds of terabytes of electron microscopy, light microscopy, and x-ray tomography data are available for free download and study.&lt;/p>
&lt;p>Have a project you want to share with the world for free? Get in touch!
&lt;a href="https://twitter.com/thebossdb">https://twitter.com/thebossdb&lt;/a>&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>JHU|APL&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://bossdb.org/">https://bossdb.org/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Jordan Matelsky&lt;/p>
&lt;hr></description></item><item><title>3D Slicer</title><link>https://open-neuroscience.com/post/3d_slicer/</link><pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/3d_slicer/</guid><description>&lt;p>3D Slicer is a software for medical image informatics, image processing, and three-dimensional visualization. It’s extremely powerful and versatile with plenty of different options. It is a great tool for volume rendering, registration, interactive segmentation of images and even offers the possibility of running Python scripts thought an embedded Python interpreter.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Ron Kikinis&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/Slicer/Slicer">https://github.com/Slicer/Slicer&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>Colaboratory</title><link>https://open-neuroscience.com/post/colaboratory/</link><pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/colaboratory/</guid><description>&lt;p>Colaboratory is a free Jupyter notebook environment that runs in the cloud. Your notebooks get stored on Google Drive. The great advantage is that you don’t have to install anything (however, for some features you need a Google account) on your system to use it. You can perform specific computations during data analysis with pre-installed Python libraries and gives you access to accelerated hardware for free (e.g. GPUs and TPUs).&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Google&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://colab.research.google.com/notebooks/intro.ipynb">https://colab.research.google.com/notebooks/intro.ipynb&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>Psychophysics toolboxes</title><link>https://open-neuroscience.com/post/psychophysics-toolboxes/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/psychophysics-toolboxes/</guid><description>&lt;br>
&lt;p>Roughly put, &lt;a href="http://en.wikipedia.org/wiki/Psychophysics" target="_blank" rel="noopener">psychophysics&lt;/a> studies the relationships of physical stimuli and their respective elicited sensations and perception. Psyhophysics also relates to the techniques used to probe these relationships and the toolboxes here presented are mainly dealing with these techniques.&lt;/p>
&lt;br>
&lt;hr>
&lt;br>
&lt;div align="center">
&lt;p>&lt;img src="https://raw.githubusercontent.com/smathot/osdoc/3.2/themes/cogsci/static/img/banner.png" alt="">&lt;/p>
&lt;/div>
&lt;p>&lt;a href="https://osdoc.cogsci.nl/" target="_blank" rel="noopener"> OpenSesame&lt;/a> is a graphical opensource experiment builder. It has drag and drop features as well as customization possibilities, via python scripting and custom plugins. here is a &lt;a href="http://link.springer.com/article/10.3758%2Fs13428-011-0168-7" target="_blank" rel="noopener">link&lt;/a> to a paper describing the software&lt;figure style="width: 853px" class="wp-caption alignnone">&lt;/p>
&lt;br>
&lt;hr>
&lt;br>
&lt;p>&lt;a href="http://psychtoolbox.org/" target="_blank" rel="noopener">Psychtoolbox&lt;/a>, or PTB, is a free versatile toolbox to be used mainly in visual experiments, it is able to deliver visual and auditory stimuli and to receive subject input. It has a big quantity of active users (15,000 as stated on their &lt;a href="http://psychtoolbox.org/forum/" target="_blank" rel="noopener">website&lt;/a>) what should make the life of the beginner user somehow easier (they have a &lt;a href="https://psychtoolbox.discourse.group/" target="_blank" rel="noopener">forum page&lt;/a>) The latest version (PTB-3 as this page was written) is able to run under MATLAB (version 7.X) and Octave (version 3.2.X) in any of the three main operational systems out there (Mac, Windows and Linux).  A paper describing the toolbox can be found &lt;a href="http://color.psych.upenn.edu/brainard/papers/Psychtoolbox.pdf" target="_blank" rel="noopener">here.&lt;/a>&lt;/p>
&lt;br>
&lt;hr>
&lt;br>
&lt;div align="center">
&lt;p>&lt;img src="./psychopyLogoOnline.png" alt="">&lt;/p>
&lt;/div>
&lt;p>&lt;a href="https://www.psychopy.org/" target="_blank" rel="noopener">PsychoPy&lt;/a> is also a free toolbox that can be used to deliver visual and auditory stimuli and receive inputs from subjects, on top of keyboard, mouse and button boxes, it also supports serial and parallel ports and compiled drivers (allowing interface with pretty much any hardware installed in your computer). It is written in Python, and it can be used with Windows, Mac or Linux. Two papers describing the toolbox can be found &lt;a href="https://www.sciencedirect.com/science/article/pii/S0165027006005772" target="_blank" rel="noopener">here&lt;/a> and &lt;a href="https://www.frontiersin.org/articles/10.3389/neuro.11.010.2008/full" target="_blank" rel="noopener">here&lt;/a>.&lt;/p></description></item><item><title>Python, NumPy, SciPy &amp; Matplotlib</title><link>https://open-neuroscience.com/post/python-numpy-scipy-matplotlib/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/python-numpy-scipy-matplotlib/</guid><description>&lt;p>Python is a free programming language that is widely used, most of the software developed for Linux is written in Python. It contains several libraries that cover a lot of problem domains, from asynchronous processing to zip files. Also it is available for most platforms. More information can be found at the language &lt;a href="http://www.python.org/" target="_blank" rel="noopener">official page.&lt;/a>&lt;/p>
&lt;p>More specifically to scientific computation, the &lt;a href="http://www.numpy.org/" target="_blank" rel="noopener">NumPy&lt;/a> project brings n-dimension array objects, random number capabilities, fourier transforms and many other useful tools.&lt;/p>
&lt;p>Boosting NumPy capabilities is &lt;a href="http://www.scipy.org/" target="_blank" rel="noopener">SciPy&lt;/a>, which is another Python library that adds signal processing, optimization and statistical tools to Python.&lt;/p>
&lt;p>After all the calculations are done, they can be plotted also using python and another useful library: &lt;a href="http://matplotlib.org/" target="_blank" rel="noopener">Matplotlib.&lt;/a>&lt;/p></description></item><item><title>Spike Gadgets</title><link>https://open-neuroscience.com/post/spike-gadgets/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/spike-gadgets/</guid><description>&lt;p>A brief description of their current software (09.Sep.2016) is provided by one of their founders, Mattias Karlsson:&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://www.spikegadgets.com/software/statescript.html" target="_blank" rel="noopener">State Script:&lt;/a>&lt;/strong>&lt;/p>
&lt;p>Do you need to control lasers for optogenetics, stimulators, or other TTL-based devices with precise, temporally defined patterns? Do you need to monitor beam breaks, lever presses, or other digital events in real time to define behavioral tasks?  You could program an Arduino, but that’s a lot of work. Or, you can use StateScript, which allows users with minimal programming experience define complex input/output relationships for the most demanding hardware control experiments.&lt;/p>
&lt;div>
&lt;p>This open-source project now runs on two available hardware platforms, the MBED LPC1768 micro controller board ($50) and the SpikeGadgets electrophysiology and behavioral control system.  More hardware support in is the works. A software interface, which is part of the Trodes open-source eletrophysiology suite (&lt;a href="http://www.spikegadgets.com/software/trodes.html" target="_blank">&lt;a href="http://www.spikegadgets.com/software/trodes.html">http://www.spikegadgets.com/software/trodes.html&lt;/a>&lt;/a>), allows you to upload scripts and dynamically interact with variables and ports states.&lt;/p>
&lt;br>
&lt;p>Anyone is welome to contribute. Here is the &lt;a href="https://bitbucket.org/mkarlsso/statescript" target="_blank" rel="noopener">bitbucket repo.&lt;/a>&lt;/p>
&lt;/div>
&lt;div align="center">
&lt;p>&lt;img src="https://i2.wp.com/www.spikegadgets.com/images/statescript_screenshot_2.png?resize=800%2C571" alt="">&lt;/p>
&lt;/div>
&lt;div>
&lt;p>&lt;strong>&lt;a href="http://www.spikegadgets.com/software/trodes.html" target="_blank" rel="noopener">Trodes:&lt;/a>&lt;/strong>&lt;/p>
&lt;p>Trodes is a software suite with a focus on data acquisition for extracellular neural recordings.  It has a growing user base and welcomes contributors with open arms! It is built using the ever-popular and powerful Qt C++ framework. While it is specialized to be used with SpikeGadgets’ ephys hardware, it also has built-in support for the Intan demo system and Open-Ephys hardware.&lt;/p>
&lt;p>It has some pretty impressive capabilities, including visualization of thousands of channels, spike viewing, online spike sorting, and low latency feedback control.  It has video processing, allowing position tracking that is synchronized to the recording, and integrates powerful environment control (lasers for optogenetics, levers, lights, pumps, etc.) with StateScript.&lt;/p>
&lt;/div>
&lt;div align="center">
&lt;p>&lt;img src="https://i1.wp.com/www.spikegadgets.com/images/trodesscreenshot.png?resize=800%2C444" alt="Trodes interface">&lt;/p>
&lt;p align="center"> Trodes interface &lt;/p>
&lt;p>&lt;img src="https://i1.wp.com/www.spikegadgets.com/images/trodes_screenshot_cameramod.png?resize=800%2C554" alt="Trodes interface">&lt;/p>
&lt;p align="center"> Trodes &lt;/p>
&lt;/div></description></item><item><title>Brainflow</title><link>https://open-neuroscience.com/post/brainflow/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/brainflow/</guid><description>&lt;p>&lt;a href="https://brainflow.ai/" target="_blank" rel="noopener">BrainFlow&lt;/a> BrainFlow is a library intended to obtain, parse and analyze EEG, EMG, ECG and other kinds of data from biosensors, it provides two APIs:&lt;/p>
&lt;ul>
&lt;li>Data Acquisition API to obtain data from BCI boards&lt;/li>
&lt;li>Signal Processing API which is completely independent and can be used without Data Acquisition API&lt;/li>
&lt;/ul>
&lt;p>Both of these APIs are uniform for all supported boards, so it allows to write completely board agnostic code.&lt;/p>
&lt;p>BrainFlow has bindings for:&lt;/p>
&lt;ul>
&lt;li>C++&lt;/li>
&lt;li>Python&lt;/li>
&lt;li>Java&lt;/li>
&lt;li>C#&lt;/li>
&lt;li>R&lt;/li>
&lt;/ul>
&lt;p>And provides almost the same API for all languages above.&lt;/p>
&lt;p>Check &lt;a href="https://brainflow.readthedocs.io" target="_blank" rel="noopener">BrainFlow Docs&lt;/a> for details.&lt;/p></description></item><item><title>Image, Office suites, and other general purpose software</title><link>https://open-neuroscience.com/post/image-office-suits-and-other-general-purpose-software/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/image-office-suits-and-other-general-purpose-software/</guid><description>&lt;p>If you are using Linux, changes are that this page is not that useful for you, since most of these programs come installed by default. For you who are not yet into linux, most of these programs have Windows/Mac versions:&lt;/p>
&lt;p>Office suites (spreadsheet calculation, slide manufacturing , document writing):&lt;/p>
&lt;p>&lt;a href="http://www.openoffice.org/" target="_blank" rel="noopener">Open Office&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://www.libreoffice.org/#0" target="_blank" rel="noopener">Libre Office&lt;/a>&lt;/p>
&lt;p>Image manipulation programs (vectorized images or photoshop style):&lt;/p>
&lt;p>&lt;a href="http://inkscape.org/" target="_blank" rel="noopener">Inkscape&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://www.gimp.org/" target="_blank" rel="noopener">Gimp&lt;/a>&lt;/p>
&lt;p>3D modelling (to create animations, solids or even things that can be printed):&lt;/p>
&lt;p>&lt;a href="http://free-cad.sourceforge.net/" target="_blank" rel="noopener">FreeCad&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://www.blender.org/" target="_blank" rel="noopener">Blender&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://www.openscad.org/" target="_blank" rel="noopener">OpenScad&lt;/a>&lt;/p></description></item><item><title>IPipet</title><link>https://open-neuroscience.com/post/ipipet/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/ipipet/</guid><description>&lt;p>IPipet is a neat system to help you not to lose track of which wells you have already pipetted in or from. The idea is simple, you place a tablet running a link with your specific pipetting protocol under your source and destination plates. The tablet will illuminate the corresponding wells. After you pipette one sample, you press next on the tablet and the next sample will be illuminated. For more details watch the video (below) and visit the &lt;a href="http://ipipet.teamerlich.org/usage" target="_blank" rel="noopener">project&amp;rsquo;s homepage.&lt;/a> They even have a &lt;a href="http://www.thingiverse.com/thing:339588" target="_blank" rel="noopener">3D printable adaptor&lt;/a> to prevent the well plate from slipping on the tablet surface.&lt;/p>
&lt;br>
&lt;div align="center">
&lt;iframe src="https://player.vimeo.com/video/90988265" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen>&lt;/iframe>
&lt;p align="center">&lt;a href="https://vimeo.com/90988265">iPipet Demo&lt;/a> from &lt;a href="https://vimeo.com/user26499168">Team Erlich&lt;/a> on &lt;a href="https://vimeo.com">Vimeo&lt;/a>.&lt;/p>
&lt;/div></description></item><item><title>Lab management software</title><link>https://open-neuroscience.com/post/lab-management-software/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/lab-management-software/</guid><description>&lt;p>Since organisation of ideas, stocks, and projects is a major concern (or at least should be) of labs and researchers, here is a small compilation of cost free sofware to help out:&lt;/p>
&lt;br>
&lt;div align="center">
&lt;p>&lt;img src="./Quartzy.png" alt="">&lt;/p>
&lt;/div>
&lt;p>&lt;a href="https://www.quartzy.com/" target="_blank" rel="noopener">Quartzy&lt;/a> is a free web based application (supported by life sciences related companies) it focuses on sharing protocols, tracking orders, manage lab inventory and shared quipment management.&lt;/p>
&lt;br>
&lt;div align="center">
&lt;p>&lt;img src="./elabftw-logo.png" alt="">&lt;/p>
&lt;/div>
&lt;p>&lt;a href="https://www.elabftw.net/" target="_blank" rel="noopener">eLabFTW&lt;/a> is a management system created by Nicolas Carpi. It is opensource (which means each lab can customize it for special needs), free and it can be installed locally. Its &lt;a href="https://demo.elabftw.net/login.php" target="_blank" rel="noopener">online demo version&lt;/a> focuses on experiment log, database (where drugs, chemicals, animal strains and etc can be logged) and team (where lab members can be listed).&lt;/p></description></item><item><title>Operating systems</title><link>https://open-neuroscience.com/post/linux-distributions/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/linux-distributions/</guid><description>&lt;p>Linux is an open source operating system and it is the major OS used in servers and supercomputers.  &lt;a href="http://www.ubuntu.com" target="_blank" rel="noopener">Ubuntu&lt;/a>, one of the best known distributions has been gaining space in the personal computing scene, now days already being factory &lt;a href="http://www.omgubuntu.co.uk/2012/05/ubuntu-to-ship-on-5-of-all-pcs-sold-next-year" target="_blank" rel="noopener">shipped&lt;/a> by major manufacturers.&lt;/p>
&lt;p>But how practical is to migrate to a Linux distribution? Well, very. If one passes beyond the hassle of backing up data and installing a new OS, there are many advantages that come with it. For starters these OSs are safer than any Microsoft or Apple OS. There is a large community of users sharing solutions to problems, bugs and so on (there hasn’t been to today a widespread of any &lt;a href="http://en.wikipedia.org/wiki/Linux_malware" target="_blank" rel="noopener">malware through Linux systems&lt;/a>). Being open source, the distributions are perfect for customization, something really useful for science labs.&lt;/p>
&lt;p>A Small list of distributions that make a good starting point:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://www.debian.org/" target="_blank" rel="noopener">Debian&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://neuro.debian.net/" target="_blank" rel="noopener">NeuroDebian&lt;/a> (Debian oriented to neuroscience)&lt;/li>
&lt;li>&lt;a href="www.ubuntu.com">Ubuntu&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.linuxmint.com/" target="_blank" rel="noopener">Mint&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://en.opensuse.org/Main_Page" target="_blank" rel="noopener">OpenSuse&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://fedoraproject.org/" target="_blank" rel="noopener">Fedora&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="http://www.ros.org/" target="_blank" rel="noopener">ROS&lt;/a> –&lt;/p>
&lt;blockquote>
&lt;p>The robot operating system is a flexible framework for writing robot software. It is a collection of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behavior across a wide variety of robotic platforms.&lt;/p>
&lt;/blockquote></description></item><item><title>Computer Vision and motion tracking software</title><link>https://open-neuroscience.com/post/computer-vision-and-motion-tracking-software/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/computer-vision-and-motion-tracking-software/</guid><description>&lt;p>Motion tracking can be really useful in neurosciences, for automatic measurements of behaviour, among other things. Here you’ll find a small list of tracking softwares or libraries used to build such softwares:&lt;/p>
&lt;p> &lt;/p>
&lt;p>&lt;strong>Complete softwares:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="http://ctrax.sourceforge.net/index.html" target="_blank" rel="noopener">Ctrax&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;figure style="width: 128px" class="wp-caption alignnone">[&lt;img src="https://i0.wp.com/ctrax.sourceforge.net/images/ctrax-logo2b_128.png?resize=128%2C128" alt="" width="128" height="128" data-recalc-dims="1" />](https://i0.wp.com/ctrax.sourceforge.net/images/ctrax-logo2b_128.png)&lt;figcaption class="wp-caption-text">taken from: http://ctrax.sourceforge.net/index.html&lt;/figcaption>&lt;/figure>&lt;/li>
&lt;ul>
&lt;li>
&lt;blockquote>
&lt;p>Ctrax is an open-source, freely available, machine vision program for estimating the positions and orientations of many walking flies, maintaining their individual identities over long periods of time. It was designed to allow high-throughput, quantitative analysis of behavior in freely moving flies.&lt;/ul>&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;p> &lt;/p>
&lt;ul>
&lt;li>The bio tracking project, designed for multiple object tracking, developed at Georgia tech:&lt;/li>
&lt;li>&lt;a href="http://www.bio-tracking.org/" target="_blank" rel="noopener">&lt;a href="http://www.bio-tracking.org/">http://www.bio-tracking.org/&lt;/a>&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;span class="embed-youtube" style="text-align:center; display: block;">&lt;/span>&lt;/p>
&lt;p> &lt;/p>
&lt;p> &lt;/p>
&lt;ul>
&lt;li>Community Core Vision: Built with computer vision and machine sensing in mind, they mention multi touch applications as one of their focus on the website.&lt;/li>
&lt;li>&lt;a href="http://ccv.nuigroup.com/">http://ccv.nuigroup.com/&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;span class="embed-youtube" style="text-align:center; display: block;">&lt;/span>&lt;/p>
&lt;p> &lt;/p>
&lt;p> &lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://derek.simkowiak.net/motion-tracking-with-python/" target="_blank" rel="noopener">Motion Tracking using python&lt;/a>: Independent developed software by Derek Simkowiak, in a project he ran a couple of years back with his daughter, to track Gerbills&lt;/li>
&lt;/ul>
&lt;p>&lt;span class="embed-youtube" style="text-align:center; display: block;">&lt;/span>&lt;/p>
&lt;p> &lt;/p>
&lt;p> &lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://personal.ee.surrey.ac.uk/Personal/Z.Kalal/tld.html" target="_blank" rel="noopener">Tracking-Learning-Detection&lt;/a>: Developed by &lt;a href="http://personal.ee.surrey.ac.uk/Personal/Z.Kalal/index.html" target="_blank" rel="noopener">Zdenek Kalal&lt;/a> this software intends to track pretty much anything (object determination can be done via mouse) in real time and to learn features from the object as tracking goes on.&lt;/li>
&lt;/ul>
&lt;p>&lt;span class="embed-youtube" style="text-align:center; display: block;">&lt;/span>&lt;/p>
&lt;p> &lt;/p>
&lt;p> &lt;/p>
&lt;ul>
&lt;li> &lt;a href="http://openvisionc.sourceforge.net/" target="_blank" rel="noopener">Open Vision Control&lt;/a>: Developed on top of OpenCV (see below) in Python, it is a general purpose tracking software with several applications&lt;/li>
&lt;/ul>
&lt;p>&lt;span class="embed-youtube" style="text-align:center; display: block;">&lt;/span>&lt;/p>
&lt;p> &lt;/p>
&lt;p> &lt;/p>
&lt;ul>
&lt;li>SwisTrack: Developed at EPFL, it is also a tracking system for multiple objects&lt;figure id="attachment_744" style="width: 300px" class="wp-caption aligncenter">&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://i0.wp.com/openeuroscience.com/wp-content/uploads/2014/04/800px-swistrack4-ubuntu.png" target="_blank" rel="noopener">&lt;img class="size-medium wp-image-744" src="https://i0.wp.com/openeuroscience.com/wp-content/uploads/2014/04/800px-swistrack4-ubuntu.png?resize=300%2C211" alt="From http://en.wikibooks.org/wiki/Swistrack" width="300" height="211" srcset="https://i0.wp.com/openeuroscience.com/wp-content/uploads/2014/04/800px-swistrack4-ubuntu.png?w=800 800w, https://i0.wp.com/openeuroscience.com/wp-content/uploads/2014/04/800px-swistrack4-ubuntu.png?resize=300%2C212 300w, https://i0.wp.com/openeuroscience.com/wp-content/uploads/2014/04/800px-swistrack4-ubuntu.png?resize=768%2C541 768w" sizes="(max-width: 300px) 100vw, 300px" data-recalc-dims="1" />&lt;/a>&lt;figcaption class="wp-caption-text">From &lt;a href="http://en.wikibooks.org/wiki/Swistrack">http://en.wikibooks.org/wiki/Swistrack&lt;/a>&lt;/figcaption>&lt;/figure>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://infoscience.epfl.ch/record/85929">http://infoscience.epfl.ch/record/85929&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://infoscience.epfl.ch/record/125704">http://infoscience.epfl.ch/record/125704&lt;/a>&lt;/li>
&lt;/ul>
&lt;p> &lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0042247" target="_blank" rel="noopener">Tracking software for Drosophila&lt;/a>, by Colomb &lt;em>et al&lt;/em>.&lt;/li>
&lt;/ul>
&lt;p> &lt;/p>
&lt;p>&lt;strong>Computer vision/tracking libraries:&lt;/strong>&lt;/p>
&lt;p>&lt;a href="http://opencv.org/" target="_blank" rel="noopener">Open CV&lt;/a> is a library for machine learning and computer vision. It is written for different computer languages and different operational systems.&lt;/p>
&lt;blockquote>
&lt;p>The library has more than 2500 optimized algorithms, which includes a comprehensive set of both classic and state-of-the-art computer vision and machine learning algorithms. These algorithms can be used to detect and recognize faces, identify objects, classify human actions in videos, track camera movements, track moving objects, extract 3D models of objects, produce 3D point clouds from stereo cameras, stitch images together to produce a high resolution image of an entire scene, find similar images from an image database, remove red eyes from images taken using flash, follow eye movements, recognize scenery and establish markers to overlay it with augmented reality, etc.&lt;/p>
&lt;/blockquote>
&lt;p> &lt;/p>
&lt;p>&lt;a href="http://www.simplecv.org/" target="_blank" rel="noopener">Simple CV&lt;/a> is a framework that tries to simplify the development of software that require computer vision/machine learning, since a lot of researchers have the necessity of building on such concepts, but sometimes don’t have the time/training necessary to do so.&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>PySpace</title><link>https://open-neuroscience.com/post/pyspace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pyspace/</guid><description>&lt;p>&lt;a href="https://pyspace.github.io/pyspace/" target="_blank" rel="noopener">PySpace&lt;/a> is a signal processing and classificiation environment for Python.&lt;/p>
&lt;p>Modular software for processing of large data streams that has been specifically designed to enable distributed execution and empirical evaluation of signal processing chains. Various signal processing algorithms are available within the software, from finite impulse response filters over data-dependent spatial filters (e.g. CSP, xDAWN) to established classifiers (e.g. SVM, LDA). pySPACE incorporates the concept of node and node chains of the Modular Toolkit for Data Processing (MDP) framework.&lt;/p>
&lt;p>A paper about PySpace can be found &lt;a href="http://journal.frontiersin.org/article/10.3389/fninf.2013.00040/full" target="_blank" rel="noopener">here&lt;/a>&lt;/p></description></item><item><title>Python for Neurosciences (Frontiers collection)</title><link>https://open-neuroscience.com/post/python-for-neuroscience-frontiers-collection/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/python-for-neuroscience-frontiers-collection/</guid><description>&lt;p>Frontiers has created not one but two nice collections about open source software for neurosciences written in Python.&lt;/p>
&lt;p>&lt;a href="http://journal.frontiersin.org/researchtopic/8/python-in-neuroscience" target="_blank" rel="noopener">Here is collection 1&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://journal.frontiersin.org/researchtopic/1591/python-in-neuroscience-ii" target="_blank" rel="noopener">Here is collection 2&lt;/a>&lt;/p>
&lt;p>In these collections the readers will find a lot of nice resources, ranging from stimulus generation, to data formatting and analysis.&lt;/p></description></item><item><title>Simulations</title><link>https://open-neuroscience.com/post/simulation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/simulation/</guid><description>&lt;div id="pl-1067" class="panel-layout" >
&lt;div id="pg-1067-0" class="panel-grid panel-no-style" >
&lt;div id="pgc-1067-0-0" class="panel-grid-cell" >
&lt;div id="panel-1067-0-0-0" class="so-panel widget widget_sow-editor panel-first-child panel-last-child" data-index="0" >
&lt;div class="so-widget-sow-editor so-widget-sow-editor-base">
&lt;div class="siteorigin-widget-tinymce textwidget">
&lt;p>
Ever thought about playing with a virtual worm? or interacting with a simulated bee brain? Sounds interesting no? These are just two projects that offer anyone the opportunity to play around with brain/neuronal simulations and models. Some of them are hardware based, and some completely software:
&lt;/p>
&lt;pre>&lt;code> &amp;lt;p&amp;gt;
&amp;lt;a href=&amp;quot;http://openeuroscience.com/open-source-simulations-and-models/open-worm/&amp;quot;&amp;gt;OpenWorm&amp;lt;/a&amp;gt;
&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;
&amp;lt;a href=&amp;quot;http://openeuroscience.com/open-source-simulations-and-models/green-brain/&amp;quot;&amp;gt;GreenBrain&amp;lt;/a&amp;gt;
&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;
&amp;lt;a href=&amp;quot;http://openeuroscience.com/open-source-simulations-and-models/neuronsneuronsneurons/&amp;quot;&amp;gt;Neurons,Neurons,Neurons&amp;lt;/a&amp;gt;
&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;
&amp;lt;a href=&amp;quot;http://openeuroscience.com/open-source-simulations-and-models/big-neuron/&amp;quot;&amp;gt;Big Neuron&amp;lt;/a&amp;gt;
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;section class="blog">
&lt;div class="container">
&lt;div class="post-list" itemscope="" itemtype="http://schema.org/Blog">
{% for page in site.pages %}
{% for category in page.categories %}
{% if category == "Simulation" %}
{% include card_page.html %}
{% endif %}
{% endfor %}
{% endfor %}
&lt;pre>&lt;code>&amp;lt;/div&amp;gt;
&lt;/code>&lt;/pre>
&lt;/div>
&lt;/section></description></item><item><title>Vision Egg</title><link>https://open-neuroscience.com/post/vision-egg/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/vision-egg/</guid><description>&lt;p>&lt;a href="http://visionegg.org/" target="_blank" rel="noopener">Vision Egg&lt;/a> is a Python library for generating visual stimuli.&lt;/p>
&lt;p>In more detail, it is a high level interface in between Python and OpenGL, and can use inexpensive consumer grade graphics cards to generate precise visual stimuli. A paper with more details can be found here &lt;a href="http://journal.frontiersin.org/article/10.3389/neuro.11.004.2008/full">http://journal.frontiersin.org/article/10.3389/neuro.11.004.2008/full&lt;/a>&lt;/p></description></item></channel></rss>
<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Open Neuroscience</title><link>https://open-neuroscience.com/</link><atom:link href="https://open-neuroscience.com/index.xml" rel="self" type="application/rss+xml"/><description>Open Neuroscience</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>CC BY SA 4.0</copyright><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate><image><url>https://open-neuroscience.com/media/openneuroscience_logo_dark.svg</url><title>Open Neuroscience</title><link>https://open-neuroscience.com/</link></image><item><title>Example Page 1</title><link>https://open-neuroscience.com/courses/example/example1/</link><pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate><guid>https://open-neuroscience.com/courses/example/example1/</guid><description>&lt;p>In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p>
&lt;h2 id="tip-1">Tip 1&lt;/h2>
&lt;p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p>
&lt;p>Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p>
&lt;p>Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p>
&lt;p>Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p>
&lt;p>Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p>
&lt;h2 id="tip-2">Tip 2&lt;/h2>
&lt;p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p>
&lt;p>Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p>
&lt;p>Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p>
&lt;p>Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p>
&lt;p>Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p></description></item><item><title>Example Page 2</title><link>https://open-neuroscience.com/courses/example/example2/</link><pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate><guid>https://open-neuroscience.com/courses/example/example2/</guid><description>&lt;p>Here are some more tips for getting started with Academic:&lt;/p>
&lt;h2 id="tip-3">Tip 3&lt;/h2>
&lt;p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p>
&lt;p>Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p>
&lt;p>Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p>
&lt;p>Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p>
&lt;p>Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p>
&lt;h2 id="tip-4">Tip 4&lt;/h2>
&lt;p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p>
&lt;p>Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p>
&lt;p>Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p>
&lt;p>Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p>
&lt;p>Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p></description></item><item><title>Metrics</title><link>https://open-neuroscience.com/metrics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/metrics/</guid><description>&lt;h4 id="open-neuroscience-visitors-page-views-twitte-engagement-and-countries">Open Neuroscience visitors, page views, twitte engagement, and countries:&lt;/h4>
&lt;p>&lt;img src="world_map.png" alt="">&lt;/p>
&lt;p>&lt;img src="cumulative_views_tweet.png" alt="">&lt;/p>
&lt;hr>
&lt;p>Since May 2020 we are using &lt;a href="https://plausible.io" target="_blank" rel="noopener">plausible.io&lt;/a> for our analytics. This is an Open Source system that respect users privacy (does not use cookies and is GDPR compliant).&lt;/p>
&lt;p>We, of course, have our stats publicly available &lt;a href="https://plausible.io/open-neuroscience.com?period=12mo" target="_blank" rel="noopener">here&lt;/a>&lt;/p></description></item><item><title>Example Talk</title><link>https://open-neuroscience.com/talk/example/</link><pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate><guid>https://open-neuroscience.com/talk/example/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click on the &lt;strong>Slides&lt;/strong> button above to view the built-in slides feature.
&lt;/div>
&lt;/div>
&lt;p>Slides can be added in a few ways:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Create&lt;/strong> slides using Academic&amp;rsquo;s &lt;a href="https://sourcethemes.com/academic/docs/managing-content/#create-slides" target="_blank" rel="noopener">&lt;em>Slides&lt;/em>&lt;/a> feature and link using &lt;code>slides&lt;/code> parameter in the front matter of the talk file&lt;/li>
&lt;li>&lt;strong>Upload&lt;/strong> an existing slide deck to &lt;code>static/&lt;/code> and link using &lt;code>url_slides&lt;/code> parameter in the front matter of the talk file&lt;/li>
&lt;li>&lt;strong>Embed&lt;/strong> your slides (e.g. Google Slides) or presentation video on this page using &lt;a href="https://sourcethemes.com/academic/docs/writing-markdown-latex/" target="_blank" rel="noopener">shortcodes&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>Further talk details can easily be added to this page using &lt;em>Markdown&lt;/em> and $\rm \LaTeX$ math code.&lt;/p></description></item><item><title>BigPint Bioconductor package that makes BIG (RNA seq) data pint sized</title><link>https://open-neuroscience.com/post/bigpint_bioconductor_package_that_makes_big__rna_seq__data_pint_sized/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/bigpint_bioconductor_package_that_makes_big__rna_seq__data_pint_sized/</guid><description>&lt;p>The BigPint package can help examine any large multivariate dataset. However, we note that the example datasets and example code in this package consider RNA-sequencing datasets. If you are using this software for RNA-sequencing data, then it can help you confirm that the variability between your treatment groups is larger than that between your replicates and determine how various normalization techniques in popular RNA-sequencing analysis packages (such as edgeR, DESeq2, and limma) affect your dataset. Moreover, you can easily superimpose lists of differentially expressed genes (DEGs) onto your dataset to check that they show the expected patterns (large variability between treatment groups and small variability between replicates).&lt;/p>
&lt;ol>
&lt;li>
&lt;p>BigPint software website: &lt;a href="https://lindsayrutter.github.io/bigPint/">https://lindsayrutter.github.io/bigPint/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Article explaining the BigPint methodology: &lt;a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2968-1">https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2968-1&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Research article showcasing the BigPint software:
&lt;a href="https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-5767-1">https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-5767-1&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Article explaining the BigPint software: &lt;a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007912">https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007912&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Lindsay Rutter; Dianne Cook&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/lindsayrutter/bigPint">https://github.com/lindsayrutter/bigPint&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Anonymous&lt;/p>
&lt;hr></description></item><item><title>Computer-controlled dog treat dispenser</title><link>https://open-neuroscience.com/post/computer_controlled_dog_treat_dispenser/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/computer_controlled_dog_treat_dispenser/</guid><description>&lt;p>When performing canine operant conditioning studies, the delivery of the reward can be a limiting factor of the study. While there are a few commercially available options for automatically delivering rewards, they generally require manual input, such as using a remote control, in accordance with the experiment script. This means that human reaction times and transmission distances can cause interruptions to the flow of the experiment. The potential for development of non-supervised conditioning studies is limited by this same factor. To remedy this, we retrofitted an off-the-shelf treat dispenser with new electronics that allow it to be remotely controllable as well as act as an experiment computation, data storage, and networking center. We present a fully integrated dispenser driver board with a complementary Raspberry Pi. With rather simple modifications, the commercial treat dispenser can be modified into a computer-controlled dispenser for canine cognition experiments or for other forms of canine training or games.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Jeffrey R. Stevens; Walker Arce&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/unl-cchil/canine_treat_dispenser">https://github.com/unl-cchil/canine_treat_dispenser&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=veKvqE5ipu4">https://www.youtube.com/watch?v=veKvqE5ipu4&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Jeffrey R. Stevens&lt;/p>
&lt;hr></description></item><item><title>ERPLAB</title><link>https://open-neuroscience.com/post/erplab/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/erplab/</guid><description>&lt;p>ERPLAB Toolbox is a free, open-source Matlab package for analyzing ERP data. It is tightly integrated with EEGLAB Toolbox, extending EEGLAB’s capabilities to provide robust, industrial-strength tools for ERP processing, visualization, and analysis. A graphical user interface makes it easy for beginners to learn, and Matlab scripting provides enormous power for intermediate and advanced users.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Andrew X Stewart; Steve Luck&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/lucklab/erplab">https://github.com/lucklab/erplab&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Andrew X Stewart&lt;/p>
&lt;hr></description></item><item><title>PocketPCR - Pocket size USB powered PCR Thermo Cycler</title><link>https://open-neuroscience.com/post/pocketpcr___pocket_size_usb_powered_pcr_thermo_cycler/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pocketpcr___pocket_size_usb_powered_pcr_thermo_cycler/</guid><description>&lt;p>The PocketPCR is a so called thermocycler used to activate biological reactions. To do so the device raises and lowers the temperature of the liquid in the small tubes. The polymerase chain reaction (PCR) is a method widely used in molecular biology to make copies of a specific DNA segment. Applications of the technique include DNA cloning for sequencing, analysis of genetic fingerprints, amplification of ancient DNA and gene cloning.&lt;/p>
&lt;p>Simpler, smaller and more affordable. This ultra portable and compact thermocycler was designed with the goal to bring the cost down to affordable price for anyone who wants to do some Do-It-Your-Self biology with a DNA starter kit in his kitchen. The PocketPCR can be run from a simple USB power adapter. The device can be operated stand alone and all parameters can be set without the need of a computer or smartphone.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Urs Gaudenz; Yanwu Guo&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://gaudi.ch/PocketPCR/">http://gaudi.ch/PocketPCR/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://youtu.be/0tXwAAMCetI">https://youtu.be/0tXwAAMCetI&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Urs Gaudenz&lt;/p>
&lt;hr></description></item><item><title>TGAC Browser</title><link>https://open-neuroscience.com/post/tgac_browser/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/tgac_browser/</guid><description>&lt;p>The TGAC Browser is a Genomic Browser with novel rendering and annotation capabilities designed to overcome some shortcomings in available approaches. It was developed to visualize genome annotations from Ensembl Database Schema.&lt;/p>
&lt;p>The TGAC Browser provides the following features:&lt;/p>
&lt;p>Responsiveness: Client-side rendering and caching, based on JSON fragments generated by server logic, helps decrease the server load and improves user experience.&lt;/p>
&lt;p>User-friendly browser Interaction: Live data searching, track modification, and drag and drop selection; actions that are seamlessly powered by modern web browsers.&lt;/p>
&lt;p>Analysis Integration: The ability to carry out heavyweight analysis tasks, using tools such as BLAST, via a dedicated extensible daemon service.&lt;/p>
&lt;p>User Annotation: Users can edit annotations which can be persisted on the server, reloaded, and shared at a later date.&lt;/p>
&lt;p>Off-the-shelf Installation: The only prerequisites are a web application container, such as Jetty or Tomcat, and a standard Ensembl database to host sequence features.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Anil S. Thanki; Xingdong Bian; Robert P. Davey&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://browser.tgac.ac.uk/">http://browser.tgac.ac.uk/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>The home cage automated skilled reaching apparatus HASRA individualized training of group housed mice in a single pellet reaching task</title><link>https://open-neuroscience.com/post/the_home_cage_automated_skilled_reaching_apparatus_hasra_individualized_training_of_group_housed_mice_in_a_single_pellet_reaching_task/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/the_home_cage_automated_skilled_reaching_apparatus_hasra_individualized_training_of_group_housed_mice_in_a_single_pellet_reaching_task/</guid><description>&lt;p>We developed an automated apparatus and single pellet reach training paradigm for group housed mice within the home cage. This task allows individualized training progression and handedness of presentation for each mouse, minimizes stress induced by experimenter interaction, and enables experiments and data collection on a massive scale, previously impossible due to the demands of one-on-one behavioral testing. We demonstrate an optimized set of task parameters and the utility of this apparatus for assessing lesion-induced motor impairments. Herein, we provide an open-source, scalable, platform for training and assessing skilled reaching in mice with minimal need for experimenter handling that is equivalent to gold-standard, manual single pellet training paradigms.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Gilles Salameh; Matthew S. Jeffers; Junzheng Wu; Julian Pitney; Gergely Silasi&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/SilasiLab/HASRAv2">https://github.com/SilasiLab/HASRAv2&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Greg Silasi&lt;/p>
&lt;hr></description></item><item><title>neurolib</title><link>https://open-neuroscience.com/post/neurolib/</link><pubDate>Tue, 10 Nov 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/neurolib/</guid><description>&lt;p>Easy whole-brain modeling for computational neuroscientists 👩🏿‍🔬💻🧠&lt;/p>
&lt;p>In its essence, neurolib is a computational framework for simulating coupled neural mass models written in Python. It helps you to easily load structural brain scan data to construct brain networks where each node is a neural mass representing a single brain area. This network model can be used to simulate whole-brain dynamics.&lt;/p>
&lt;p>neurolib provides a simulation and optimization framework which allows you to easily implement your own neural mass model, simulate fMRI BOLD activity, analyse the results and fit your model to empirical data.&lt;/p>
&lt;p>With neurolib, our goal is to create a hackable framework for coders and focus on the simulation and optimization machinery. In this sense, neurolib is primarily a modern research tool and our main goal is to provide an accessible research framework. However, it is built with people in mind who are new to the field and just want to get going. We have made it as easy as possible to setup a simulation or to implement your own model and run your experiments.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Caglar Cakan; Nikola Jajcay&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/neurolib-dev/neurolib">https://github.com/neurolib-dev/neurolib&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Caglar Cakan&lt;/p>
&lt;hr></description></item><item><title>YAPiC</title><link>https://open-neuroscience.com/post/yapic/</link><pubDate>Tue, 10 Nov 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/yapic/</guid><description>&lt;p>With YAPiC you can make your own customized filter (also called model or classifier) to enhance a certain structure of your choice with a simple Python based command line interface, installable with pip. We have used YAPiC so far for analyzing various microscopy image data. Our experiments are mainly related to neurobiology, cell biology, histopathology and drug discovery (high content screening). However, YAPiC is a very generally applicable tool and can be applied to very different domains. It could be used for detecting e.g. forest regions in satellite images, clouds in landscape photographs or fried eggs in food photography.
Pixel classification in YAPiC is based on deep learning wit fully convolutional neural networks. Development of YAPiC started in 2015, when Ronneberger et al. presented a U-shaped fully convolutional neural network that was capable of solving highly challenging pixel classification tasks in bio images, such as tumor classification in histological slides or cell segmentation in brightfield DIC images.&lt;/p>
&lt;p>YAPiC was designed to make this new kind of AI powered pixel classification simply applicable, i.e feasible to use for a PhD student in his/her imaging project.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Christoph Moehl; Manuel Schoelling&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://yapic.github.io/yapic/">https://yapic.github.io/yapic/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Christoph Moehl&lt;/p>
&lt;hr></description></item><item><title>Computational Cognitive Neuroscience 4th Ed</title><link>https://open-neuroscience.com/post/computational_cognitive_neuroscience_4th_ed/</link><pubDate>Wed, 04 Nov 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/computational_cognitive_neuroscience_4th_ed/</guid><description>&lt;p>This is the 4th edition of the online, freely available textbook, providing a complete, self-contained introduction to the field of Computational Cognitive Neuroscience, where computer models of the brain are used to understand a wide range of cognitive functions, including perception, attention, motor control, learning, memory, language, and executive function.&lt;/p>
&lt;p>The first part of this textbook develops a coherent set of computational and neural principles that capture the behavior of networks of interconnected neurons, and the second part applies these principles to understand the above-listed cognitive functions.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Randall C. O&amp;rsquo;Reilly; Yuko Munakata; Michael J. Frank; Thomas E. Hazy&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://compcogneuro.org">https://compcogneuro.org&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Randall C. O&amp;rsquo;Reilly&lt;/p>
&lt;hr></description></item><item><title>Emergent Neural Network Simulation Software</title><link>https://open-neuroscience.com/post/emergent_neural_network_simulation_software/</link><pubDate>Wed, 04 Nov 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/emergent_neural_network_simulation_software/</guid><description>&lt;p>Neural network simulation software written in Go and Python, for developing biologically-based but also computationally functional neural models. Features an interactive 3D interface for visualizing networks and data, and has many implemented models of a wide range of cognitive phenomena.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Randall C. O&amp;rsquo;Reilly&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/emer/emergent">https://github.com/emer/emergent&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Randall C. O&amp;rsquo;Reilly&lt;/p>
&lt;hr></description></item><item><title>Uncertainpy A python toolbox for uncertainty quantification and sensitivity analysis tailored towards computational neuroscience</title><link>https://open-neuroscience.com/post/uncertainpy_a_python_toolbox_for_uncertainty_quantification_and_sensitivity_analysis_tailored_towards_computational_neuroscience/</link><pubDate>Wed, 04 Nov 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/uncertainpy_a_python_toolbox_for_uncertainty_quantification_and_sensitivity_analysis_tailored_towards_computational_neuroscience/</guid><description>&lt;p>Uncertainpy is a python toolbox for uncertainty quantification and sensitivity analysis tailored towards computational neuroscience.&lt;/p>
&lt;p>Uncertainpy is model independent and treats the model as a black box where the model can be left unchanged. Uncertainpy implements both quasi-Monte Carlo methods and polynomial chaos expansions using either point collocation or the pseudo-spectral method. Both of the polynomial chaos expansion methods have support for the rosenblatt transformation to handle dependent input parameters.&lt;/p>
&lt;p>Uncertainpy is feature based, i.e., if applicable, it recognizes and calculates the uncertainty in features of the model, as well as the model itself. Examples of features in neuroscience can be spike timing and the action potential shape.&lt;/p>
&lt;p>Uncertainpy is tailored towards neuroscience models, and comes with several common neuroscience models and features built in, but new models and features can easily be implemented. It should be noted that while Uncertainpy is tailored towards neuroscience, the implemented methods are general, and Uncertainpy can be used for many other types of models and features within other fields.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Simen Tennøe; Geir Halnes; Gaute Einevoll&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/simetenn/uncertainpy">https://github.com/simetenn/uncertainpy&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Simen Tennøe&lt;/p>
&lt;hr></description></item><item><title>EmotiBit</title><link>https://open-neuroscience.com/post/emotibit/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/emotibit/</guid><description>&lt;p>EmotiBit is a wearable sensor to capture high-quality emotional, physiological, and movement data from just about anywhere on the body.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Sean Montgomery&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.emotibit.com/">https://www.emotibit.com/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=jbcL2jzyWj4&amp;amp;ab_channel=EmotiBit">https://www.youtube.com/watch?v=jbcL2jzyWj4&amp;amp;ab_channel=EmotiBit&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Sean Montgomery&lt;/p>
&lt;hr></description></item><item><title>BrainSMASH</title><link>https://open-neuroscience.com/post/brainsmash/</link><pubDate>Tue, 06 Oct 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/brainsmash/</guid><description>&lt;p>BrainSMASH is a Python-based framework for quantifying the significance of a brain map’s spatial topography in studies of large-scale brain organization. BrainSMASH was designed to generate synthetic brain maps with spatial autocorrelation (SA) matched to the SA of a target brain map.&lt;/p>
&lt;p>SA is a prominent and ubiquitous property of brain maps that violates the assumptions of independence/exchangeability that underlie many conventional statistical tests. Controlling for this property is therefore necessary to disambiguate meaningful topographic relationships from chance associations. To this end, BrainSMASH instantiates a generative null model for simulating surrogate brain maps, constrained by empirical data, that preserve the SA of cortical (surface-based), subcortical (volumetric), parcellated, and dense brain maps.&lt;/p>
&lt;p>BrainSMASH requires only two inputs: a brain map of interest, and a matrix of pairwise distances between elements of the brain map. How these inputs are derived is left to user discretion, though additional support has been provided for investigators working with HCP-compliant neuroimaging files. Specifically, BrainSMASH includes routines to generate two-dimensional Euclidean and geodesic distance matrices from surface geometry (GIFTI) files, and subcortical Euclidean distance matrices from CIFTI-format neuroimaging files.&lt;/p>
&lt;p>Detailed documentation for BrainSMASH can be found at &lt;a href="https://brainsmash.readthedocs.io/">https://brainsmash.readthedocs.io/&lt;/a>.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Joshua B. Burt; Markus Helmer; Maxwell Shinn; Alan Anticevic; John D. Murray&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/murraylab/brainsmash">https://github.com/murraylab/brainsmash&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Joshua B. Burt&lt;/p>
&lt;hr></description></item><item><title>Mouse VR</title><link>https://open-neuroscience.com/post/mouse_vr/</link><pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/mouse_vr/</guid><description>&lt;p>Harvey Lab miniaturized mouse VR rig for head-fixed virtual navigation and decision-making tasks.&lt;/p>
&lt;p>The VR setup is comprised of several independent assemblies:&lt;/p>
&lt;p>The screen assembly: a laser projector projects onto a parabolic screen surrounding the mouse. This is the basis for the visual virtual reality.&lt;/p>
&lt;p>Ball cup assembly: an air-supported 8&amp;quot; styrofoam ball that the mouse can run on, with associated ball cup, sensors, and electronics&lt;/p>
&lt;p>Reward delivery system and lick sensor: lick spout, liquid reward reservoir, solenoid, and associated electronics&lt;/p>
&lt;p>Enclosure: A box surrounding the behavioral setup.&lt;/p>
&lt;p>Each of these components is independent of the others: i.e. just the screen could be used in combination with a different treadmill and reward delivery system. The electronics for the ball sensors, reward delivery, and lick detection are all mounted on the same PCB. If only one or two of these functions are needed, you do not need to populate the entire PCB.&lt;/p>
&lt;p>The screen assembly is designed to be small enough to be mounted within a standard 19&amp;quot; server rack, which could easily fit 3 rigs stacked vertically (or two + monitor and keyboard station).&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Noah Pettit; Matthias Minderer; Selmaan Chettih; Charlotte Arlt; Jim Bohnslav; Pavel Gorelick; Ofer Mazor; Christopher Harvey&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/HarveyLab/mouseVR">https://github.com/HarveyLab/mouseVR&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Noah Pettit&lt;/p>
&lt;hr></description></item><item><title>PsychRNN: An Accessible and Flexible Python Package for Training Recurrent Neural Network Models on Cognitive Tasks</title><link>https://open-neuroscience.com/post/psychrnn_an_accessible_and_flexible_python_package_for_training_recurrent_neural_network_models_on_cognitive_tasks/</link><pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/psychrnn_an_accessible_and_flexible_python_package_for_training_recurrent_neural_network_models_on_cognitive_tasks/</guid><description>&lt;p>PsychRNN is designed for neuroscientists and psychologists who are interested in RNNs as models of cognitive function in the brain.&lt;/p>
&lt;p>Despite growing interest in RNNs as models of brain function, this approach poses relatively high barriers to entry to researchers, due to the technical know-how required for specialized deep learning software (e.g. TensorFlow or PyTorch) to train artificial neural network models.&lt;/p>
&lt;p>We designed PsychRNN with accessibility and flexibility as important goals.&lt;/p>
&lt;p>The frontend for users to define tasks and train RNNs uses only Python &amp;amp; NumPy, with no requirement for deep learning software.&lt;/p>
&lt;p>The backend, based on TensorFlow for model training, is readily extensible. This design allows for accessible high-level specification and parameterization of tasks and models, using only a few lines of Python.&lt;/p>
&lt;p>Modularity is central to PsychRNN&amp;rsquo;s design, to achieve flexibility in defining and parameterizing tasks and networks. This facilitates investigation of how task features (e.g. timing or input/output channels) shape the network solutions learned by the models.&lt;/p>
&lt;p>PsychRNN also provides support for implementation of neurobiologically motivated constraints on synaptic connectivity, such as: no autapses, structured connectivity (e.g. for multi-region RNNs), Dale&amp;rsquo;s principle (separate excitatory &amp;amp; inhibitory cells), and fixed nonplastic subset of synapses. Modularity enables implementation of curriculum learning, or task shaping. RNNs can be trained in closed-loop, with tasks progressively adjusted as behavioral performance improves. This is more similar to animal training, for investigation of how shaping impacts neural solutions.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Daniel B Ehrlich; Jasmine T Stone; David Brandfonbrener; Alexander Atanasov; John D Murray&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/murraylab/PsychRNN">https://github.com/murraylab/PsychRNN&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Jasmine Stone&lt;/p>
&lt;hr></description></item><item><title>FastTrack</title><link>https://open-neuroscience.com/post/fasttrack/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/fasttrack/</guid><description>&lt;p>FastTrack is an open-source cross-platform tracking software. Easy to install and easy to use, it can track a large variety of systems from active particles to animals, with a known or unknown number of objects. It can process movies from any quality on low-end to high-end computers.&lt;/p>
&lt;p>Two main features are implemented in the software:
- A fast and automatic tracking algorithm that can detect and track objects, conserving the objects' identities across the video recording.
- A manual tool to review the tracking where errors can be corrected rapidly and easily to achieve 100% accuracy with a minimum of efforts.&lt;/p>
&lt;p>FastTrack do not require coding abilities to be used. A developer documentation is available for users who want to embed FastTrack tracking algorithm directly inside their projects.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Benjamin Gallois&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/FastTrackOrg/FastTrack">https://github.com/FastTrackOrg/FastTrack&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="http://www.fasttrack.sh/UserManual/docs/assets/example_vid.webm">http://www.fasttrack.sh/UserManual/docs/assets/example_vid.webm&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Benjamin Gallois&lt;/p>
&lt;hr></description></item><item><title>INCF KnowledgeSpace</title><link>https://open-neuroscience.com/post/incf_knowledgespace/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/incf_knowledgespace/</guid><description>&lt;p>KnowledgeSpace aims to be a globally-used, community-based, data-driven encyclopedia for neuroscience that links brain research concepts to data, models, and the literature that support them. Further it aims to serve as a framework where large-scale neuroscience projects can expose their data to the neuroscience community-at-large. KnowledgeSpace is a framework that combines general descriptions of neuroscience concepts found in wikipedia with more detailed content from NeuroLex. It then integrates the content from those two sources with the latest neuroscience citations found in PubMed and data found in some of the world’s leading neuroscience repositories. KnowledgeSpace is a joint development between the Human Brain Project (HBP), the International Neuroinformatics Coordinating Facility (INCF), and the Neuroscience Information Framework (NIF).&lt;/p>
&lt;p>KnowledgeSpace is currently being developed with funding from the European Union&amp;rsquo;s Horizon 2020 Framework Programme for Research and Innovation under the Specific Grant Agreement No. 945539 (Human Brain Project SGA3), INCF, and Neuroscience Information Framework (NIF). Earlier development was supported by the European Union&amp;rsquo;s Horizon 2020 Framework Programme for Research and Innovation under the Framework Partnership Agreement No. 650003 (HBP FPA), INCF, NIF, and the Blue Brain Project.&lt;/p>
&lt;p>KS builds on a vocabulary service, populated with an integrated set of neuroscience ontologies with initial content coming from the Neuroscience Lexicon (NeuroLex), and the Brain Architecture Management System (BAMS). It links to an expanding set of data sources through the NIF federated search infrastructure.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>INCF team&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://knowledge-space.org">https://knowledge-space.org&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=GQCGZOPLED0">https://www.youtube.com/watch?v=GQCGZOPLED0&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Malin Sandström&lt;/p>
&lt;hr></description></item><item><title>INCF NeuroStars</title><link>https://open-neuroscience.com/post/incf_neurostars/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/incf_neurostars/</guid><description>&lt;p>Neurostars is an open source question and answer site that serves the INCF network and the global neuroscience community as a forum for knowledge exchange between neuroscience researchers at all levels of expertise, software developers, and infrastructure providers. Access to Neurostars is free to the neuroscience community thanks to the generous support of our sponsors.&lt;/p>
&lt;p>Neurostars is a forum for research questions, software, infrastructure, scientific societies, and virtual courses, including:&lt;/p>
&lt;ul>
&lt;li>Software developers and infrastructure providers in need of a mechanism in which to engage and provide support to user communities and those in need of a platform to integrate tutorials and discussion forums (via INCF TrainingSpace)&lt;/li>
&lt;li>Scientific societies in need of a forum for knowledge exchange and support between members&lt;/li>
&lt;li>Virtual course organizers in need of an archivable Q &amp;amp; A forum to support learning in their courses where access can be restricted to registered participants with automated notifications and the ability to have multiple, linked subdiscussion groups&lt;/li>
&lt;li>Individuals in need of sharing ideas, asking questions and finding answers, and helping others within neuroscience without any cost&lt;/li>
&lt;/ul>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>INCF team&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://neurostars.org">https://neurostars.org&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=veVD1olyaW4">https://www.youtube.com/watch?v=veVD1olyaW4&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Malin Sandström&lt;/p>
&lt;hr></description></item><item><title>INCF Portal</title><link>https://open-neuroscience.com/post/incf_portal/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/incf_portal/</guid><description>&lt;p>The INCF portal is the guide to the INCF activities and its community resources.&lt;/p>
&lt;p>INCF advances data reuse and reproducibility in brain research by coordinating the development of Open, FAIR, and Citable tools and resources for neuroscience&lt;/p>
&lt;p>The global INCF network&lt;/p>
&lt;ul>
&lt;li>supports the development of neuroinformatics as a discipline&lt;/li>
&lt;li>moves neuroscience towards FORCE (a FAIR, Open, Research-object based, Citable Ecosystem)&lt;/li>
&lt;li>provides coordination of global neuroscience infrastructure through the development and endorsement of consensus-based standards and best practices&lt;/li>
&lt;li>trains scientists, administrators, and students in using neuroinformatics tools and methods&lt;/li>
&lt;/ul>
&lt;p>INCF provides materials, expertise, training, and standards and best practices for: 1) scientists seeking to improve their science through neuroinformatics, 2) infrastructure providers so they can do their jobs better and participate in the global network, and 3) those seeking to add their tools, services, and expertise to the INCF network.&lt;/p>
&lt;p>INCF currently has Governing and Associate Nodes spanning 4 continents, with an extended network comprising organizations, individual researchers, industry, and publishers. The INCF Secretariat supports the Nodes with outreach, project management, and administration of community-driven projects.&lt;/p>
&lt;p>You can join the INCF network: &lt;a href="https://www.incf.org/join-incf">https://www.incf.org/join-incf&lt;/a>&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>INCF team&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.incf.org">https://www.incf.org&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=teFJ_W6nVUY">https://www.youtube.com/watch?v=teFJ_W6nVUY&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Malin Sandström&lt;/p>
&lt;hr></description></item><item><title>INCF TrainingSpace</title><link>https://open-neuroscience.com/post/incf_trainingspace/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/incf_trainingspace/</guid><description>&lt;p>TrainingSpace is an online hub that aims to make neuroscience educational materials more accessible to the global neuroscience community developed by the Training and Education Committee composed of members from the INCF network, HBP, SfN, FENS, IBRO, IEEE, BD2K, and iNeuro Initiative. As a hub, TrainingSpace provides users with access to:&lt;/p>
&lt;ul>
&lt;li>Multimedia educational content from courses, conference lectures, and laboratory exercises from some of the world’s leading neuroscience institutes and societies&lt;/li>
&lt;li>Study tracks to facilitate self-guided study&lt;/li>
&lt;li>Tutorials on tools and open science resources for neuroscience research&lt;/li>
&lt;li>A Q&amp;amp;A forum&lt;/li>
&lt;li>A neuroscience encyclopedia that provides users with access to over 1.000.000 publicly available datasets as well as links to literature references and scientific abstracts&lt;/li>
&lt;/ul>
&lt;p>Topics currently included in TrainingSpace include: general neuroscience, clinical neuroscience, computational neuroscience, neuroinformatics, computer science, data science, and open science.&lt;/p>
&lt;p>All courses and conference lectures in TrainingSpace include a general description, topics covered, links to prerequisite courses if applicable, and links to software described in or required for the course, as well as links to the next lecture in the course or more advanced related courses. In addition to providing resources for students and researchers, TrainingSpace also provides resources for instructors, such laboratory exercises, open science services, and access to publicly available datasets and models.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>INCF team&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://training.incf.org">https://training.incf.org&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=nHnW0z2qeQE">https://www.youtube.com/watch?v=nHnW0z2qeQE&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Malin Sandström&lt;/p>
&lt;hr></description></item><item><title>Open Source Brain</title><link>https://open-neuroscience.com/post/open_source_brain/</link><pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open_source_brain/</guid><description>&lt;p>Open Source Brain, a platform for sharing, viewing, analyzing, and simulating standardized models from different brain regions and species.&lt;/p>
&lt;p>Model structure and parameters can be automatically visualized and their dynamical properties explored through browser-based simulations.&lt;/p>
&lt;p>Infrastructure and tools for collaborative interaction, development, and testing are also provided.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Open Source Brain contributors&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://www.opensourcebrain.org/">http://www.opensourcebrain.org/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Ankur Sinha (@ OSB/University College London)&lt;/p>
&lt;hr></description></item><item><title>MorphoPy: A python package for feature extraction of neural morphologies</title><link>https://open-neuroscience.com/post/morphopy_a_python_package_for_feature_extraction_of_neural_morphologies/</link><pubDate>Mon, 28 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/morphopy_a_python_package_for_feature_extraction_of_neural_morphologies/</guid><description>&lt;p>MorphoPy is an open software package written in Python3 that allows for visualization and processing of morphological reconstructions of neural data. It has been created to facilitate the translation from morphology graphs into descriptive features like density maps, morphometric statistics, and persistence diagrams for down-stream exploration and statistical analysis.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Sophie Laturnus; Adam von Daranyi; Ziwei Huang; Philipp Berens&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/berenslab/MorphoPy">https://github.com/berenslab/MorphoPy&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Sophie Laturnus&lt;/p>
&lt;hr></description></item><item><title>OpenCitations</title><link>https://open-neuroscience.com/post/opencitations/</link><pubDate>Sun, 27 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/opencitations/</guid><description>&lt;p>OpenCitations is an independent infrastructure organization for open scholarship dedicated to the publication of open bibliographic and citation data by the use of Semantic Web (Linked Data) technologies. It is also engaged in advocacy for open citations, particularly by organizing the Workshops for Open Citations and Scholarly Metadata, and in its role as a key founding member of the Initiative for Open Citations (I4OC) and the Initiative for Open Abstracts (I4OA).&lt;/p>
&lt;p>OpenCitations espouses fully the founding principles of Open Science. It complies with the FAIR data principles by Force11 that data should be findable, accessible, interoperable and re-usable, and it complies with the recommendations of I4OC that citation data in particular should be structured, separable, and open. On the latter topic, OpenCitations has recently published a formal definition of an Open Citation, and has launched a system for globally unique and persistent identifiers (PIDs) for bibliographic citations – Open Citation Identifiers (OCIs).&lt;/p>
&lt;p>The following publication is the canonical publication describing OpenCitations itself, as an infrastructure organization for open scholarship. The article describes OpenCitations and its datasets, tools, services and activities.&lt;/p>
&lt;p>Silvio Peroni, David Shotton (2020). OpenCitations, an infrastructure organization for open scholarship. Quantitative Science Studies, 1(1): 428-444. &lt;a href="https://doi.org/10.1162/qss_a_00023">https://doi.org/10.1162/qss_a_00023&lt;/a>&lt;/p>
&lt;p>The OpenCitations Data Model (OCDM) is the metadata model used for the data stored in all the OpenCitations' datasets, described in&lt;/p>
&lt;p>Marilena Daquino, Silvio Peroni, David Shotton (2020). The OpenCitations Data Model. Figshare. &lt;a href="https://doi.org/10.6084/m9.figshare.3443876">https://doi.org/10.6084/m9.figshare.3443876&lt;/a>.&lt;/p>
&lt;p>The largest dataset created by OpenCitations is COCI, the OpenCitations Index of Crossref open DOI-to-DOI citations, an RDF dataset containing details of all the citations that are specified by the open references to DOI-identified works present in Crossref. COCI does not index Crossref references that are not open, nor Crossref open references to entities that lack DOIs. The citations available in COCI are treated as first-class data entities, with accompanying properties including the citations timespan, modelled according to the OpenCitations Data Model.&lt;/p>
&lt;p>Currently, COCI contains information concerning 733,367,140 citations, and 59,455,917 bibliographic resources. COCI was most recently updated on 6 September 2020.&lt;/p>
&lt;p>For an in-depth description of COCI, see:&lt;/p>
&lt;p>Ivan Heibi, Silvio Peroni, David Shotton (2019). Software review: COCI, the OpenCitations Index of Crossref open DOI-to-DOI citations. Scientometrics, 121 (2): 1213-1228. &lt;a href="https://doi.org/10.1007/s11192-019-03217-6">https://doi.org/10.1007/s11192-019-03217-6&lt;/a>&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>David Shotton; Silvio Peroni&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://opencitations.net/">http://opencitations.net/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=-bCPS2iIdCc&amp;amp;feature=youtu.be">https://www.youtube.com/watch?v=-bCPS2iIdCc&amp;amp;feature=youtu.be&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
David Shotton&lt;/p>
&lt;hr></description></item><item><title>DIPY</title><link>https://open-neuroscience.com/post/dipy/</link><pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/dipy/</guid><description>&lt;p>DIPY is the paragon 3D/4D+ imaging library in Python. Contains generic methods for spatial normalization, signal processing, machine learning, statistical analysis and visualization of medical images. Additionally, it contains specialized methods for computational anatomy including diffusion, perfusion and structural imaging.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>&lt;a href="https://github.com/dipy/dipy/graphs/contributors">https://github.com/dipy/dipy/graphs/contributors&lt;/a>&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://dipy.org">https://dipy.org&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/channel/UCHnEuCRDGFOR5cfEo0nD3pw">https://www.youtube.com/channel/UCHnEuCRDGFOR5cfEo0nD3pw&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
anonymous&lt;/p>
&lt;hr></description></item><item><title>NeuroFedora</title><link>https://open-neuroscience.com/post/neurofedora/</link><pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/neurofedora/</guid><description>&lt;p>NeuroFedora is an initiative to provide a ready to use Fedora Linux based Free/Open source software platform for neuroscience. We believe that similar to Free software, science should be free for all to use, share, modify, and study. The use of Free software also aids reproducibility, data sharing, and collaboration in the research community. By making the tools used in the scientific process easier to use, NeuroFedora aims to take a step to enable this ideal.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>NeuroFedora volunteers @ the Fedora project&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://neuro.fedoraproject.org">https://neuro.fedoraproject.org&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Ankur Sinha (NeuroFedora SIG member)&lt;/p>
&lt;hr></description></item><item><title>Brainglobe atlas API</title><link>https://open-neuroscience.com/post/brainglobe_atlas_api/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/brainglobe_atlas_api/</guid><description>&lt;p>Many excellent brain atlases exist for different species. Some of them have an API (application programming interface) to allow users to interact with the data programmatically (e.g. the excellent Allen Mouse Brain Atlas), but many do not, and there is no consistent way to process data from multiple sources.&lt;/p>
&lt;p>The brainglobe atlas API (BG-AtlasAPI) deals with this problem by providing a common interface for programmers to download and process atlas data from multiple sources.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Adam Tyson; Federico Claudi; Luigi Petrucco&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/brainglobe/bg-atlasapi">https://github.com/brainglobe/bg-atlasapi&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Adam Tyson&lt;/p>
&lt;hr></description></item><item><title>culture_shock</title><link>https://open-neuroscience.com/post/culture_shock/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/culture_shock/</guid><description>&lt;p>Culture Shock is an open-source electroporator that was developed
through internet based collaboration, starting on the DIYbio Google
Group. It is an evolution on the traditional capacitive discharge
circuit topology, instead using pulsed induction to enable a
programmable waveform as well as reduce the size, weight, and cost of
the equipment. With all these benefits, we hope to reduce the burden
of laboratory consumables for DNA transformation and electrofusion
procedures, where chemical supplies are currently relied on. The added
benefit of programmability allows many cell types to be manipulated by
altering the voltage level, or even giving the voltage profile a
particular shape.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>John Griessen; Nathan McCorkle; Bryan Bishop&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/kanzure/culture_shock">https://github.com/kanzure/culture_shock&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Nathan McCorkle&lt;/p>
&lt;hr></description></item><item><title>WholeBrain</title><link>https://open-neuroscience.com/post/wholebrain/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/wholebrain/</guid><description>&lt;p>WholeBrain is a software to create anatomical maps. With a code base in C/C++ with wrappers to R and JavaScript/WASM.&lt;/p>
&lt;p>The purpose of WholeBrain is to provide a user-friendly and efficient way for scientist with minimal knowledge of computers to create anatomical maps and integrate this information with behavioral and physiological data for sharing on the web.&lt;/p>
&lt;p>WholeBrain is conceived and created by Daniel Fürth, CSHL.&lt;/p>
&lt;p>Quick question about something you think can be resolved quite fast? Then just go to the gitter room and chat with me: &lt;a href="https://gitter.im/tractatus/Lobby">https://gitter.im/tractatus/Lobby&lt;/a>&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Daniel Fürth&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/tractatus/wholebrain/">https://github.com/tractatus/wholebrain/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Daniel Fürth&lt;/p>
&lt;hr></description></item><item><title>brainrender</title><link>https://open-neuroscience.com/post/brainrender/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/brainrender/</guid><description>&lt;p>brainrender is a python package for the visualization of three dimensional neuro-anatomical data. It can be used to render data from publicly available data set (e.g. Allen Brain atlas) as well as user generated experimental data. The goal of brainrender is to facilitate the exploration and dissemination of neuro-anatomical data by providing a user-friendly platform to create high-quality 3D renderings.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Federico Claudi&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/BrancoLab/BrainRender">https://github.com/BrancoLab/BrainRender&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Federico Claudi&lt;/p>
&lt;hr></description></item><item><title>JASP</title><link>https://open-neuroscience.com/post/jasp/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/jasp/</guid><description>&lt;p>JASP is a cross-platform statistical software program with a state-of-the-art graphical user interface. The JASP interface allows you to conduct statistical analyses in seconds, and without having to learn programming or risking a programming mistake. JASP is open-source and free of charge, and we provide it as a service to the community. JASP is statistically inclusive as it offers both frequentist and Bayesian analysis methods.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>The JASP Team&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://jasp-stats.org/">https://jasp-stats.org/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=HxqB7CUA-XI">https://www.youtube.com/watch?v=HxqB7CUA-XI&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
EJ Wagenmakers&lt;/p>
&lt;hr></description></item><item><title>PiDose</title><link>https://open-neuroscience.com/post/pidose/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pidose/</guid><description>&lt;p>PiDose is an open-source tool for scientists performing drug administration experiments with mice. It allows for automated daily oral dosing of mice over long time periods (weeks to months) without the need for experimenter interaction and handling. To accomplish this, a small 3D-printed chamber is mounted adjacent to a regular mouse home-cage, with an opening in the cage to allow animals to freely access the chamber.&lt;/p>
&lt;p>The chamber is supported by a load cell, and does not contact the cage but sits directly next to the entrance opening. Prior to treatment, mice have a small RFID capsule implanted subcutaneously, and when they enter the chamber they are detected by an RFID reader. While the mouse is in the chamber, readings are taken from the load cell in order to determine the mouse&amp;rsquo;s bodyweight. At the opposite end of the chamber from the entrance, a nose-poke port accesses a spout which dispenses drops from two separate liquid reservoirs. This spout is wired to a capacitive touch sensor controller in order to detect licks, and delivers liquid drops in response to licking.&lt;/p>
&lt;p>Each day, an average weight is calculated for each mouse and a drug dosage is determined based on this. When a mouse licks at the spout it dispenses either regular drinking water or a drop of drug solution depending on if they have received their daily dosage or not. All components are controlled by a Python script running on a Raspberry Pi.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Cameron Woodard; Wissam Nasrallah; Bahram Samiei; Tim Murphy; Lynn Raymond&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://osf.io/rpyfm/">https://osf.io/rpyfm/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Cameron Woodard&lt;/p>
&lt;hr></description></item><item><title>pyControl</title><link>https://open-neuroscience.com/post/pycontrol/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pycontrol/</guid><description>&lt;p>pyControl is a system of open source hardware and software for controlling behavioural experiments, built around the Micropython microcontroller.&lt;/p>
&lt;p>pyControl makes it easy to program complex behavioural tasks using a clean, intuitive, and flexible syntax for specifying tasks as state machines. User created task definition files, written in Python, run directly on the microcontroller, supported by pyControl framework code. This gives users the power and simplicity of Python for specifying task behaviour, while allowing advanced users low-level access to the microcontroller hardware.&lt;/p>
&lt;p>pyControl hardware consists of a breakout board and a set of devices such as nose-pokes, audio boards, LED drivers, rotary encoders and stepper motor controllers that are connected to the breakout board to create behavioural setups.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Thomas Akam&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://pycontrol.readthedocs.io">https://pycontrol.readthedocs.io&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Thomas Akam&lt;/p>
&lt;hr></description></item><item><title>pyPhotometry</title><link>https://open-neuroscience.com/post/pyphotometry/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pyphotometry/</guid><description>&lt;p>pyPhotometry is system of open source, Python based, hardware and software for neuroscience fiber photometry data acquisition, consisting of an acquisition board and graphical user interface.&lt;/p>
&lt;p>pyPhotometry supports data aquisition from two analog and two digital inputs, and control of two LEDs via built in LED drivers with an adjustable 0-100mA output. The system supports time-division multiplexed illumination which allows fluoresence evoked by different excitation wavelengths to be independenly readout from a single photoreciever signal.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Thomas Akam&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://pyphotometry.readthedocs.io">https://pyphotometry.readthedocs.io&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Thomas Akam&lt;/p>
&lt;hr></description></item><item><title>SLEAP</title><link>https://open-neuroscience.com/post/sleap/</link><pubDate>Sun, 26 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/sleap/</guid><description>&lt;p>SLEAP (Social LEAP Estimates Animal Poses) is a multi-animal pose tracker based on deep learning. It is the successor of LEAP (Pereira et al., Nature Methods, 2019) and was designed to deal with the problem of tracking body landmarks of multiple freely interacting animals.&lt;/p>
&lt;p>Using deep learning, SLEAP trains neural network models from few user annotations to enable highly accurate body part localization, grouping and tracking. It supports multiple neural network architectures, including pretrained state-of-the-art models and lightweight customizable architectures. SLEAP has been used successfully to track mice, fruit flies, bees and other species of animals under a variety of experimental and imaging conditions.&lt;/p>
&lt;p>The software was designed to make it easy for users with no experience with deep learning through a fully featured GUI, as well as providing a rich functionality for advanced users seeking to develop a custom solution for their project. Tutorials and guides are available on our website (&lt;a href="https://sleap.ai">https://sleap.ai&lt;/a>) detailing steps for easy installation (Windows/Mac/Linux), labeling a new project, training on the locally or on the cloud, and tracking new data.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Talmo Pereira; Joshua Shaevitz; Mala Murthy&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://sleap.ai">https://sleap.ai&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=zwCf1pGnBUw">https://www.youtube.com/watch?v=zwCf1pGnBUw&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Talmo Pereira&lt;/p>
&lt;hr></description></item><item><title>Neuroimaging Informatics Tools and Resources Collaboratory (NITRC)</title><link>https://open-neuroscience.com/post/neuroimaging_informatics_tools_and_resources_collaboratory_nitrc/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/neuroimaging_informatics_tools_and_resources_collaboratory_nitrc/</guid><description>&lt;p>NeuroImaging Tools &amp;amp; Resources Collaboratory is an award-winning free web-based resource that offers comprehensive information on an ever expanding scope of neuroinformatics software and data. Since debuting in 2007, NITRC has helped the neuroscience community make further discoveries using software and data produced from research that used to end up lost or disregarded.&lt;/p>
&lt;p>NITRC also provides free access to data and enables pay-per-use cloud-based access to unlimited computing power, enabling worldwide scientific collaboration with minimal startup and cost. NITRC’s scientific focus includes: MR, PET/SPECT, CT, EEG/MEG, optical imaging, clinical neuroimaging, computational neuroscience, and imaging genomics software tools, data, and computational resources.&lt;/p>
&lt;p>With NITRC and its components—the Resources Registry (NITRC-R), Image Repository (NITRC-IR), and Computational Environment (NITRC-CE)—a researcher can obtain pilot or proof-of-concept data to validate a hypothesis for just a few dollars.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>NITRC Development Team&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://www.nitrc.org">http://www.nitrc.org&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
David Kennedy&lt;/p>
&lt;hr></description></item><item><title>PiVR</title><link>https://open-neuroscience.com/post/pivr/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pivr/</guid><description>&lt;p>PiVR is a system that allows experimenters to immerse small animals into virtual realities. The system tracks the position of the animal and presents light stimulation according to predefined rules, thus creating a virtual landscape in which the animal can behave. By using optogenetics, we have used PiVR to present fruit fly larvae with virtual olfactory realities, adult fruit flies with a virtual gustatory reality and zebrafish larvae with a virtual light gradient.&lt;/p>
&lt;p>PiVR operates at high temporal resolution (70Hz) with low latencies (&amp;lt;30 milliseconds) while being affordable (&amp;lt;US$500) and easy to build (&amp;lt;6 hours). Through extensive documentation (&lt;a href="http://www.PiVR.org">www.PiVR.org&lt;/a>), this tool was designed to be accessible to a wide public, from high school students to professional researchers studying systems neuroscience in academia.&lt;/p>
&lt;p>The project is open source (BSD-3) and the documented code written in the freely available programming language Python. We hope that PiVR will be adapted by advanced users for their particular needs, for example to create closed-loop experiments involving other sensory modalities (e.g., sound/vibration) through the use of PWM controllable devices. We envision PiVR to be used as the central module when creating virtual realities for a variety of sensory modalities. This ‘PiVR module’ takes care of detecting the animal and presenting the appropriate PWM signal that is then picked up by the PWM controllable device installed by the user, for example to produce a sound whenever an animal enters a pre-defined region.&lt;/p>
&lt;p>In short, PiVR is a powerful and affordable experimental platform allowing experimenters to create a wide array of virtual reality experiments. Our hope is that PiVR will be adapted by several labs to democratize closed-loop experiments and, by standardizing image quality and the animal detection algorithm, increase reproducibility.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>David Tadres; Matthieu Louis&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://www.PiVR.org">http://www.PiVR.org&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=w5tIG6B6FWo">https://www.youtube.com/watch?v=w5tIG6B6FWo&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
David Tadres&lt;/p>
&lt;hr></description></item><item><title>ReproNim: A Center for Reproducible Neuroimaging Computation</title><link>https://open-neuroscience.com/post/repronim_a_center_for_reproducible_neuroimaging_computation/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/repronim_a_center_for_reproducible_neuroimaging_computation/</guid><description>&lt;p>ReproNim&amp;rsquo;s goal is to improve the reproducibility of neuroimaging science and extend the value of our national investment in neuroimaging research, while making the process easier and more efficient for investigators.&lt;/p>
&lt;p>ReproNim delivers a reproducible analysis framework comprised of components that include: 1) data and software discovery; 2) implementation of standardized description of data, results and workflows; 3) development of execution options that facilitates operation in all computational environments; 4)
provision of training and education to the community.&lt;/p>
&lt;p>All components of the framework are intended to foster continued use and development of the reproducible and generalizable framework in neuroimaging research.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>The ReproNim Development Team&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/ReproNim">https://github.com/ReproNim&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
David Kennedy&lt;/p>
&lt;hr></description></item><item><title>Simple Behavioral Analysis (SimBA)</title><link>https://open-neuroscience.com/post/simple_behavioral_analysis_simba/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/simple_behavioral_analysis_simba/</guid><description>&lt;p>Several excellent computational frameworks exist that enable high-throughput and consistent tracking of freely moving unmarked animals. SimBA introduce and distribute a plug-and play pipeline that enables users to use these pose-estimation approaches in combination with behavioral annotation for the generation of supervised machine-learning behavioral predictive classifiers.&lt;/p>
&lt;p>SimBA was developed for the analysis of complex social behaviors, but includes the flexibility for users to generate predictive classifiers across other behavioral modalities with minimal effort and no specialized computational background.&lt;/p>
&lt;p>SimBA has a variety of extended functions for large scale batch video pre-processing, generating descriptive statistics from movement features, and interactive modules for user-defined regions of interest and visualizing classification probabilities and movement patterns.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Simon Nilsson: Jia Jie Chhong; Sophia Hwang; Nastacia Goodwin; Sam A Golden&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/sgoldenlab/simba">https://github.com/sgoldenlab/simba&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=Frq6mMcaHBc&amp;amp;list=PLi5Vwf0hhy1R6NDQJ3U28MOUJPfl2YWYl&amp;amp;index=2&amp;amp;t=0s">https://www.youtube.com/watch?v=Frq6mMcaHBc&amp;amp;list=PLi5Vwf0hhy1R6NDQJ3U28MOUJPfl2YWYl&amp;amp;index=2&amp;amp;t=0s&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Simon Nilsson&lt;/p>
&lt;hr></description></item><item><title>DataJoint</title><link>https://open-neuroscience.com/post/datajoint/</link><pubDate>Sun, 19 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/datajoint/</guid><description>&lt;p>DataJoint is an open-source library for managing and sharing scientific data pipelines in Python and Matlab.&lt;/p>
&lt;p>DataJoint allows creating and sharing computational data pipelines, which are defined as databases and analysis code for executing steps of activities for data collection and analysis. For example, many neuroscience studies are organized around DataJoint pipelines that start with basic information about the experiment, then ingest acquired data, and then perform processing, analysis, and visualization of results. The entire pipeline is diagrammed as a graph where each node is a table in the database with a corresponding class in the programming language; together they define the data structure and computations.&lt;/p>
&lt;p>DataJoint key features include:&lt;/p>
&lt;ul>
&lt;li>access to shared data pipelines in a relational database (MySQL-compatible) from Python, Matlab, or both.&lt;/li>
&lt;li>data integrity and consistency based founded on the relational data model and transactions&lt;/li>
&lt;li>an intuitive data definition language for pipeline design&lt;/li>
&lt;li>a diagramming notation to visualize data structure and dependencies&lt;/li>
&lt;li>a serialization framework: storing large numerical arrays and other scientific data in a language-independent way&lt;/li>
&lt;li>a flexible query language to retrieve precise cross-sections of data in a desired format&lt;/li>
&lt;li>automated execution of computational jobs, with built-in job management for distributed computing&lt;/li>
&lt;li>managed storage of large data objects outside the database&lt;/li>
&lt;/ul>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Dimitri Yatsenko; Edgar Walker; Fabian Sinz; Christopher Turner; Raphael Guzman&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://datajoint.io">https://datajoint.io&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Dimitri Yatsenko&lt;/p>
&lt;hr></description></item><item><title>Neurodata Without Borders</title><link>https://open-neuroscience.com/post/neurodata_without_borders/</link><pubDate>Sun, 19 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/neurodata_without_borders/</guid><description>&lt;p>Neurodata Without Borders is a data standard for neurophysiology, providing neuroscientists with a common standard to share, archive, use, and build analysis tools for neurophysiology data. NWB is designed to store a variety of neurophysiology data, including data from intracellular and extracellular electrophysiology experiments, data from optical physiology experiments, and tracking and stimulus data.&lt;/p>
&lt;p>The NWB team consists of neuroscientists and software developers who recognize that adoption of a unified data format is an important step toward breaking down the barriers to data sharing in neuroscience.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Andrew Tritt; Ryan Ly; Ben Dichter; Oliver Ruebel&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.nwb.org/">https://www.nwb.org/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://youtu.be/vfQsMyl0HQI">https://youtu.be/vfQsMyl0HQI&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Ben Dichter&lt;/p>
&lt;hr></description></item><item><title>OpenDrop Digital Microfluidics Platform</title><link>https://open-neuroscience.com/post/opendrop_digital_microfluidics_platform/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/opendrop_digital_microfluidics_platform/</guid><description>&lt;p>OpenDrop a modular, open source digital microfludics platform for research purposes. The device uses recent electro-wetting technology to control small droplets of liquids. Potential applications are lab on a chip devices for automating processes of digital biology.&lt;/p>
&lt;p>The OpenDrop V4 is modular electrowetting controller. The driver board is equipped with a connector that can host a circuit board cartridge with a 14×8 electrode array and 4 reservoirs. The liquids stay on a thin, hydrophobic foil laminated to the circuit board . The device is powered from USB though an included USB-C cable. All the voltage level are generated on the device and can be set with the built in soft menu from 150-300 Volts, DC or AC.&lt;/p>
&lt;p>OpenDrop Cartridges
The modular concepts of the OpenDrop V4 allows different configurations of cartridges: A gold coated electrode array board that can be coated with any dielectric layer and hydrophobic coating to make cartridges for topless digital microfluidic applications using readily available materials.The OpenDrop V4 Cartridge is a close-cell cartridge capable of “move, mix, split and reservoir dispensing”. The 4×8 electrode array and 4 reservoirs are laminated with a 15um ETFE foil, hydrophobic coating and ITO top cover.&lt;/p>
&lt;p>Programming
The OpenDrop V4 can be operated standalone and droplets can be moved through the built in joystick. A control software to program sequences of patterns from a computer is available as a free download. The board is also compatible with Adafruit Feather M0 controller boards and can be reprogrammed through the free Arduino IDE for custom specific applications. A sample code with the instruction to activate electrodes can be found on the OpenDrop GitHub.&lt;/p>
&lt;p>Features:&lt;/p>
&lt;ul>
&lt;li>Modular Cartridge System
- Connector to connect electrode board with up to 128 channels
- Gold coated 14×8 electrodes array, 2.75 mm x 2.75 mm in size, 4mil gaps&lt;/li>
&lt;li>Reservoirs – the new electrode array features 4 CT-type reservoirs&lt;/li>
&lt;li>AC and DC voltage generated on the device form USB power. True AC voltage driving capability (up to 300VAC).&lt;/li>
&lt;li>32bit AVR SAMD21G18 microprocessor with plenty of memory and power
- Electronic settings for voltage level, frequency and AC/DC selection
- Electronic reading of actual voltage level
- One connector for communication and powering (USB-C)
- Optical isolation of the high-voltage electronics trough opto-couplers and PhotoMOS
- New polyphonic audio amplifier and speaker (it’s a synth!)
- Cartridge presence detection
- Feedback amplifier
- Super flat OLED Display
- Nice joystick and 2 buttons, 3 LEDs
- Reset button
- All files open source, designed on KiCAD&lt;/li>
&lt;/ul>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>MSc Urs Gaudenz&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://www.gaudi.ch/OpenDrop/">http://www.gaudi.ch/OpenDrop/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=TY97QfWY6J4">https://www.youtube.com/watch?v=TY97QfWY6J4&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Anonymous&lt;/p>
&lt;hr></description></item><item><title>Stytra</title><link>https://open-neuroscience.com/post/stytra/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/stytra/</guid><description>&lt;p>Stytra, a flexible, open-source software package, written in Python and designed to cover all the general requirements involved in larval zebrafish behavioral experiments.&lt;/p>
&lt;p>It provides timed stimulus presentation, interfacing with external devices and simultaneous real-time tracking of behavioral parameters such as position, orientation, tail and eye motion in both freely-swimming and head-restrained preparations.&lt;/p>
&lt;p>Stytra logs all recorded quantities, metadata, and code version in standardized formats to allow full provenance tracking, from data acquisition through analysis to publication.&lt;/p>
&lt;p>The package is modular and expandable for different experimental protocols and setups. We also provide complete documentation with examples for extending the package to new stimuli and hardware, as well as a schema and parts list for behavioural setups.&lt;/p>
&lt;p>The software can be used in the context of calcium imaging experiments by interfacing with other acquisition devices.&lt;/p>
&lt;p>Our aims are to enable more laboratories to easily implement behavioral experiments, as well as to provide a platform for sharing stimulus protocols that permits easy reproduction of experiments and straightforward validation.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Vilim Stih; Luigi Petrucco&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/portugueslab/stytra">https://github.com/portugueslab/stytra&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Anonymous&lt;/p>
&lt;hr></description></item><item><title>An Open-source Anthropomorphic Robot Hand System: HRI Hand</title><link>https://open-neuroscience.com/post/an_open-source_anthropomorphic_robot_hand_system_hri_hand/</link><pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/an_open-source_anthropomorphic_robot_hand_system_hri_hand/</guid><description>&lt;p>We present an open-source anthropomorphic robot hand system called HRI hand. Our robot hand system was developed with a focus on the end-effector role of the collaborative robot manipulator. HRI hand is a research platform that can be built at a lower price (approximately $500, using only 3D printing) than commercial end-effectors. Moreover, it was designed as a two four-bar linkage for the under-actuated mechanism and provides pre-shaping motion similar to the human hand prior to touching an object. A URDF, python node, and rviz package is also provided to support the Robot Operating System (ROS). All hardware CAD design files and software source codes have been released and can be easily assembled and modified. The system proposed in this paper is developed with a five-finger structure, but each finger is modularized, so it can be developed with end-effectors of various shapes depending on the shape of the palm.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Hyeonjun Park; Donghan Kim&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/MrLacuqer/HRI-hand-firmware.git">https://github.com/MrLacuqer/HRI-hand-firmware.git&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://youtu.be/c5Ry3tl9FVw">https://youtu.be/c5Ry3tl9FVw&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Andre M Chagas&lt;/p>
&lt;hr></description></item><item><title>Open Source Automated Western Blot Processor</title><link>https://open-neuroscience.com/post/open_source_automated_western_blot_processor/</link><pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open_source_automated_western_blot_processor/</guid><description>&lt;p>Researchers in the biomedical area are always involved in methodologies comprising several processes that are repetitive and time-consuming; these researchers can take advantage of this time for other more important things.&lt;/p>
&lt;p>For many years, the trend for this type of problem has been automation. One of the routine methodologies used by researchers in broad areas of basic investigation is the Western blot technique.&lt;/p>
&lt;p>This method allows the detection through specific antibodies and the eventual quantification of the protein of interest in different biological lysates transferred onto a suitable membrane. This methodology involves several repetitive processes; one of them is the washing of blots after incubations with primary and secondary antibodies.&lt;/p>
&lt;p>The present device has been designed to automate this process at a low cost. Researchers must use several tools to carry out the same task at a much higher price, and more importantly, in a time-consuming process. Although it is designed for the Western blot, it can be optimized for other cyclic tasks.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Jorge Bravo-Martinez&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://dx.doi.org/10.17632/xcvckyc9mh.1">http://dx.doi.org/10.17632/xcvckyc9mh.1&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Jorge Bravo-Martinez&lt;/p>
&lt;hr></description></item><item><title>OpenNeuro</title><link>https://open-neuroscience.com/post/openneuro/</link><pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/openneuro/</guid><description>&lt;p>A free and open platform for sharing MRI, MEG, EEG, iEEG, and ECoG data.&lt;/p>
&lt;p>With OpenNeuro, you can:&lt;/p>
&lt;ul>
&lt;li>Browse and explore public datasets and analyses from a wide range of global contributors. Our collection of public datasets continues to grow as more and more become BIDS compatible.&lt;/li>
&lt;li>Download and use public data to create new datasets and run your own analyses.&lt;/li>
&lt;li>Privately share your data so your colleagues can view and edit your work.&lt;/li>
&lt;li>Publish your dataset where anyone can view, download, and run analyses on it.&lt;/li>
&lt;li>Create snapshots of your datasets to ensure past analyses remain reproducible as your datasets grow and change. Publish any of your snapshots while you continue work on your original data behind the scenes.&lt;/li>
&lt;li>Explore your published OpenNeuro dataset using BrainLife&amp;rsquo;s computing network. Utilize their community driven apps to run a variety of analysis and processing software in the browser.&lt;/li>
&lt;/ul>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Russell A. Poldrack; Krzysztof Jacek Gorgolewski&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://openneuro.org/">https://openneuro.org/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=FK_c1x1Pilk">https://www.youtube.com/watch?v=FK_c1x1Pilk&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Elizabeth DuPre&lt;/p>
&lt;hr></description></item><item><title>Small cost efficient 3D printed peristaltic pumps</title><link>https://open-neuroscience.com/post/small_cost_efficient_3d_printed_peristaltic_pumps/</link><pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/small_cost_efficient_3d_printed_peristaltic_pumps/</guid><description>&lt;p>The project overall aim is to provide cost efficient solution to drive microfluidics systems for e.g. cell culture and organ on a chip applications. Pumps, valves and other accessories are ofter expensive to buy or very expensive to custom made. The 8-channel FAST pump is a 3D printed pump that uses some off the shelf parts (steel pins and ball bearings) and is easily fabricated and assembled. A step by step protocol is published (&lt;a href="https://www.sciencedirect.com/science/article/pii/S2468067220300249)">https://www.sciencedirect.com/science/article/pii/S2468067220300249)&lt;/a>. The link to the picture is from this publication. The pump is far cheaper and smaller than commercial pumps and still has at least as good as or better pump performance. The 8-channel pump is excellent for use in parallel cell culture applications.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Alexander Jönsson; Arianna Toppi; Martin Dufva&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.sciencedirect.com/science/article/pii/S2468067220300249">https://www.sciencedirect.com/science/article/pii/S2468067220300249&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Martin Dufva&lt;/p>
&lt;hr></description></item><item><title>Stackable titre plates for organ on a chip applications</title><link>https://open-neuroscience.com/post/stackable_titre_plates_for_organ_on_a_chip_applications/</link><pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/stackable_titre_plates_for_organ_on_a_chip_applications/</guid><description>&lt;p>Organ on a chip is typically difficult to achieve due to large technical challenges such as fabrication of chips and systems and biological challenges such as co-culture of cells. In this project we have developed a system to stack 12 well plates inserts on top of each other where each plate holds a tissue. We illustrated this approach by creating an intestine model in the top plate, a blood vessel model in the middle plate and a liver model in the lower plate. The respective insert contain specific cell types are developed independently and just before use, the plates are stacked on top of each other where in this case, the complete assembly models the first pass metabolism. The independent development circumvent long term co-culture and medium incompatibilities. The plates are 3D printed in a biocompatible resin and modified with a disc of gelatine where cell are cultured either on top (epithelial and endothelial cells) or inside (hepatocytes). Hence there is diffusional communication from intestine to the lived provided that studies compounds can penetrate the respective barrier. The simple to use system can be modified with any cell type including stem cell organoids and likely neuronal cells.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Martin Dufva; Morten Jepsen; Anja Boisen; Line Hagner Nielsen; Chiara Mazzoni; Andreas Willumsen&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/adbi.201900289">https://onlinelibrary.wiley.com/doi/abs/10.1002/adbi.201900289&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Martin Dufva&lt;/p>
&lt;hr></description></item><item><title>cellfinder</title><link>https://open-neuroscience.com/post/cellfinder/</link><pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/cellfinder/</guid><description>&lt;p>cellfinder is software from the Margrie Lab at the Sainsbury Wellcome Centre for automated 3D cell detection and registration of whole-brain images (e.g. serial two-photon or lightsheet imaging).&lt;/p>
&lt;p>It’s a work in progress, but cellfinder can:&lt;/p>
&lt;ul>
&lt;li>Detect labelled cells in 3D in whole-brain images (many hundreds of GB)&lt;/li>
&lt;li>Register the image to an atlas (such as the Allen Mouse Brain Atlas)&lt;/li>
&lt;li>Segment the brain based on the reference atlas&lt;/li>
&lt;li>Calculate the volume of each brain area, and the number of labelled cells within it&lt;/li>
&lt;li>Transform everything into standard space for analysis and visualisation&lt;/li>
&lt;/ul>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Adam Tyson&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/SainsburyWellcomeCentre/cellfinder">https://github.com/SainsburyWellcomeCentre/cellfinder&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Adam Tyson&lt;/p>
&lt;hr></description></item><item><title>DeepLabCut</title><link>https://open-neuroscience.com/post/deeplabcut/</link><pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/deeplabcut/</guid><description>&lt;p>DeepLabCut™ is an efficient method for 3D markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results (i.e. you can match human labeling accuracy) with minimal training data (typically 50-200 frames). We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors.&lt;/p>
&lt;p>The package is open source, fast, robust, and can be used to compute 3D pose estimates. Please see the original paper and the latest work below. This package is collaboratively developed by the Mathis Group &amp;amp; Mathis Lab at EPFL/Harvard.&lt;/p>
&lt;p>The code is freely available and easy to install in a few clicks with Anaconda (and pypi). Please see instructions on deeplabcut.org. We also provide a very easy to use GUI interface, and a step-by-step user guide!&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Mackenzie Mathis, Alexander Mathis &amp;amp; contributors&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://deeplabcut.org/">http://deeplabcut.org/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/channel/UC2HEbWpC_1v6i9RnDMy-dfA">https://www.youtube.com/channel/UC2HEbWpC_1v6i9RnDMy-dfA&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Mackenzie Mathis&lt;/p>
&lt;hr></description></item><item><title>MNE-Python</title><link>https://open-neuroscience.com/post/mne-python/</link><pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/mne-python/</guid><description>&lt;p>MNE is a software package for processing electrophysiological signals primarily from magnetoencephalographic (MEG) and electroencephalographic (EEG) recordings, and more recently sEEG, ECoG and fNIRS. It provides a comprehensive solution for data preprocessing, forward modeling (with boundary element models), distributed source imaging, time–frequency analysis, non-parametric multivariate statistics, multivariate pattern analysis, and connectivity estimation. Importantly, this package allows all of these analyses to be applied in both sensor or source space. MNE is developed by an international team, with particular care for computational efficiency, code quality, and readability, as well as the common goal of facilitating reproducibility in neuroscience.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Alexandre Gramfort;Eric Larson;Denis Engemann;Daniel Strohmeier;Christian Brodbeck;Roman Goj;Mainak Jas;Teon Brooks;Lauri Parkkonen;Matti Hämäläinen;Jaakko Leppakangas;Jona Sassenhagen;Jean-Rémi King;Daniel McCloy;Marijn van Vliet;Clemens Brunner;Chris Holdgraf;Martin Luessi;Joan Massich;Guillaume Favelier;Andrew R. Dykstra;Mikolaj Magnuski;Stefan Appelhoff;Britta Westner;Richard Höchenberger;Robert Luke;Luke Bloy;Thomas Hartmann;Olaf Hauk;Adam Li&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://mne.tools">https://mne.tools&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Anonymous&lt;/p>
&lt;hr></description></item><item><title>Nilearn</title><link>https://open-neuroscience.com/post/nilearn/</link><pubDate>Sat, 11 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/nilearn/</guid><description>&lt;p>Nilearn is a Python module for fast and easy statistical learning on NeuroImaging data. It leverages the scikit-learn Python toolbox for multivariate statistics with applications such as predictive modelling, classification, decoding, or connectivity analysis.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>&lt;a href="https://github.com/orgs/nilearn/people">https://github.com/orgs/nilearn/people&lt;/a>&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://nilearn.github.io/">http://nilearn.github.io/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Anonymous&lt;/p>
&lt;hr></description></item><item><title>A Cartesian Coordinate Robot for Dispensing Fruit Fly Food</title><link>https://open-neuroscience.com/post/a_cartesian_coordinate_robot_for_dispensing_fruit_fly_food/</link><pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/a_cartesian_coordinate_robot_for_dispensing_fruit_fly_food/</guid><description>&lt;p>The fruit fly, Drosophila melanogaster, continues to be one of the most widely used model organisms in biomedical research.&lt;/p>
&lt;p>Though chosen for its ease of husbandry, maintaining large numbers of stocks of fruit flies, as done by many laboratories, is labour-intensive.&lt;/p>
&lt;p>One task which lends itself to automation is the production of the vials of food in which the flies are reared. Fly facilities typically have to generate several thousand vials of fly food each week to sustain their fly stocks.&lt;/p>
&lt;p>The system presented here combines a cartesian coordinate robot with a peristaltic pump. The design of the robot is based on an open hardware CNC (computer numerical control) machine, and uses belt and pulley actuators for the X and Y axes, and a leadscrew actuator for the Z axis.&lt;/p>
&lt;p>CNC motion and operation of the peristaltic pump are controlled by grbl (&lt;a href="https://github.com/gnea/grbl),">https://github.com/gnea/grbl),&lt;/a> an open source, embedded, G-code parser. Grbl is written in optimized C and runs directly on an Arduino. A Raspberry Pi is used to generate and stream G-code instructions to Grbl.&lt;/p>
&lt;p>A touch screen on the Raspberry Pi provides a graphical user interface to the system. Whilst the robot was built for the express purpose of filling vials of fly food, it could potentially be used for other liquid handling tasks in the laboratory.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Matt Wayland; Matthias Landgraf&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/WaylandM/fly-food-robot">https://github.com/WaylandM/fly-food-robot&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://doi.org/10.6084/m9.figshare.5175223.v1">https://doi.org/10.6084/m9.figshare.5175223.v1&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matt Wayland&lt;/p>
&lt;hr></description></item><item><title>Bonsai</title><link>https://open-neuroscience.com/post/bonsai/</link><pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/bonsai/</guid><description>&lt;p>Bonsai is a high-performance, easy to use, and flexible visual programming language for designing closed-loop neuroscience experiments combining physiology and behaviour data.&lt;/p>
&lt;p>Bonsai has allowed scientists with no previous programming experience to quickly develop their own experimental rigs and is also being increasingly used as a platform to integrate new open-source hardware and software from the experimental neuroscience community.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Gonçalo Lopes&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://bonsai-rx.org/">https://bonsai-rx.org/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Gonçalo Lopes&lt;/p>
&lt;hr></description></item><item><title>Ethoscopes</title><link>https://open-neuroscience.com/post/ethoscopes/</link><pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/ethoscopes/</guid><description>&lt;p>Ethoscopes are machines for high-throughput analysis of behavior in Drosophila and other animals.&lt;/p>
&lt;p>Ethoscopes provide a software and hardware solution that is reproducible and easily scalable.&lt;/p>
&lt;p>They perform, in real-time, tracking and profiling of behavior by using a supervised machine learning algorithm, are able to deliver behaviorally triggered stimuli to flies in a feedback-loop mode, and are highly customizable and open source.&lt;/p>
&lt;p>Ethoscopes can be built easily by using 3D printing technology and rely on Raspberry Pi microcomputers and Arduino boards to provide affordable and flexible hardware.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Quentin Geissmann; Luis Garcia; Giorgio Gilestro&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://lab.gilest.ro/ethoscope">http://lab.gilest.ro/ethoscope&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=5oWGBUMJON8&amp;amp;feature=emb_title">https://www.youtube.com/watch?v=5oWGBUMJON8&amp;amp;feature=emb_title&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Giorgio Gilestro&lt;/p>
&lt;hr></description></item><item><title>Bonvision</title><link>https://open-neuroscience.com/post/bonvision/</link><pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/bonvision/</guid><description>&lt;p>BonVision is an open-source closed-loop visual environment generator developed by the Saleem Lab and Solomon Lab at the UCL Institute of Behavioural Neuroscience in collaboration with NeuroGEARS.&lt;/p>
&lt;p>BonVision’s key features include:&lt;/p>
&lt;pre>&lt;code>Naturally closed-loop system based on reactive coding of the Bonsai framework
Handles 2D and 3D stimuli with equal ease
Visual environment generated independent of display configuration
Graphical programming language of the Bonsai framework
Can be used for Augmented Reality, Virtual Reality or 2D visual stimuli
Does not require the observer to be in a fixed position
&lt;/code>&lt;/pre>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Bonvision&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://bonvision.github.io">http://bonvision.github.io&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Poseidon</title><link>https://open-neuroscience.com/post/poseidon/</link><pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/poseidon/</guid><description>&lt;p>The Poseidon is an open-source syringe pump and microscope system. It uses 3D printed parts and common components that can be easily purchased. It can be used in microfluidics experiments or other applications. You can assemble it in a short-time for under $400. The system is modular and highly customizable. Examples of applications are: control the chemical environment of a bioreactor, purify proteins and precisely add reagents to chemical reactions over time.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Pachter Lab&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://pachterlab.github.io/poseidon">https://pachterlab.github.io/poseidon&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>DeepLabStream</title><link>https://open-neuroscience.com/post/deeplabstream/</link><pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/deeplabstream/</guid><description>&lt;p>DeepLabStream is a python based multi-purpose tool that enables the realtime tracking of animals and manipulation of experiments. Our toolbox is adapted from the previously published DeepLabCut (Mathis et al., 2018) and expands on its core capabilities. DeepLabStreams core feature is the real-time analysis using any type of camera-based video stream (incl. multiple streams). Building onto that, we designed a full experimental closed-loop toolkit. It enables running experimental protocols that are dependent on a constant stream of bodypart positions and feedback activation of several input/output devices. It&amp;rsquo;s capabilities range from simple region of interest (ROI) based triggers to headdirection or behavior dependent stimulation.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Schwarz Neurocon Lab&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/SchwarzNeuroconLab/DeepLabStream">https://github.com/SchwarzNeuroconLab/DeepLabStream&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>LED Zappelin'</title><link>https://open-neuroscience.com/post/led_zappelin/</link><pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/led_zappelin/</guid><description>&lt;p>Two-photon (2P) microscopy is a cornerstone technique in neuroscience research. However, combining 2P imaging with spectrally arbitrary light stimulation can be challenging due to crosstalk between stimulation light and fluorescence detection. To overcome this limitation, we present a simple and low-cost electronic solution based on an ESP32 microcontroller and a TLC5947 LED driver to rapidly time-interleave stimulation and detection epochs during scans. Implemented for less than $100, our design can independently drive up to 24 arbitrary spectrum LEDs to meet user requirements. We demonstrate the utility of our stimulator for colour vision experiments on the in vivo tetrachromatic zebrafish retina and for optogenetic circuit mapping in Drosophila.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Maxime Zimmermann; Andre Maia Chagas; Philipp Bartel; Sinzi Pop, Lucia Pierto Godino; Tom Baden&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/BadenLab/LED-Zappelin">https://github.com/BadenLab/LED-Zappelin&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Maxime Zimmermann&lt;/p>
&lt;hr></description></item><item><title>SpikeInterface</title><link>https://open-neuroscience.com/post/spikeinterface/</link><pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/spikeinterface/</guid><description>&lt;p>SpikeInterface is a unified Python framework for spike sorting. With its high-level API, it is designed to be accessible and easy to use, allowing users to build full analysis pipelines for spike sorting (reading-writing (IO) / preprocessing / spike sorting / postprocessing / validation / curation / comparison / visualization) with a few lines of code.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Alessio Buccino*; Cole Hurwitz*; Samuel Garcia; Jeremy Magland; Josh Siegle; Matthias Hennig&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/SpikeInterface/spikeinterface">https://github.com/SpikeInterface/spikeinterface&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=nWJGwFB7oII">https://www.youtube.com/watch?v=nWJGwFB7oII&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Alessio Buccino&lt;/p>
&lt;hr></description></item><item><title>Addgene's AAV Data Hub</title><link>https://open-neuroscience.com/post/addgenes_aav_data_hub/</link><pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/addgenes_aav_data_hub/</guid><description>&lt;p>AAV are versatile tools used by neuroscientists for expression and manipulation of neurons. Many scientists have benefited from the high-quality, ready-to-use AAV prep service from Addgene, a nonprofit plasmid repository. However, it can be challenging to determine which AAV tool and techniques are best to use for an experiment. Scientists also may have questions about how much virus to inject or which serotype or promoter should be used to target the desired neuron or brain region. To help scientists answer these questions, Addgene launched an open platform called the AAV Data Hub (&lt;a href="https://datahub.addgene.org/aav/">https://datahub.addgene.org/aav/&lt;/a>) which allows researchers to easily share practical experimental details with the scientific community (AAV used, in vivo model used, injection site, injection volumes, etc.). The goal of this platform is to help scientists find the best AAV tool for their experiments by reviewing combined data from a broad range of research labs. The AAV Data Hub launched in late 2019 and over 100 experiments have since been contributed to this project. The dataset includes details and images from experiments conducted in six different species and several different expression sites.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Addgene&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://datahub.addgene.org/aav/">https://datahub.addgene.org/aav/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=ZPKdr1RdtGI&amp;amp;feature=youtu.be">https://www.youtube.com/watch?v=ZPKdr1RdtGI&amp;amp;feature=youtu.be&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Angela Abitua&lt;/p>
&lt;hr></description></item><item><title>FishCam</title><link>https://open-neuroscience.com/post/fishcam/</link><pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/fishcam/</guid><description>&lt;p>We describe the “FishCam”, a low-cost (500 USD) autonomous camera package to record videos and images underwater. The system is composed of easily accessible components and can be programmed to turn ON and OFF on customizable schedules. Its 8-megapixel camera module is capable of taking 3280 × 2464-pixel images and videos. An optional buzzer circuit inside the pressure housing allows synchronization of the video data from the FishCam with passive acoustic recorders. Ten FishCam deployments were performed along the east coast of Vancouver Island, British Columbia, Canada, from January to December 2019. Field tests demonstrate that the proposed system can record up to 212 h of video data over a period of at least 14 days. The FishCam data collected allowed us to identify fish species and observe species interactions and behaviors. The FishCam is an operational, easily-reproduced and inexpensive camera system that can help expand both the temporal and spatial coverage of underwater observations in ecological research. With its low cost and simple design, it has the potential to be integrated into educational and citizen science projects, and to facilitate learning the basics of electronics and programming.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Xavier Mouy&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.sciencedirect.com/science/article/pii/S2468067220300195">https://www.sciencedirect.com/science/article/pii/S2468067220300195&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>neuTube</title><link>https://open-neuroscience.com/post/neutube/</link><pubDate>Mon, 22 Jun 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/neutube/</guid><description>&lt;p>neuTube is an open source software for reconstructing neurons from fluorescence microscope images. It is easy to use and improves the efficiency of reconstructing neuron structures accurately. The framework combines 2D/3D visualization, semi-automated tracing algorithms, and flexible editing options that simplify the task of neuron reconstruction.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Ting Zhao&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.neutracing.com/">https://www.neutracing.com/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>OpenTrons</title><link>https://open-neuroscience.com/post/opentrons/</link><pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/opentrons/</guid><description>&lt;p>Today, biologists spend too much time pipetting by hand. We think biologists should have robots to do pipetting for them. People doing science should be free of tedious benchwork and repetitive stress injuries. They should be able to spend their time designing experiments and analyzing data.&lt;/p>
&lt;p>That&amp;rsquo;s why we started Opentrons.&lt;/p>
&lt;p>We make robots for biologists. Our mission is to provide the scientific community with a common platform to easily share protocols and reproduce each other&amp;rsquo;s results. Our robots automate experiments that would otherwise be done by hand, allowing our community to spend more time pursuing answers to some of the 21st century’s most important questions&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Opentrons&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://opentrons.com/about">https://opentrons.com/about&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/channel/UCvMRmXIxnHs3AutkVhuqaQg">https://www.youtube.com/channel/UCvMRmXIxnHs3AutkVhuqaQg&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Deep Cinac</title><link>https://open-neuroscience.com/post/deep_cinac/</link><pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/deep_cinac/</guid><description>&lt;p>Two-photon calcium imaging is now widely used to infer neuronal dynamics from changes in fluorescence of an indicator. However, state of the art computational tools are not optimized for the reliable detection of fluorescence transients from highly synchronous neurons located in densely packed regions such as the CA1 pyramidal layer of the hippocampus during early postnatal stages of development. Indeed,the latest analytical tools often lack proper benchmark measurements. To meet this challenge, we first developed a graphical user interface allowing for a precise manual detection of all calcium transients from imaged neurons based on the visualization of the calcium imaging movie. Then, we analyzed the movies using a convolutional neural network with an attention process and a bidirectional long-short term memory network. This method is able to reach human performance and offers a better F1 score (harmonic mean of sensitivity and precision) than CaImAn to infer neural activity in the developingCA1 without any user intervention. It also enables automatically identifying activity originating from GABAergic neurons. Overall, DeepCINAC offers a simple, fast and flexible open-source toolbox for processing a wide variety of calcium imaging datasets while providing the tools to evaluate its performance.&lt;/p>
&lt;p>See full text at &lt;a href="https://www.biorxiv.org/content/10.1101/803726v2.full.pdf">https://www.biorxiv.org/content/10.1101/803726v2.full.pdf&lt;/a>&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Julien Denis&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://gitlab.com/cossartlab/deepcinac">https://gitlab.com/cossartlab/deepcinac&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Fingertip laser sensor</title><link>https://open-neuroscience.com/post/fingertip_laser_sensor/</link><pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/fingertip_laser_sensor/</guid><description>&lt;p>&lt;a href="http://toychest.ai.uni-bremen.de/wiki/projects:fingertip#fingertip_laser_sensor" target="_blank" rel="noopener">The fingertip laser project&lt;/a> makes use of the sensor used in an Avago ADNS-9500 laser mouse, to improve the capabilities of robotic hands, giving them the capability to detect distance, surface type and slippage of grasped objects. Very elegant hack of a mouse sensor!&lt;/p></description></item><item><title>SciDraw</title><link>https://open-neuroscience.com/post/scidraw/</link><pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/scidraw/</guid><description>&lt;p>SciDraw is a free repository of high quality drawings of animals, scientific setups, and anything that might be useful for scientific presentations and posters.
We want this repository to be as open as possible, so do not require signing up to post a drawing. This however means that you won&amp;rsquo;t be able to edit your drawings after submission, so upload carefully!
The drawings on SciDraw are made by and for scientists. You are free to download, modify and use all drawings on the website.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Federico Claudi and Alex Harston&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://scidraw.io/">https://scidraw.io/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>UC2</title><link>https://open-neuroscience.com/post/uc2/</link><pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/uc2/</guid><description>&lt;p>The open-source optical toolbox UC2 [YouSeeToo] simplifies the process of building optical setups, by combining 3D-printed cubes, each holding a specific component (e.g. lens, mirror) on a magnetic square-grid baseplate. The use of widely available consumables and 3D printing, together with documentation and software, offers an extremely low-cost and accessible alternative for both education and research areas. In order to reduce the entry barrier, we provide a fully comprehensive toolbox called TheBOX. A paper describing the scientific application in detail can be found &lt;a href="https://www.biorxiv.org/content/10.1101/2020.03.02.973073v1" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Benedict Diederich; René Lachmann; Barbora Marsikova&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://useetoo.org">https://useetoo.org&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=ey4uEFEG6MY">https://www.youtube.com/watch?v=ey4uEFEG6MY&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Barbora Marsikova&lt;/p>
&lt;hr></description></item><item><title>Head-Mounted Mesoscope</title><link>https://open-neuroscience.com/post/head-mounted_mesoscope/</link><pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/head-mounted_mesoscope/</guid><description>&lt;p>The advent of genetically encoded calcium indicators, along with surgical preparations such as thinned skulls or refractive index matched skulls, have enabled mesoscale cortical activity imaging in head-fixed mice. Such imaging studies have revealed complex patterns of coordinated activity across the cortex during spontaneous behaviors, goal-directed behavior, locomotion, motor learning,and perceptual decision making. However, neural activity during unrestrained behavior significantly differs from neural activity in head-fixed animals. Whole-cortex imaging in freely behaving mice will enable the study of neural activity in a larger, more complex repertoire of behaviors not possible in head-fixed animals. Here we present the “Mesoscope,” a wide-field miniaturized, head-mounted fluorescence microscope compatible with transparent polymer skulls recently developed by our group. With afield of view of 8 mm x 10 mm and weighing less than 4 g, the Mesoscope can image most of the mouse dorsal cortex with resolution ranging from 39 to 56μm. Stroboscopic illumination with blue and green LEDs allows fort he measurement of both fluorescence changes due to calcium activity and reflectance signals to capture hemodynamic changes. We have used the Mesoscope to successfully record mesoscale calcium activity across the dorsal cortex during sensory-evoked stimuli, open field behaviors, and social interactions.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Biosensing and Biorobotics Lab&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.biorxiv.org/content/10.1101/2020.05.25.114892v1.full.pdf">https://www.biorxiv.org/content/10.1101/2020.05.25.114892v1.full.pdf&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Open Source Eye Tracking</title><link>https://open-neuroscience.com/post/open_source_eye_tracking/</link><pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open_source_eye_tracking/</guid><description>&lt;p>The purpose of this project is to convey a location in 3 dimensional space to a machine, hands free and in real time.&lt;/p>
&lt;p>Currently it is very difficult to control machines without making the user provide input with their hands. Additionally it can be very difficult to specify a location in space without a complex input device. This system provides a novel solution to this problem by allowing the user to specify a location simply by looking at it.&lt;/p>
&lt;p>Normally eyetracking solutions are prohibitively expensive and not open source, limiting their use for creators to integrate them into new projects. This solution is fully open source, easy to build and will provide a huge variety of options for makers interested in using this fascinating and powerful technology.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>John Evans&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://hackaday.io/project/153293-low-cost-open-source-eye-tracking">https://hackaday.io/project/153293-low-cost-open-source-eye-tracking&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Open Source Syringe Pump Controller</title><link>https://open-neuroscience.com/post/open_source_syringe_pump_controller/</link><pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open_source_syringe_pump_controller/</guid><description>&lt;p>Syringe pumps are a necessary piece of laboratory equipment that are used for fluid delivery in behavioral neuroscience laboratories. Many experiments provide rodents and primates with fluid rewards such as juice, water, or liquid sucrose. Current commercialized syringe pumps are not customizable and do not have the ability to deliver multiple volumes of fluid based on different inputs to the pump. Additionally, many syringe pumps are expensive and cannot be used in experiments with paired neurophysiological recordings due to electrical noise. We developed an open source syringe pump controller using commonly available parts. The controller adjusts the acceleration and speed of the motor to deliver three different volumes of fluid reward within one common time epoch. This syringe pump controller is cost effective and has been successfully implemented in rodent behavioral experiments with paired neurophysiological recordings in the rat frontal cortex while rats lick for different volumes of liquid sucrose rewards. Our syringe pump controller will enable new experiments to address the potential confound of temporal information in studies of reward signaling by fluid magnitude.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Laubach Lab&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/LaubachLab/OpenSourceSyringePump">https://github.com/LaubachLab/OpenSourceSyringePump&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Sample Rotator Mixer and Shaker</title><link>https://open-neuroscience.com/post/sample_rotator_mixer_and_shaker/</link><pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/sample_rotator_mixer_and_shaker/</guid><description>&lt;p>An open-source 3-D printable laboratory sample rotator mixer is developed here in two variants that allow users to opt for the level of functionality, cost saving and associated complexity needed in their laboratories. First, a laboratory sample rotator is designed and demonstrated that can be used for tumbling as well as gentle mixing of samples in a variety of tube sizes by mixing them horizontally, vertically, or any position in between. Changing the mixing angle is fast and convenient and requires no tools. This device is battery powered and can be easily transported to operate in various locations in a lab including desktops, benches, clean hoods, chemical hoods, cold rooms, glove boxes, incubators or biological hoods. Second, an on-board Arduino-based microcontroller is incorporated that adds the functionality of a laboratory sample shaker. These devices can be customized both mechanically and functionally as the user can simply select the operation mode on the switch or alter the code to perform custom experiments. The open source laboratory sample rotator mixer can be built by non-specialists for under US$30 and adding shaking functionality can be done for under $20 more. Thus, these open source devices are technically superior to the proprietary commercial equipment available on the market while saving over 90% of the costs.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>MOST&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.appropedia.org/Open_Source_Laboratory_Sample_Rotator_Mixer_and_Shaker">https://www.appropedia.org/Open_Source_Laboratory_Sample_Rotator_Mixer_and_Shaker&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=Ta2ACV1oIjI&amp;amp;feature=emb_logo">https://www.youtube.com/watch?v=Ta2ACV1oIjI&amp;amp;feature=emb_logo&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Automated Operant Conditioning</title><link>https://open-neuroscience.com/post/automated_operant_conditioning/</link><pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/automated_operant_conditioning/</guid><description>&lt;p>Operant conditioning (OC) is a classical paradigm and a standard technique used in experimental psychology in which animals learn to perform an action to achieve a reward. By using this paradigm, it is possible to extract learning curves and measure accurately reaction times (RTs). Both these measurements are proxy of cognitive capabilities and can be used to evaluate the effectiveness of therapeutic interventions in mouse models of disease. Here, we describe a fully 3D printable device that is able to perform OC on freely moving mice, while performing real-time tracking of the animal position. We successfully trained six mice, showing stereotyped learning curves that are highly reproducible across mice and reaching &amp;gt;70% of accuracy after 2 d of conditioning. Different products for OC are commercially available, though most of them do not provide customizable features and are relatively expensive. This data demonstrate that this system is a valuable alternative to available state-of-the-art commercial devices, representing a good balance between performance, cost, and versatility in its use.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Raffaele Mazziotti, Giulia Sagona, Leonardo Lupori, Virginia Martini and Tommaso Pizzorusso&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/raffaelemazziotti/oc_chamber">https://github.com/raffaelemazziotti/oc_chamber&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Heuristic Spike Sorting Tuner (HSST), a framework to determine optimal parameter selection for a generic spike sorting algorithm</title><link>https://open-neuroscience.com/post/heuristic_spike_sorting_tuner_hsst_a_framework_to_determine_optimal_parameter_selection_for_a_generic_spike_sorting_algorithm/</link><pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/heuristic_spike_sorting_tuner_hsst_a_framework_to_determine_optimal_parameter_selection_for_a_generic_spike_sorting_algorithm/</guid><description>&lt;p>Extracellular microelectrodes frequently record neural activity from more than one neuron in the vicinity of the electrode. The process of labeling each recorded spike waveform with the identity of its source neuron is called spike sorting and is often approached from an abstracted statistical perspective. However, these approaches do not consider neurophysiological realities and may ignore important features that could improve the accuracy of these methods. Further, standard algorithms typically require selection of at least one free parameter, which can have significant effects on the quality of the output. We describe a Heuristic Spike Sorting Tuner (HSST) that determines the optimal choice of the free parameters for a given spike sorting algorithm based on the neurophysiological qualification of unit isolation and signal discrimination. A set of heuristic metrics are used to score the output of a spike sorting algorithm over a range of free parameters resulting in optimal sorting quality. We demonstrate that these metrics can be used to tune parameters in several spike sorting algorithms. The HSST algorithm shows robustness to variations in signal to noise ratio, number and relative size of units per channel. Moreover, the HSST algorithm is computationally efficient, operates unsupervised, and is parallelizable for batch processing.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>David A. Bjanes; Lee B. Fisher; Robert A. Gaunt; Douglas J. Weber&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/davidbjanes/hsst">https://github.com/davidbjanes/hsst&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
David Bjanes&lt;/p>
&lt;hr></description></item><item><title>VocalMat</title><link>https://open-neuroscience.com/post/vocalmat/</link><pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/vocalmat/</guid><description>&lt;p>Mice emit ultrasonic vocalizations (USV) to transmit socially-relevant information. To detect and classify these USVs, here we describe the development of VocalMat. VocalMat is a software that uses image-processing and differential geometry approaches to detect USVs in audio files, eliminating the need for user-defined parameter tuning. VocalMat also uses computational vision and machine learning methods to classify USVs into distinct categories. In a dataset of &amp;gt;4,000 USVs emitted by mice, VocalMat detected more than &amp;gt;98% of the USVs and accurately classified ≈86% of USVs when considering the most likely label out of 11 different USV types. We then used Diffusion Maps and Manifold Alignment to analyze the probability distribution of USV classification among different experimental groups, providing a robust method to quantify and qualify the vocal repertoire of mice. Thus, VocalMat allows accurate and highly quantitative analysis of USVs, opening the opportunity for detailed and high-throughput analysis of this behavior.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Antonio H. O. Fonseca, Gustavo M. Santana, Sergio Bampi, Marcelo O Dietrich&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.dietrich-lab.org/vocalmat">https://www.dietrich-lab.org/vocalmat&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>BossDB</title><link>https://open-neuroscience.com/post/bossdb/</link><pubDate>Tue, 26 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/bossdb/</guid><description>&lt;p>BossDB is a volumetric database that lives in the AWS cloud. Hundreds of terabytes of electron microscopy, light microscopy, and x-ray tomography data are available for free download and study.&lt;/p>
&lt;p>Have a project you want to share with the world for free? Get in touch!
&lt;a href="https://twitter.com/thebossdb">https://twitter.com/thebossdb&lt;/a>&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>JHU|APL&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://bossdb.org/">https://bossdb.org/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Jordan Matelsky&lt;/p>
&lt;hr></description></item><item><title>Neuroanatomy and Behaviour</title><link>https://open-neuroscience.com/post/neuroanatomy_and_behaviour/</link><pubDate>Tue, 26 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/neuroanatomy_and_behaviour/</guid><description>&lt;p>Neuroanatomy and Behaviour (ISSN: 2652-1768) is a free open access journal for behavioural neuroscience and related fields. Powered by free open source software to eliminate costs and keep grant funds doing science. Scientist-run and non-profit.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Episteme Health Inc.&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://epistemehealth.com">https://epistemehealth.com&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Shaun Khoo&lt;/p>
&lt;hr></description></item><item><title>3D Slicer</title><link>https://open-neuroscience.com/post/3d_slicer/</link><pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/3d_slicer/</guid><description>&lt;p>3D Slicer is a software for medical image informatics, image processing, and three-dimensional visualization. It’s extremely powerful and versatile with plenty of different options. It is a great tool for volume rendering, registration, interactive segmentation of images and even offers the possibility of running Python scripts thought an embedded Python interpreter.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Ron Kikinis&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/Slicer/Slicer">https://github.com/Slicer/Slicer&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>Craniobot</title><link>https://open-neuroscience.com/post/craniobot/</link><pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/craniobot/</guid><description>&lt;p>The Craniobot is a cranial microsurgery platform that combines automated skull surface profiling with a computer numerical controlled (CNC) milling machine to perform a variety of cranial microsurgical procedures in mice. The Craniobot utilizes a low force contact sensor to profile the skull surface and uses this information to perform micrometer-scale precise milling operations within minutes. The procedure of removing the sub-millimeter thick mouse skull precisely without damaging the underlying brain can be technically challenging and often takes significant skill and practice. This can now be overcome using the Craniobot.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Mathew Rynes, Leila Ghanbari, Micheal Laroque, Greg Johnson, Daniel Sousa Schulman, Suhasa Kodandaramaiah&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.labmaker.org/products/craniobot">https://www.labmaker.org/products/craniobot&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Labmaker</title><link>https://open-neuroscience.com/post/labmaker/</link><pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/labmaker/</guid><description>&lt;p>LabMaker is a maker and assembly service for OPEN SCIENCE instruments. OPEN SCIENCE initiatives provide part lists or &amp;ldquo;Bill Of Materials&amp;rdquo; (BOM) for openly available scientific instruments. LabMaker bridges the gap between the BOM and the ready-to-use instrument for those not wanting to build by themselves. LabMaker is based in Berlin, Germany and ships worldwide. Berlin, as a city, is not only amongst the frontrunners for the title &amp;ldquo;start-up capital of Europe&amp;rdquo;, but also home to a large diversity of companies rooted in traditional precision manufacturing.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Labmaker&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.labmaker.org/">https://www.labmaker.org/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>MicroscoPy</title><link>https://open-neuroscience.com/post/microscopy/</link><pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/microscopy/</guid><description>&lt;p>An open-source, motorized, and modular microscope built using LEGO bricks, Arduino, Raspberry Pi and 3D printing. The microscope uses a Raspberry Pi mini-computer with an 8MP camera to capture images and videos. Stepper motors and the illumination are controlled using a circuit board comprising an Arduino microcontroller, six stepper motor drivers and a high-power LED driver. All functions can be controlled from a keyboard connected to the Raspberry Pi or a separate custom-built Arduino joystick connected to the mainboard. LEGO bricks are used to construct the main body of the microscope to achieve a modular and easy-to-assemble design concept.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Yuksel Temiz and IBM&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/IBM/MicroscoPy">https://github.com/IBM/MicroscoPy&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=PBSYnk9T4o4&amp;amp;feature=youtu.be">https://www.youtube.com/watch?v=PBSYnk9T4o4&amp;amp;feature=youtu.be&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Colaboratory</title><link>https://open-neuroscience.com/post/colaboratory/</link><pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/colaboratory/</guid><description>&lt;p>Colaboratory is a free Jupyter notebook environment that runs in the cloud. Your notebooks get stored on Google Drive. The great advantage is that you don’t have to install anything (however, for some features you need a Google account) on your system to use it. You can perform specific computations during data analysis with pre-installed Python libraries and gives you access to accelerated hardware for free (e.g. GPUs and TPUs).&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Google&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://colab.research.google.com/notebooks/intro.ipynb">https://colab.research.google.com/notebooks/intro.ipynb&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>Suite2P</title><link>https://open-neuroscience.com/post/suite2p/</link><pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/suite2p/</guid><description>&lt;p>Suite2P is a very modular imaging processing pipeline written in Python which allows you to perform registration of raw data movies, automatic cell detection, extraction of calcium traces and infers spike times. It is a very fast and accurate tool and can work on standard workstations. It also includes a visualization graphical user interface (GUI) that facilitates analysis and manual curation of the cell detection algorithm.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Carsen Stringer and Marius Pachitariu&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://mouseland.github.io/suite2p/_build/html/index.html">https://mouseland.github.io/suite2p/_build/html/index.html&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>Autoreward 2</title><link>https://open-neuroscience.com/post/autoreward2/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/autoreward2/</guid><description>&lt;p>The &lt;strong>motivation&lt;/strong> to start this project arises when we started to include a new behavioral paradigm in the lab, an alternation T-mace with return arms (like the one in Wood e_t al._ 2000). We wanted a clean performance, as well as a clean video record, so we consider necessary to interfere neither with the animal attention (mice, how they are!) nor the camera’s field of view. I decided then to give a try to the new hobby I was getting into, “Do-It-Yourself” (DIY) stuff.&lt;/p>
&lt;p>In my head, it was pictured very simple. At the end of the day, I just needed a) something to detect the animal passing by, b) something to deliver a drop of water and c) something to make it happen in a coordinated way. And that’s what Autoreward2 is, no more, no less.&lt;/p>
&lt;p>Well perhaps it is a bit more. &lt;strong>So far, the project can&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Detect&lt;/strong> when the animal reaches the end of any of the two arms.&lt;/li>
&lt;li>&lt;strong>Deliver&lt;/strong> a small drop of fluid through the corresponding licking port (easy to make it happen in the opposite, if wanted).&lt;/li>
&lt;li>Give visual cues to the experimenter, indicating which arm has been reached.&lt;/li>
&lt;li>Allow to &lt;strong>select&lt;/strong> different modes of working for different working protocols: ‘Waiting for selection’, ‘Habituation’, ‘Training’, ‘Experimental’ and “Filling and cleaning” modes (and is ready to include more!).&lt;/li>
&lt;/ul>
&lt;p>To achieve it, I decided for very &lt;strong>simple approach&lt;/strong>. A couple of cheap infrared emitters are continuously read by an UNO R3 board. Breaking any of the beams triggers the signal to open the corresponding solenoid valve, connected to the fluid tank. That lets the liquid flow by gravity for around 75 milliseconds, resulting in a single drop at the tip of the licking port.&lt;/p>
&lt;div align="center">
&lt;p>&lt;img src="./featured2.jpg" alt="">&lt;/p>
&lt;/div>
&lt;p>There is a delay after each detection, to avoid repetitive delivery if animals don’t leave the area. A couple LEDs mounted in the bare-board (out of animal sight) light up when the process is triggered, one for each side. They also work as indicators for the ‘Waiting for selection’ mode, when they are continuously on, meanwhile no option is choose or the ‘return to waiting mode action’ is pressed.&lt;/p>
&lt;p>&lt;strong>The selection&lt;/strong> is made through a 4×4 membrane keypad. Right now, only options 1 to 4 are programmed, making up to 12 more programs available! When any section is made, the in-built LED blinks the corresponding times and the system is ready to work. At any moment, pressing any key makes the system reset to the waiting mode. As easy as that.&lt;/p>
&lt;p>Everything is &lt;strong>powered&lt;/strong> by a regular 9V wall adapter, giving 3.3V to the LEDs and Infrared detectors, and 9V to the solenoids. Of course, it is possible to use a 9V batterie to power it. To avoid damage coming from the solenoid discharges, the circuit is protected by a couple of diodes at this level.&lt;/p>
&lt;p>And that’s all, &lt;strong>it’s simple&lt;/strong>. The most important thing: it &lt;strong>works&lt;/strong>. The other most important thing: it costs around &lt;strong>80€&lt;/strong>. Here is the to-buy list (or equivalent):&lt;/p>
&lt;ul>
&lt;li>Elegoo UNO R3 (I found them for &lt;strong>10€&lt;/strong>, with USB cable)&lt;/li>
&lt;li>BreadBoard + Acrylic base (&lt;strong>7€&lt;/strong>)&lt;/li>
&lt;li>9V 1A Wall power supply (&lt;strong>9€&lt;/strong>)&lt;/li>
&lt;li>2x InfraRed beams, 5mm (&lt;strong>15€&lt;/strong> both, the 3mm ones are even cheaper)&lt;/li>
&lt;li>2x Mini-Solenoid valves (&lt;strong>10€&lt;/strong> both)&lt;/li>
&lt;li>2x red LEDs&lt;/li>
&lt;li>4x 1 KΩ resistors&lt;/li>
&lt;li>2x TIP120 Transistors&lt;/li>
&lt;li>2x 1N4001 diodes&lt;/li>
&lt;li>Wiring (set of jumpers for less than &lt;strong>10€&lt;/strong>)&lt;/li>
&lt;li>‘Velcro’ to attach the acrylic base where the boards are mounted.&lt;/li>
&lt;li>Plastic tubing and laboratory sample tubes, modified with turning siringe tips to attach/deattach the tubing easily.&lt;/li>
&lt;li>2x or 4x weak magnets to fix the tubes to the walls.&lt;/li>
&lt;/ul>
&lt;p> &lt;/p>
&lt;p>Feel free to access the &lt;a href="https://github.com/jjballesteros/Arduino-AutoReward" target="_blank" rel="noopener">Github&lt;/a> page or the &lt;a href="http://forum.arduino.cc/index.php?topic=476643.0" target="_blank" rel="noopener">Arduino forum post&lt;/a> to obtain the &lt;strong>code&lt;/strong>, check for the circuit &lt;strong>sketch&lt;/strong>, and see some &lt;strong>pictures&lt;/strong>.&lt;/p>
&lt;p>PD: If someone is scandalized by the code, I am getting better on it, it is not my main strength. Please, improve it! Of course, I have in mind many possible upgrades such as a screen, a SD card port, to change the Keypad for a wireless interface (tactile?) … Did someone say smartphone plus Bluetooth? Going fancy, a barcode reader to easily introduce subjects’ data… And here is where I relay in the open-access idea, I offer it and hopefully someone implement any of the ideas. If so, remember to share!&lt;/p>
&lt;p>Jesús J. Ballesteros&lt;/p>
&lt;p>Contact me:&lt;/p>
&lt;p>&lt;a href="https://twitter.com/jjballesterosc" target="_blank" rel="noopener">Twitter&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.researchgate.net/profile/J_J_Ballesteros" target="_blank" rel="noopener">ResearchGate&lt;/a>&lt;/p></description></item><item><title>Backlog</title><link>https://open-neuroscience.com/post/backlog/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/backlog/</guid><description>&lt;p>Bellow is a list of interesting projects related to science and research, that we didn&amp;rsquo;t have time to curate yet. Feel free to browse through them and make comments and suggestions!&lt;/p>
&lt;p>&lt;a href="http://wiki.cogain.org/index.php/Eye_Trackers">http://wiki.cogain.org/index.php/Eye_Trackers&lt;/a> Low cost eye tracking&lt;/p>
&lt;p>&lt;a href="http://home.gna.org/veusz/">http://home.gna.org/veusz/&lt;/a> scientific plotting package&lt;/p>
&lt;p>&lt;a href="http://erkutlu.blogspot.com.es/2012/12/eeg-and-arduino-do-it-yourself-eeg-ekg.html">http://erkutlu.blogspot.com.es/2012/12/eeg-and-arduino-do-it-yourself-eeg-ekg.html&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://www.wired.com/wiredscience/2011/03/diy-cellphone-microscope">http://www.wired.com/wiredscience/2011/03/diy-cellphone-microscope&lt;/a> cellphone into microscope spectrometer&lt;/p>
&lt;p>&lt;a href="http://www.ncbi.nlm.nih.gov/geo/">http://www.ncbi.nlm.nih.gov/geo/&lt;/a> gene expression omnibus database&lt;/p>
&lt;p>&lt;a href="http://journal.frontiersin.org/Journal/10.3389/fninf.2014.00024/abstract">http://journal.frontiersin.org/Journal/10.3389/fninf.2014.00024/abstract&lt;/a> Broccoli software for fast fMRI analyses&lt;/p>
&lt;p>&lt;a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0086733">http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0086733&lt;/a> smartphone brain scanner&lt;/p>
&lt;p>&lt;a href="http://paper.li/IbrahimMalick/1320890343">http://paper.li/IbrahimMalick/1320890343&lt;/a>  open source by ibrahim malick&lt;/p>
&lt;p>&lt;a href="http://www.theguardian.com/public-leaders-network/2014/apr/15/big-data-open-data-transform-government?CMP=twt_gu">http://www.theguardian.com/public-leaders-network/2014/apr/15/big-data-open-data-transform-government?CMP=twt_gu&lt;/a> big data&lt;/p>
&lt;p>&lt;a href="https://code.google.com/p/arduino-v-neusci/">https://code.google.com/p/arduino-v-neusci/&lt;/a> example on how to use arduino for visual neuroscience&lt;/p>
&lt;p>&lt;a href="http://boinc.berkeley.edu/">http://boinc.berkeley.edu/&lt;/a>  the same idea from seth but generalized&lt;/p>
&lt;p>&lt;a href="http://pybossa.com/">http://pybossa.com/&lt;/a> the same idea from boinc (above) but sharing cognition&lt;/p>
&lt;p>&lt;a href="http://www.acq4.org/">http://www.acq4.org/&lt;/a> neurophysiology and data analysis systems&lt;/p>
&lt;p>&lt;a href="https://about.gitlab.com/about/">https://about.gitlab.com/about/&lt;/a> version control system for projects&lt;/p>
&lt;p>&lt;a href="http://leaflabs.com/willow">http://leaflabs.com/willow&lt;/a> neuroscience 1000 channels array&lt;/p>
&lt;p>&lt;a href="http://scalablephysiology.org/">http://scalablephysiology.org/&lt;/a> the page dedicated to willow&lt;/p>
&lt;p>&lt;a href="http://heywhatsthebigidea.net/projects/pi-vision-a-raspberry-pi-camera-controller/">http://heywhatsthebigidea.net/projects/pi-vision-a-raspberry-pi-camera-controller/&lt;/a> pivision&lt;/p>
&lt;p>&lt;a href="https://libre3d.com/category/687/Test-Equipment/listings/717/Open-Source-Water-Testing-Platform.html">https://libre3d.com/category/687/Test-Equipment/listings/717/Open-Source-Water-Testing-Platform.html&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://www.peekvision.org/">http://www.peekvision.org/&lt;/a> ophtamology exams with smartphones&lt;/p>
&lt;p>&lt;a href="http://www.gpugrid.net/">http://www.gpugrid.net/&lt;/a> distributed computing&lt;/p>
&lt;p>&lt;a href="http://www.iorodeo.com/consulting">http://www.iorodeo.com/consulting&lt;/a> open source hardware company&lt;/p>
&lt;p>&lt;a href="http://hackaday.com/2015/01/06/3d-printing-circuits-gets-rid-of-the-box-altogether/">http://hackaday.com/2015/01/06/3d-printing-circuits-gets-rid-of-the-box-altogether/&lt;/a>  3d print plastic and electronics together&lt;/p>
&lt;p>&lt;a href="https://synbiota.com/" target="_blank" rel="noopener">https://synbiota.com/ &lt;/a> electronic lab notebook&lt;/p>
&lt;p>&lt;a href="http://biojs.net/">http://biojs.net/&lt;/a> biological data visualization tool&lt;/p>
&lt;p>&lt;a href="http://open-access.net/de_en/homepage/">http://open-access.net/de_en/homepage/&lt;/a> portal that gathers information on open access&lt;/p>
&lt;p>&lt;a href="http://www.ohwr.org/">http://www.ohwr.org/&lt;/a> open hardware repository&lt;/p>
&lt;p>&lt;a href="http://www.openingscience.org/">http://www.openingscience.org/&lt;/a> an umbrella t open scholarly data to a multitude of stakeholders&lt;/p>
&lt;p>&lt;a href="http://woodenhaptics.org/">http://woodenhaptics.org/&lt;/a> open source haptics device&lt;/p>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=Ss-9iXRUeGc">https://www.youtube.com/watch?v=Ss-9iXRUeGc&lt;/a> Pneuflex actuators&lt;/p>
&lt;p>&lt;a href="https://sites.google.com/site/openspinmicroscopy/">https://sites.google.com/site/openspinmicroscopy/&lt;/a> Openspin microscope&lt;/p>
&lt;p>&lt;a href="http://openspim.org/Welcome_to_the_OpenSPIM_Wiki" target="_blank" rel="noopener">http://openspim.org/Welcome_to_the_OpenSPIM_Wiki&lt;/a> openspin microscope&lt;/p>
&lt;p>&lt;a href="http://journal.frontiersin.org/Journal/10.3389/fneng.2014.00043/abstract">http://journal.frontiersin.org/Journal/10.3389/fneng.2014.00043/abstract&lt;/a> signal generator&lt;/p>
&lt;p>&lt;a href="http://hackaday.io/project/1395-open-source-science-tricorder">http://hackaday.io/project/1395-open-source-science-tricorder&lt;/a> arduino based gagdet with lots of sensors&lt;/p>
&lt;p>&lt;a href="https://plot.ly/feed/">https://plot.ly/feed/&lt;/a> plots and data online&lt;/p>
&lt;p>&lt;a href="http://littledevices.org/">http://littledevices.org/&lt;/a> small portable devices for health related tests/exams and etc&lt;/p>
&lt;p>&lt;a href="https://gnu.io/">https://gnu.io/&lt;/a> social interaction&lt;/p>
&lt;p>&lt;a href="http://thinklab.com/how_it_works" target="_blank" rel="noopener">http://thinklab.com/how_it_works&lt;/a> online platform for science project management&lt;/p>
&lt;p>&lt;a href="http://credit.casrai.org/about-us/">http://credit.casrai.org/about-us/&lt;/a> changing the way scientific contributions are measured/displayed&lt;/p>
&lt;p>&lt;a href="http://journal.frontiersin.org/article/10.3389/fneng.2015.00001/full?utm_source=newsletter&amp;amp;utm_medium=email&amp;amp;utm_campaign=Neuroscience-w15-2015" target="_blank" rel="noopener">http://journal.frontiersin.org/article/10.3389/fneng.2015.00001/full?utm_source=newsletter&amp;amp;utm_medium=email&amp;amp;utm_campaign=Neuroscience-w15-2015&lt;/a> system for light stimulation and data recording for optogenetics&lt;/p>
&lt;p>&lt;a href="https://arcturus.io/">https://arcturus.io/&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://cmictig.cs.ucl.ac.uk/wiki/index.php/Main_Page" target="_blank" rel="noopener">http://cmictig.cs.ucl.ac.uk/wiki/index.php/Main_Page &lt;/a> brain imaging software suite&lt;/p>
&lt;p>&lt;a href="https://openhatch.org/wiki/Open_Science_Projects_and_Organizations#Neuroscience_2" target="_blank" rel="noopener">https://openhatch.org/wiki/Open_Science_Projects_and_Organizations#Neuroscience_2  &lt;/a> Open science projects for neurosciences&lt;/p>
&lt;p>&lt;a href="http://openpump.org/">http://openpump.org/&lt;/a> open source syringe pump&lt;/p>
&lt;p>&lt;a href="http://fab.cba.mit.edu/classes/4.140/people/wildebeest/projects/final/index.html">http://fab.cba.mit.edu/classes/4.140/people/wildebeest/projects/final/index.html&lt;/a> another open source pump&lt;/p>
&lt;p>&lt;a href="http://guides.teklalabs.org/c/Science_Lab_Equipment" target="_blank" rel="noopener">http://guides.teklalabs.org/c/Science_Lab_Equipment&lt;/a> tekla labs&lt;/p>
&lt;p>&lt;a href="http://blogs.lse.ac.uk/impactofsocialsciences/2014/08/05/oer-impact-map-open-university/">http://blogs.lse.ac.uk/impactofsocialsciences/2014/08/05/oer-impact-map-open-university/&lt;/a> open lectures and their impact&lt;/p>
&lt;p>&lt;a href="http://www.graphicsmagick.org/index.html">http://www.graphicsmagick.org/index.html&lt;/a> image processing library&lt;/p>
&lt;p>&lt;a href="http://www.kinectotherapy.in/">http://www.kinectotherapy.in/&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://www.oshwa.org/">http://www.oshwa.org/&lt;/a> open source hardware association page&lt;/p>
&lt;p>&lt;a href="https://www.peerageofscience.org/how-it-works/">https://www.peerageofscience.org/how-it-works/&lt;/a> peerage of science&lt;/p>
&lt;p>&lt;a href="http://www.linux-usb-daq.co.uk/">http://www.linux-usb-daq.co.uk/&lt;/a> general IO boards for linux&lt;/p>
&lt;p>&lt;a href="https://pubpeer.com/">https://pubpeer.com/&lt;/a> post review of papers&lt;/p>
&lt;p>&lt;a href="https://opensource.com/business/15/8/open-source-products-four-rules?utm_content=buffer7c7ae&amp;amp;utm_medium=social&amp;amp;utm_source=facebook.com&amp;amp;utm_campaign=buffer" target="_blank" rel="noopener">https://opensource.com/business/15/8/open-source-products-four-rules?utm_content=buffer7c7ae&amp;amp;utm_medium=social&amp;amp;utm_source=facebook.com&amp;amp;utm_campaign=buffer&lt;/a> Open source for products&lt;/p>
&lt;p>[http://journal.frontiersin.org/article/10.3389/fnins.2015.00206/full?utm_source=newsletter&amp;amp;utm_medium=email&amp;amp;utm_campaign=Neuroscience-w33-2015 spinnaker. millisecond](&lt;a href="http://journal.frontiersin.org/article/10.3389/fnins.2015.00206/full?utm_source=newsletter&amp;amp;utm_medium=email&amp;amp;utm_campaign=Neuroscience-w33-2015">http://journal.frontiersin.org/article/10.3389/fnins.2015.00206/full?utm_source=newsletter&amp;amp;utm_medium=email&amp;amp;utm_campaign=Neuroscience-w33-2015&lt;/a> spinnaker. millisecond) range modelling&lt;/p>
&lt;p>&lt;a href="http://wiki.openscienceschool.com/wiki/Tools/DI-Lambda">http://wiki.openscienceschool.com/wiki/Tools/DI-Lambda&lt;/a> do it yourself spectrophotometer&lt;/p>
&lt;p>&lt;a href="http://thurj.org/research/2011/01/432/">http://thurj.org/research/2011/01/432/&lt;/a> automated head fixed prep for rats&lt;/p>
&lt;p>&lt;a href="http://journal.frontiersin.org/article/10.3389/fninf.2015.00004/full?utm_source=newsletter&amp;amp;utm_medium=email&amp;amp;utm_campaign=Neuroscience-w17-2015" target="_blank" rel="noopener">http://journal.frontiersin.org/article/10.3389/fninf.2015.00004/full?utm_source=newsletter&amp;amp;utm_medium=email&amp;amp;utm_campaign=Neuroscience-w17-2015&lt;/a> map reduce, scalable data analysis for ephys&lt;/p>
&lt;p>&lt;a href="http://www.elsevier.com/connect/the-changing-face-of-journal-metrics" target="_blank" rel="noopener">www.elsevier.com/connect/the-changing-face-of-journal-metrics&lt;/a> the changing face of journal metrics&lt;/p>
&lt;p>&lt;a href="http://www.johnstowers.co.nz/blog/2014/05/27/flymad/">http://www.johnstowers.co.nz/blog/2014/05/27/flymad/&lt;/a> fly brain altering device&lt;/p>
&lt;p>&lt;a href="https://jupyter.org/">https://jupyter.org/&lt;/a> open source notebook for over 40 programming languages&lt;/p>
&lt;p>&lt;a href="https://sites.google.com/site/neurorighter/">https://sites.google.com/site/neurorighter/&lt;/a> closed loop recording and stimulation ephys&lt;/p>
&lt;p>&lt;a href="http://www.kitware.com/opensource/opensource.html">http://www.kitware.com/opensource/opensource.html&lt;/a> several open source software suites/libraries&lt;/p>
&lt;p>&lt;a href="https://jasp-stats.org/">https://jasp-stats.org/&lt;/a> a fresh way to do statistics&lt;/p>
&lt;p>&lt;a href="http://www.brain-map.org/">http://www.brain-map.org/&lt;/a> allen institute page with data, tools and maps&lt;/p>
&lt;p>&lt;a href="https://www.circuitlab.com/">https://www.circuitlab.com/&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://www.123dapp.com/circuits" target="_blank" rel="noopener">www.123dapp.com/circuits&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://madresistor.org/box0/">https://madresistor.org/box0/&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://zenodo.org/">https://zenodo.org/&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://ipfs.io/">https://ipfs.io/&lt;/a>&lt;/p>
&lt;blockquote class="wp-embedded-content" data-secret="nea1VBynr1">
&lt;p>
&lt;a href="https://techcrunch.com/2016/06/19/the-next-wave-in-software-is-open-adoption-software/">The next wave in software is open adoption software&lt;/a>
&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4933559/pdf/1604.pdf">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4933559/pdf/1604.pdf&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.rs-online.com/designspark/the-teaching-lab-of-tomorrow?cm_mmc=DE-EM-_-DSN_20160822-_-DM3300-_-TTB_URL2&amp;amp;cid=DM3300&amp;amp;bid=53290840" target="_blank" rel="noopener">https://www.rs-online.com/designspark/the-teaching-lab-of-tomorrow?cm_mmc=DE-EM-_-DSN_20160822-_-DM3300-_-TTB_URL2&amp;amp;cid=DM3300&amp;amp;bid=53290840&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://www.nytimes.com/2016/06/28/technology/amazon-unveils-online-education-service-for-teachers.html?_r=5%C2%AEister=google">http://www.nytimes.com/2016/06/28/technology/amazon-unveils-online-education-service-for-teachers.html?_r=5®ister=google&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.hackster.io/arduino">https://www.hackster.io/arduino&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://brainwaves.io/wp/">http://brainwaves.io/wp/&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://www.mkme.org/">http://www.mkme.org/&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://outernet.is/">https://outernet.is/&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://hackaday.io/project/16024-openwheel-parametric-osh-wheelstyrestracks">https://hackaday.io/project/16024-openwheel-parametric-osh-wheelstyrestracks&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0166735#sec016">http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0166735#sec016&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://hackaday.io/project/18643-open-source-freakin-scanning-electron-microscope">https://hackaday.io/project/18643-open-source-freakin-scanning-electron-microscope&lt;/a>  open source electron microscope&lt;/p>
&lt;p>&lt;a href="http://jn.physiology.org/content/116/2/252.long">http://jn.physiology.org/content/116/2/252.long&lt;/a> Open notebooks on ephys&lt;/p>
&lt;p>&lt;a href="http://www.instructables.com/id/Laser-Scanning-Microscope/">http://www.instructables.com/id/Laser-Scanning-Microscope/&lt;/a> make a laser scanning microscope&lt;/p>
&lt;p>&lt;a href="https://github.com/maxritter/DIY-Thermocam">https://github.com/maxritter/DIY-Thermocam&lt;/a> DIY thermal camera&lt;/p>
&lt;p>&lt;a href="https://neuinfo.org/about/organization">https://neuinfo.org/about/organization&lt;/a> neuroscience information framework&lt;/p>
&lt;p>&lt;a href="https://publishing.aip.org/publishing/journal-highlights/how-3-d-print-your-own-sonic-tractor-beam">https://publishing.aip.org/publishing/journal-highlights/how-3-d-print-your-own-sonic-tractor-beam&lt;/a> DIY sonic tractor beam&lt;/p>
&lt;p>&lt;a href="http://www.upb.edu/en/contenido/mini-spectrometer-3d-printable-model">http://www.upb.edu/en/contenido/mini-spectrometer-3d-printable-model&lt;/a> DIY spectrometer&lt;/p>
&lt;p>&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4857158/pdf/ac5b04153.pdf">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4857158/pdf/ac5b04153.pdf&lt;/a> lab on a drone&lt;/p>
&lt;p>&lt;a href="http://oceanographyforeveryone.com/">http://oceanographyforeveryone.com/&lt;/a> page hosting hardware projects related to oceanography&lt;/p>
&lt;p>&lt;a href="https://makingscience.withgoogle.com/science-journal?lang=en">https://makingscience.withgoogle.com/science-journal?lang=en&lt;/a> google app for data collection using mobile phone sensors&lt;/p>
&lt;p>&lt;a href="http://3d.si.edu/browser">http://3d.si.edu/browser&lt;/a> smithsonian museum repository of scanned objects&lt;/p>
&lt;p>&lt;a href="https://nasa3d.arc.nasa.gov/models">https://nasa3d.arc.nasa.gov/models&lt;/a> nasa models for 3d p nting&lt;/p>
&lt;p>&lt;a href="http://www.qtiplot.com/">http://www.qtiplot.com/&lt;/a> Data analysis and visualization&lt;/p>
&lt;p>&lt;a href="https://github.com/BigCorvus/Physio">https://github.com/BigCorvus/Physio&lt;/a> hacked medical device&lt;/p>
&lt;p>&lt;a href="https://mousetube.pasteur.fr/">https://mousetube.pasteur.fr/&lt;/a> mouse vocalization database&lt;/p>
&lt;p>&lt;a href="http://www.thinkering.de/cms/">http://www.thinkering.de/cms/&lt;/a> blog on tinkering with science tool examples&lt;/p></description></item><item><title>Computer clusters</title><link>https://open-neuroscience.com/post/computer_cluster/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/computer_cluster/</guid><description>&lt;br>
&lt;p>Here are two projects that use card sized computers as the basic units for computing clusters:&lt;/p>
&lt;br>
&lt;ul>
&lt;li>A &lt;a href="http://www.southampton.ac.uk/~sjc/raspberrypi/" target="_blank" rel="noopener">64 node cluster&lt;/a>, build using pi’s and lego, built at the University of Southampton.&lt;/li>
&lt;li>&lt;a href="https://www.linux.com/training-tutorials/building-compute-cluster-beaglebone-black/" target="_blank" rel="noopener">BeagleBone Black cluster&lt;/a> by &lt;a href="https://www.linux.com/author/mazdacardinal/" target="_blank" rel="noopener">Dan Ricart&lt;/a>&lt;/li>
&lt;/ul>
&lt;br></description></item><item><title>OpenFlexure</title><link>https://open-neuroscience.com/post/openflexure/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/openflexure/</guid><description>&lt;p>OpenFlexure is a 3D printed flexure translation stage, developed by a group at the Bath University. The stage is capable of sub-micron-scale motion, with very small drift over time. Which makes it quite good, among other things, for time-lapse protocols that need to be done over days/weeks time, and under space restricted areas, such as fume hoods. A paper describing it in detail can be found &lt;a href="http://arxiv.org/abs/1509.05394" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>Adding a camera and servo motors, turns the stage into an automated microscope. More details about the project can be found &lt;a href="https://openflexure.org/" target="_blank" rel="noopener">here&lt;/a>.&lt;/p></description></item><item><title>OpenFuge</title><link>https://open-neuroscience.com/post/openfuge/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/openfuge/</guid><description>&lt;p>&lt;a href="https://www.thingiverse.com/thing:151406" target="_blank" rel="noopener">OpenFuge&lt;/a> describes all the materials and gives step by step instructions to the assembly of a centrifuge that is able to deliver 6000 G’s of force and to rotate at 9000 RPM, while being able to hold 4 eppendorf tubes. Developed by &lt;a href="https://www.thingiverse.com/CopabX/about" target="_blank" rel="noopener">CopabX&lt;/a>&lt;/p></description></item><item><title>Psychophysics toolboxes</title><link>https://open-neuroscience.com/post/psychophysics-toolboxes/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/psychophysics-toolboxes/</guid><description>&lt;br>
&lt;p>Roughly put, &lt;a href="http://en.wikipedia.org/wiki/Psychophysics" target="_blank" rel="noopener">psychophysics&lt;/a> studies the relationships of physical stimuli and their respective elicited sensations and perception. Psyhophysics also relates to the techniques used to probe these relationships and the toolboxes here presented are mainly dealing with these techniques.&lt;/p>
&lt;br>
&lt;hr>
&lt;br>
&lt;div align="center">
&lt;p>&lt;img src="https://raw.githubusercontent.com/smathot/osdoc/3.2/themes/cogsci/static/img/banner.png" alt="">&lt;/p>
&lt;/div>
&lt;p>&lt;a href="https://osdoc.cogsci.nl/" target="_blank" rel="noopener"> OpenSesame&lt;/a> is a graphical opensource experiment builder. It has drag and drop features as well as customization possibilities, via python scripting and custom plugins. here is a &lt;a href="http://link.springer.com/article/10.3758%2Fs13428-011-0168-7" target="_blank" rel="noopener">link&lt;/a> to a paper describing the software&lt;figure style="width: 853px" class="wp-caption alignnone">&lt;/p>
&lt;br>
&lt;hr>
&lt;br>
&lt;p>&lt;a href="http://psychtoolbox.org/" target="_blank" rel="noopener">Psychtoolbox&lt;/a>, or PTB, is a free versatile toolbox to be used mainly in visual experiments, it is able to deliver visual and auditory stimuli and to receive subject input. It has a big quantity of active users (15,000 as stated on their &lt;a href="http://psychtoolbox.org/forum/" target="_blank" rel="noopener">website&lt;/a>) what should make the life of the beginner user somehow easier (they have a &lt;a href="https://psychtoolbox.discourse.group/" target="_blank" rel="noopener">forum page&lt;/a>) The latest version (PTB-3 as this page was written) is able to run under MATLAB (version 7.X) and Octave (version 3.2.X) in any of the three main operational systems out there (Mac, Windows and Linux).  A paper describing the toolbox can be found &lt;a href="http://color.psych.upenn.edu/brainard/papers/Psychtoolbox.pdf" target="_blank" rel="noopener">here.&lt;/a>&lt;/p>
&lt;br>
&lt;hr>
&lt;br>
&lt;div align="center">
&lt;p>&lt;img src="./psychopyLogoOnline.png" alt="">&lt;/p>
&lt;/div>
&lt;p>&lt;a href="https://www.psychopy.org/" target="_blank" rel="noopener">PsychoPy&lt;/a> is also a free toolbox that can be used to deliver visual and auditory stimuli and receive inputs from subjects, on top of keyboard, mouse and button boxes, it also supports serial and parallel ports and compiled drivers (allowing interface with pretty much any hardware installed in your computer). It is written in Python, and it can be used with Windows, Mac or Linux. Two papers describing the toolbox can be found &lt;a href="https://www.sciencedirect.com/science/article/pii/S0165027006005772" target="_blank" rel="noopener">here&lt;/a> and &lt;a href="https://www.frontiersin.org/articles/10.3389/neuro.11.010.2008/full" target="_blank" rel="noopener">here&lt;/a>.&lt;/p></description></item><item><title>Pulse Pal</title><link>https://open-neuroscience.com/post/pulse-pal/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pulse-pal/</guid><description>&lt;blockquote>
&lt;p>Pulse Pal is an open and inexpensive (~$210) alternative to pulse generators used in neurophysiology research, and is most often used to create precisely timed light trains in optogenetics assays. Pulse Pal generates four channels of configurable square pulse trains ranging in voltage from +10 to -10V using a bipolar DAC. Two digital trigger channels can be used to start and stop playback. APIs are available in C++, Python and MATLAB, and the hardware designs and firmware are fully open source.&lt;/p>
&lt;/blockquote>
&lt;p>Be sure to check the &lt;a href="http://journal.frontiersin.org/article/10.3389/fneng.2014.00043/abstract" target="_blank" rel="noopener">paper&lt;/a> about it and their &lt;a href="https://sites.google.com/site/pulsepalwiki/home" target="_blank" rel="noopener">wiki page.&lt;/a>&lt;/p></description></item><item><title>Python, NumPy, SciPy &amp; Matplotlib</title><link>https://open-neuroscience.com/post/python-numpy-scipy-matplotlib/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/python-numpy-scipy-matplotlib/</guid><description>&lt;p>Python is a free programming language that is widely used, most of the software developed for Linux is written in Python. It contains several libraries that cover a lot of problem domains, from asynchronous processing to zip files. Also it is available for most platforms. More information can be found at the language &lt;a href="http://www.python.org/" target="_blank" rel="noopener">official page.&lt;/a>&lt;/p>
&lt;p>More specifically to scientific computation, the &lt;a href="http://www.numpy.org/" target="_blank" rel="noopener">NumPy&lt;/a> project brings n-dimension array objects, random number capabilities, fourier transforms and many other useful tools.&lt;/p>
&lt;p>Boosting NumPy capabilities is &lt;a href="http://www.scipy.org/" target="_blank" rel="noopener">SciPy&lt;/a>, which is another Python library that adds signal processing, optimization and statistical tools to Python.&lt;/p>
&lt;p>After all the calculations are done, they can be plotted also using python and another useful library: &lt;a href="http://matplotlib.org/" target="_blank" rel="noopener">Matplotlib.&lt;/a>&lt;/p></description></item><item><title>Red Pitaya</title><link>https://open-neuroscience.com/post/red-pitaya/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/red-pitaya/</guid><description>&lt;p>&lt;a href="https://www.redpitaya.com/?skip_intro=yes" target="_blank" rel="noopener">Red Pitaya&lt;/a> is an computer+FPGA that has digital input and outputs and really fast analog inputs and outputs. It allows connection over ethernet and programming of custom routines. The system is powerful enough to have application in mostly all branches of neuroscience labs: oscilloscopes, signal generators and even a candidate for recording systems.&lt;/p></description></item><item><title>School of Data</title><link>https://open-neuroscience.com/post/school_of_data/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/school_of_data/</guid><description>&lt;p>School of Data is a global network that aims to train civil society in the practical use of the large amount of data available nowadays.&lt;/p>
&lt;p>The network is composed of individuals and organizations that carry out training programs, hands-on courses and other activities in different regions and countries of the world. They also offer fellowships and experts programs to prepare new people in different parts of the world to continue with the training process and the application of the ideas of School of Data organization.&lt;/p>
&lt;p>In the web page you can also find open material about basic concepts of data analysis and the methodological approach considered appropriate.&lt;/p>
&lt;p>The school in numbers&lt;/p>
&lt;ul>
&lt;li>more than 6000 people trained&lt;/li>
&lt;li>44 learning modules&lt;/li>
&lt;li>13 member organizations&lt;/li>
&lt;li>100 individual members&lt;/li>
&lt;li>34 countries represented&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>For more information visit the webpage:&lt;/strong> &lt;a href="https://schoolofdata.org/">https://schoolofdata.org/&lt;/a>&lt;/p></description></item><item><title>Spike Gadgets</title><link>https://open-neuroscience.com/post/spike-gadgets/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/spike-gadgets/</guid><description>&lt;p>A brief description of their current software (09.Sep.2016) is provided by one of their founders, Mattias Karlsson:&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://www.spikegadgets.com/software/statescript.html" target="_blank" rel="noopener">State Script:&lt;/a>&lt;/strong>&lt;/p>
&lt;p>Do you need to control lasers for optogenetics, stimulators, or other TTL-based devices with precise, temporally defined patterns? Do you need to monitor beam breaks, lever presses, or other digital events in real time to define behavioral tasks?  You could program an Arduino, but that’s a lot of work. Or, you can use StateScript, which allows users with minimal programming experience define complex input/output relationships for the most demanding hardware control experiments.&lt;/p>
&lt;div>
&lt;p>This open-source project now runs on two available hardware platforms, the MBED LPC1768 micro controller board ($50) and the SpikeGadgets electrophysiology and behavioral control system.  More hardware support in is the works. A software interface, which is part of the Trodes open-source eletrophysiology suite (&lt;a href="http://www.spikegadgets.com/software/trodes.html" target="_blank">&lt;a href="http://www.spikegadgets.com/software/trodes.html">http://www.spikegadgets.com/software/trodes.html&lt;/a>&lt;/a>), allows you to upload scripts and dynamically interact with variables and ports states.&lt;/p>
&lt;br>
&lt;p>Anyone is welome to contribute. Here is the &lt;a href="https://bitbucket.org/mkarlsso/statescript" target="_blank" rel="noopener">bitbucket repo.&lt;/a>&lt;/p>
&lt;/div>
&lt;div align="center">
&lt;p>&lt;img src="https://i2.wp.com/www.spikegadgets.com/images/statescript_screenshot_2.png?resize=800%2C571" alt="">&lt;/p>
&lt;/div>
&lt;div>
&lt;p>&lt;strong>&lt;a href="http://www.spikegadgets.com/software/trodes.html" target="_blank" rel="noopener">Trodes:&lt;/a>&lt;/strong>&lt;/p>
&lt;p>Trodes is a software suite with a focus on data acquisition for extracellular neural recordings.  It has a growing user base and welcomes contributors with open arms! It is built using the ever-popular and powerful Qt C++ framework. While it is specialized to be used with SpikeGadgets’ ephys hardware, it also has built-in support for the Intan demo system and Open-Ephys hardware.&lt;/p>
&lt;p>It has some pretty impressive capabilities, including visualization of thousands of channels, spike viewing, online spike sorting, and low latency feedback control.  It has video processing, allowing position tracking that is synchronized to the recording, and integrates powerful environment control (lasers for optogenetics, levers, lights, pumps, etc.) with StateScript.&lt;/p>
&lt;/div>
&lt;div align="center">
&lt;p>&lt;img src="https://i1.wp.com/www.spikegadgets.com/images/trodesscreenshot.png?resize=800%2C444" alt="Trodes interface">&lt;/p>
&lt;p align="center"> Trodes interface &lt;/p>
&lt;p>&lt;img src="https://i1.wp.com/www.spikegadgets.com/images/trodes_screenshot_cameramod.png?resize=800%2C554" alt="Trodes interface">&lt;/p>
&lt;p align="center"> Trodes &lt;/p>
&lt;/div></description></item><item><title>Brainflow</title><link>https://open-neuroscience.com/post/brainflow/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/brainflow/</guid><description>&lt;p>&lt;a href="https://brainflow.ai/" target="_blank" rel="noopener">BrainFlow&lt;/a> BrainFlow is a library intended to obtain, parse and analyze EEG, EMG, ECG and other kinds of data from biosensors, it provides two APIs:&lt;/p>
&lt;ul>
&lt;li>Data Acquisition API to obtain data from BCI boards&lt;/li>
&lt;li>Signal Processing API which is completely independent and can be used without Data Acquisition API&lt;/li>
&lt;/ul>
&lt;p>Both of these APIs are uniform for all supported boards, so it allows to write completely board agnostic code.&lt;/p>
&lt;p>BrainFlow has bindings for:&lt;/p>
&lt;ul>
&lt;li>C++&lt;/li>
&lt;li>Python&lt;/li>
&lt;li>Java&lt;/li>
&lt;li>C#&lt;/li>
&lt;li>R&lt;/li>
&lt;/ul>
&lt;p>And provides almost the same API for all languages above.&lt;/p>
&lt;p>Check &lt;a href="https://brainflow.readthedocs.io" target="_blank" rel="noopener">BrainFlow Docs&lt;/a> for details.&lt;/p></description></item><item><title>Genome RNAi</title><link>https://open-neuroscience.com/post/genome-rnai/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/genome-rnai/</guid><description>&lt;blockquote>
&lt;p>&lt;a href="http://www.genomernai.org/Index" target="_blank" rel="noopener">GenomeRNAi  &lt;/a> is a database containing phenotypes from RNA interference (RNAi) screens in Drosophila and Homo sapiens. In addition, the database provides an updated resource of RNAi reagents and their predicted quality.&lt;/p>
&lt;p> &lt;/p>
&lt;/blockquote></description></item><item><title>GogoFuge</title><link>https://open-neuroscience.com/post/gogofuge/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/gogofuge/</guid><description>&lt;p>&lt;a href="https://diybio.org/2012/06/12/gogofuge/" target="_blank" rel="noopener">GogoFuge&lt;/a> is a good example of the power of opensource designs. IT was based on the idea of the DremelFuge and altered to be a tabletop centrifuge with vortex capability. It was created by &lt;a href="fablabatschool.org/profile/KeeganCooke">Keegan Cooke&lt;/a>&lt;/p>
&lt;iframe width="474" height="360" src="https://www.youtube.com/embed/Qcl04sqXqY4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe></description></item><item><title>Green Brain</title><link>https://open-neuroscience.com/post/green-brain/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/green-brain/</guid><description>&lt;p>&lt;a href="http://greenbrain.group.shef.ac.uk/" target="_blank" rel="noopener">The Green Brain project&lt;/a> wants to create an artificial &lt;em>Apis mellifera&lt;/em> brain and implement said brain into a robot, that will be able to fly and and behave just like a honey bee!&lt;/p>
&lt;p>The reasons for this project are:&lt;/p>
&lt;ul>
&lt;li>The bee brain has way less neurons than the brain of rodents (but still in the order of 10^6 neurons!), so understanding how this simple brain works could be a nice step towards understanding more complex brains.&lt;/li>
&lt;li>The bee population has been declining and scientists are not exactly sure why.&lt;/li>
&lt;li>Understanding bee behaviour and the organ that produces them might help solve the problem&lt;/li>
&lt;li>Improvement of unmanned aerial vehicle control&lt;/li>
&lt;/ul></description></item><item><title>Image, Office suites, and other general purpose software</title><link>https://open-neuroscience.com/post/image-office-suits-and-other-general-purpose-software/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/image-office-suits-and-other-general-purpose-software/</guid><description>&lt;p>If you are using Linux, changes are that this page is not that useful for you, since most of these programs come installed by default. For you who are not yet into linux, most of these programs have Windows/Mac versions:&lt;/p>
&lt;p>Office suites (spreadsheet calculation, slide manufacturing , document writing):&lt;/p>
&lt;p>&lt;a href="http://www.openoffice.org/" target="_blank" rel="noopener">Open Office&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://www.libreoffice.org/#0" target="_blank" rel="noopener">Libre Office&lt;/a>&lt;/p>
&lt;p>Image manipulation programs (vectorized images or photoshop style):&lt;/p>
&lt;p>&lt;a href="http://inkscape.org/" target="_blank" rel="noopener">Inkscape&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://www.gimp.org/" target="_blank" rel="noopener">Gimp&lt;/a>&lt;/p>
&lt;p>3D modelling (to create animations, solids or even things that can be printed):&lt;/p>
&lt;p>&lt;a href="http://free-cad.sourceforge.net/" target="_blank" rel="noopener">FreeCad&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://www.blender.org/" target="_blank" rel="noopener">Blender&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://www.openscad.org/" target="_blank" rel="noopener">OpenScad&lt;/a>&lt;/p></description></item><item><title>Intelligent hearing aid</title><link>https://open-neuroscience.com/post/intelligent-hearing-aid/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/intelligent-hearing-aid/</guid><description>&lt;p>Ojoshi at instructables.com has posted a manual on how to build this arduino based &lt;a href="http://www.instructables.com/id/Intelligent-Hearing-Aid/?ALLSTEPS" target="_blank" rel="noopener">hearing aid system&lt;/a>.&lt;/p>
&lt;p>From his &lt;a href="http://www.instructables.com/id/Intelligent-Hearing-Aid/?ALLSTEPS" target="_blank" rel="noopener">instructables page&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>it has tuning functionality that allows the wearer to tune the amplification to his or her needs. It has a conversational mode which recognizes voice input and amplifies it while reducing background noise. It saves all data to memory so that the device can be quickly powered up and ready to use. This device also has a very easy user interface to keep operation quick and simple.&lt;/p>
&lt;/blockquote>
&lt;p> &lt;/p>
&lt;p>If you are going to try and build this, take maximum care and do it at your own risk!&lt;/p>
&lt;p> &lt;/p>
&lt;p> &lt;figure style="width: 703px" class="wp-caption aligncenter">&lt;/p>
&lt;p>&lt;img src="https://i1.wp.com/cdn.instructables.com/F5D/JQVI/HOUFWUWY/F5DJQVIHOUFWUWY.LARGE.jpg?resize=703%2C937" alt="" width="703" height="937" data-recalc-dims="1" />&lt;figcaption class="wp-caption-text">from: &lt;a href="http://cdn.instructables.com/F5D/JQVI/HOUFWUWY/F5DJQVIHOUFWUWY.LARGE.jpg">http://cdn.instructables.com/F5D/JQVI/HOUFWUWY/F5DJQVIHOUFWUWY.LARGE.jpg&lt;/a>&lt;/figcaption>&lt;/figure>&lt;/p></description></item><item><title>IPipet</title><link>https://open-neuroscience.com/post/ipipet/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/ipipet/</guid><description>&lt;p>IPipet is a neat system to help you not to lose track of which wells you have already pipetted in or from. The idea is simple, you place a tablet running a link with your specific pipetting protocol under your source and destination plates. The tablet will illuminate the corresponding wells. After you pipette one sample, you press next on the tablet and the next sample will be illuminated. For more details watch the video (below) and visit the &lt;a href="http://ipipet.teamerlich.org/usage" target="_blank" rel="noopener">project&amp;rsquo;s homepage.&lt;/a> They even have a &lt;a href="http://www.thingiverse.com/thing:339588" target="_blank" rel="noopener">3D printable adaptor&lt;/a> to prevent the well plate from slipping on the tablet surface.&lt;/p>
&lt;br>
&lt;div align="center">
&lt;iframe src="https://player.vimeo.com/video/90988265" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen>&lt;/iframe>
&lt;p align="center">&lt;a href="https://vimeo.com/90988265">iPipet Demo&lt;/a> from &lt;a href="https://vimeo.com/user26499168">Team Erlich&lt;/a> on &lt;a href="https://vimeo.com">Vimeo&lt;/a>.&lt;/p>
&lt;/div></description></item><item><title>Lab management software</title><link>https://open-neuroscience.com/post/lab-management-software/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/lab-management-software/</guid><description>&lt;p>Since organisation of ideas, stocks, and projects is a major concern (or at least should be) of labs and researchers, here is a small compilation of cost free sofware to help out:&lt;/p>
&lt;br>
&lt;div align="center">
&lt;p>&lt;img src="./Quartzy.png" alt="">&lt;/p>
&lt;/div>
&lt;p>&lt;a href="https://www.quartzy.com/" target="_blank" rel="noopener">Quartzy&lt;/a> is a free web based application (supported by life sciences related companies) it focuses on sharing protocols, tracking orders, manage lab inventory and shared quipment management.&lt;/p>
&lt;br>
&lt;div align="center">
&lt;p>&lt;img src="./elabftw-logo.png" alt="">&lt;/p>
&lt;/div>
&lt;p>&lt;a href="https://www.elabftw.net/" target="_blank" rel="noopener">eLabFTW&lt;/a> is a management system created by Nicolas Carpi. It is opensource (which means each lab can customize it for special needs), free and it can be installed locally. Its &lt;a href="https://demo.elabftw.net/login.php" target="_blank" rel="noopener">online demo version&lt;/a> focuses on experiment log, database (where drugs, chemicals, animal strains and etc can be logged) and team (where lab members can be listed).&lt;/p></description></item><item><title>Micro-Manager</title><link>https://open-neuroscience.com/post/micro-manager/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/micro-manager/</guid><description>&lt;p>&lt;a href="http://www.micro-manager.org/wiki/Micro-Manager%20Project%20Overview" target="_blank" rel="noopener">Micro-Manager&lt;/a> is an ImageJ plugin dedicated to the control of microscopes. Their intent is to have a “one fits all” software for the control of microscopes, stages, filters and cameras. A comprehensive list of supported devives can be found on &lt;a href="http://www.micro-manager.org/wiki/Device_Support" target="_blank" rel="noopener">devices section&lt;/a> of the &lt;a href="http://www.micro-manager.org/wiki/Micro-Manager" target="_blank" rel="noopener">project webpage.&lt;/a> As the other projects listed on this website, the software is open source and freely distributed.&lt;/p>
&lt;p>&lt;a href="http://openeuroscience.wordpress.com/software/microscopy/micro-manager/" title="Micro-Manager" target="_blank" rel="noopener">&lt;img src="https://i1.wp.com/www.micro-manager.org/skins/mmskin/mm_logo.gif?w=800" alt="micro-manager logo" data-recalc-dims="1" />&lt;/a>&lt;/p></description></item><item><title>Neuromorpho</title><link>https://open-neuroscience.com/post/neuromorpho/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/neuromorpho/</guid><description>&lt;p>&lt;a href="http://neuromorpho.org/index.jsp" target="_blank" rel="noopener">NeuroMorpho.Org&lt;/a> is a centrally curated inventory of &lt;strong>digitally reconstructed neurons&lt;/strong> associated with peer-reviewed publications. It contains contributions from over 100 laboratories worldwide and is continuously updated as new morphological reconstructions are collected, published, and shared. (taken from neuromorpho.org)&lt;/p></description></item><item><title>NeuroTinker</title><link>https://open-neuroscience.com/post/neurotinker/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/neurotinker/</guid><description>&lt;p>&lt;a href="https://hackaday.io/project/3339-neurons-neurons-neurons" target="_blank" rel="noopener">NeuroTinker project&lt;/a> is all about hardware emulated neurons. The creators made them in a way that each hardware neuron has excitatory and inhibitory inputs and one output that can be split up to affect dowsntream neurons. They are also cheap enough so that one can build several of them and wire them together to see which properties will emerge in the system. Design files are available on the &lt;a href="https://github.com/neurotinker" target="_blank" rel="noopener">project&amp;rsquo;s GitHub organization&lt;/a>&lt;/p></description></item><item><title>NiBabel</title><link>https://open-neuroscience.com/post/nibabel/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/nibabel/</guid><description>&lt;p>&lt;a href="http://nipy.org/nibabel/" target="_blank" rel="noopener">NiBabel&lt;/a> is a python package, under the NiPy project, that aims at unifying the process of opening different medical and neuroimaging file formats, including: &lt;a href="http://www.grahamwideman.com/gw/brain/analyze/formatdoc.htm" target="_blank" rel="noopener">ANALYZE&lt;/a>,&lt;a href="http://www.nitrc.org/projects/gifti" target="_blank" rel="noopener">GIFTI&lt;/a>, &lt;a href="http://nifti.nimh.nih.gov/nifti-1/" target="_blank" rel="noopener">NIfTI1&lt;/a>, &lt;a href="http://en.wikibooks.org/wiki/MINC/Reference/MINC2.0_File_Format_Reference" target="_blank" rel="noopener">MINC&lt;/a>, &lt;a href="http://surfer.nmr.mgh.harvard.edu/fswiki/FsTutorial/MghFormat" target="_blank" rel="noopener">MGH&lt;/a> and &lt;a href="http://xmedcon.sourceforge.net/Docs/Ecat" target="_blank" rel="noopener">ECAT&lt;/a> as well as PAR/REC. The package is also able to read and write &lt;a href="http://surfer.nmr.mgh.harvard.edu/" target="_blank" rel="noopener">Freesurfer&lt;/a> format.&lt;/p>
&lt;div align="center">
&lt;p>&lt;img src="https://nipy.org/nibabel/_static/nipy-logo-bg-138x120.png" alt="">&lt;/p>
&lt;/div></description></item><item><title>Nipy</title><link>https://open-neuroscience.com/post/nipy/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/nipy/</guid><description>&lt;p>&lt;a href="https://nipy.org/" target="_blank" rel="noopener">NiPy&lt;/a> is an effort to make brain imaging research easier and more clear. This is implemented by providing a series of software that deal with file IO, analysis, and interfaces &amp;amp; pipelines.&lt;/p>
&lt;p>The software present up to now (05/05/2020)&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/nipype/index.html" target="_blank" rel="noopener">nipype&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/dipy/index.html" target="_blank" rel="noopener">diPy&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/mindboggle/index.html" target="_blank" rel="noopener">mindboggle&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/nibabel/index.html" target="_blank" rel="noopener">NiBabel&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/sdm/index.html" target="_blank" rel="noopener">scitran SDM&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/nipy/index.html" target="_blank" rel="noopener">nipy&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/nitime/index.html" target="_blank" rel="noopener">nitime&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/popeye/index.html" target="_blank" rel="noopener">popeye&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/nilearn/index.html" target="_blank" rel="noopener">niLearn&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/pymvpa/index.html" target="_blank" rel="noopener">PyMVPA&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/mne/index.html" target="_blank" rel="noopener">MNE&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/niwidgets/index.html" target="_blank" rel="noopener">niwidgets&lt;/a>&lt;/p></description></item><item><title>Nose poke device for rats using arduino and 3d printed parts</title><link>https://open-neuroscience.com/post/nose-poke-device-for-rats-using-arduino-and-3d-printed-parts/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/nose-poke-device-for-rats-using-arduino-and-3d-printed-parts/</guid><description>&lt;p>This is a small set of instructions on how to build a nose poke device for rats, using an arduino, some 3D printed parts and some off-the-shelf electronic components.&lt;/p>
&lt;p>&lt;span class="embed-youtube" style="text-align:center; display: block;">&lt;/span>&lt;/p>
&lt;p>All the files necessary to reproduce this can be found &lt;a href="https://github.com/amchagas/poke_device_arduino" target="_blank" rel="noopener">here&lt;/a>&lt;/p>
&lt;p>The description is still not very detailed, but if something is not clear, do not hesitate to make contact! &lt;a href="mailto:openeuroscience@gmail.com">openeuroscience@gmail.com&lt;/a>&lt;/p>
&lt;p>If you ever use this informatíon, please cite it like this:&lt;/p>
&lt;div id="citecontent">
Chagas, Andre Maia (2014): Nose poke device using 3d printed parts and Arduino. fig&lt;b>share&lt;/b>.&lt;br /> &lt;a class="cite-doi" href="http://dx.doi.org/10.6084/m9.figshare.1057762">http://dx.doi.org/10.6084/m9.figshare.1057762&lt;/a>
&lt;/div></description></item><item><title>Open Microscopy Environment</title><link>https://open-neuroscience.com/post/ome-open-microscopy-environment/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/ome-open-microscopy-environment/</guid><description>&lt;p>The &lt;a href="https://www.openmicroscopy.org/site" target="_blank" rel="noopener">Open Microscopy Environment&lt;/a> is a collaborative project between several labs. They are developing file formats and software standards for light microscopy.&lt;/p>
&lt;p>Within the project they have &lt;a href="https://www.openmicroscopy.org/site/products/bio-formats" target="_blank" rel="noopener">BIO-formats&lt;/a>, a Java library for reading and writing data. It can be used in &lt;a href="https://imagej.nih.gov/ij/" target="_blank" rel="noopener">ImageJ&lt;/a> (it comes pre-packaged in &lt;a href="https://fiji.sc/" target="_blank" rel="noopener">FIJI&lt;/a> and matlab.&lt;/p>
&lt;p>&lt;a href="https://www.openmicroscopy.org/site/products/omero" target="_blank" rel="noopener">OMERO&lt;/a> is a client-server software for storage and data-analysis of microscopy images.&lt;/p></description></item><item><title>Operating systems</title><link>https://open-neuroscience.com/post/linux-distributions/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/linux-distributions/</guid><description>&lt;p>Linux is an open source operating system and it is the major OS used in servers and supercomputers.  &lt;a href="http://www.ubuntu.com" target="_blank" rel="noopener">Ubuntu&lt;/a>, one of the best known distributions has been gaining space in the personal computing scene, now days already being factory &lt;a href="http://www.omgubuntu.co.uk/2012/05/ubuntu-to-ship-on-5-of-all-pcs-sold-next-year" target="_blank" rel="noopener">shipped&lt;/a> by major manufacturers.&lt;/p>
&lt;p>But how practical is to migrate to a Linux distribution? Well, very. If one passes beyond the hassle of backing up data and installing a new OS, there are many advantages that come with it. For starters these OSs are safer than any Microsoft or Apple OS. There is a large community of users sharing solutions to problems, bugs and so on (there hasn’t been to today a widespread of any &lt;a href="http://en.wikipedia.org/wiki/Linux_malware" target="_blank" rel="noopener">malware through Linux systems&lt;/a>). Being open source, the distributions are perfect for customization, something really useful for science labs.&lt;/p>
&lt;p>A Small list of distributions that make a good starting point:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://www.debian.org/" target="_blank" rel="noopener">Debian&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://neuro.debian.net/" target="_blank" rel="noopener">NeuroDebian&lt;/a> (Debian oriented to neuroscience)&lt;/li>
&lt;li>&lt;a href="www.ubuntu.com">Ubuntu&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.linuxmint.com/" target="_blank" rel="noopener">Mint&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://en.opensuse.org/Main_Page" target="_blank" rel="noopener">OpenSuse&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://fedoraproject.org/" target="_blank" rel="noopener">Fedora&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="http://www.ros.org/" target="_blank" rel="noopener">ROS&lt;/a> –&lt;/p>
&lt;blockquote>
&lt;p>The robot operating system is a flexible framework for writing robot software. It is a collection of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behavior across a wide variety of robotic platforms.&lt;/p>
&lt;/blockquote></description></item><item><title>Slides</title><link>https://open-neuroscience.com/slides/example/</link><pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/slides/example/</guid><description>&lt;h1 id="create-slides-in-markdown-with-academic">Create slides in Markdown with Academic&lt;/h1>
&lt;p>&lt;a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic&lt;/a> | &lt;a href="https://sourcethemes.com/academic/docs/managing-content/#create-slides" target="_blank" rel="noopener">Documentation&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="features">Features&lt;/h2>
&lt;ul>
&lt;li>Efficiently write slides in Markdown&lt;/li>
&lt;li>3-in-1: Create, Present, and Publish your slides&lt;/li>
&lt;li>Supports speaker notes&lt;/li>
&lt;li>Mobile friendly slides&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="controls">Controls&lt;/h2>
&lt;ul>
&lt;li>Next: &lt;code>Right Arrow&lt;/code> or &lt;code>Space&lt;/code>&lt;/li>
&lt;li>Previous: &lt;code>Left Arrow&lt;/code>&lt;/li>
&lt;li>Start: &lt;code>Home&lt;/code>&lt;/li>
&lt;li>Finish: &lt;code>End&lt;/code>&lt;/li>
&lt;li>Overview: &lt;code>Esc&lt;/code>&lt;/li>
&lt;li>Speaker notes: &lt;code>S&lt;/code>&lt;/li>
&lt;li>Fullscreen: &lt;code>F&lt;/code>&lt;/li>
&lt;li>Zoom: &lt;code>Alt + Click&lt;/code>&lt;/li>
&lt;li>&lt;a href="https://github.com/hakimel/reveal.js#pdf-export" target="_blank" rel="noopener">PDF Export&lt;/a>: &lt;code>E&lt;/code>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="code-highlighting">Code Highlighting&lt;/h2>
&lt;p>Inline code: &lt;code>variable&lt;/code>&lt;/p>
&lt;p>Code block:&lt;/p>
&lt;pre>&lt;code class="language-python">porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
print(&amp;quot;Eating...&amp;quot;)
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="math">Math&lt;/h2>
&lt;p>In-line math: $x + y = z$&lt;/p>
&lt;p>Block math:&lt;/p>
&lt;p>$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p>
&lt;hr>
&lt;h2 id="fragments">Fragments&lt;/h2>
&lt;p>Make content appear incrementally&lt;/p>
&lt;pre>&lt;code>{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code>&lt;/pre>
&lt;p>Press &lt;code>Space&lt;/code> to play!&lt;/p>
&lt;p>&lt;span class="fragment " >
One
&lt;/span>
&lt;span class="fragment " >
&lt;strong>Two&lt;/strong>
&lt;/span>
&lt;span class="fragment " >
Three
&lt;/span>&lt;/p>
&lt;hr>
&lt;p>A fragment can accept two optional parameters:&lt;/p>
&lt;ul>
&lt;li>&lt;code>class&lt;/code>: use a custom style (requires definition in custom CSS)&lt;/li>
&lt;li>&lt;code>weight&lt;/code>: sets the order in which a fragment appears&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="speaker-notes">Speaker Notes&lt;/h2>
&lt;p>Add speaker notes to your presentation&lt;/p>
&lt;pre>&lt;code class="language-markdown">{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code>&lt;/pre>
&lt;p>Press the &lt;code>S&lt;/code> key to view the speaker notes!&lt;/p>
&lt;aside class="notes">
&lt;ul>
&lt;li>Only the speaker can read these notes&lt;/li>
&lt;li>Press &lt;code>S&lt;/code> key to view&lt;/li>
&lt;/ul>
&lt;/aside>
&lt;hr>
&lt;h2 id="themes">Themes&lt;/h2>
&lt;ul>
&lt;li>black: Black background, white text, blue links (default)&lt;/li>
&lt;li>white: White background, black text, blue links&lt;/li>
&lt;li>league: Gray background, white text, blue links&lt;/li>
&lt;li>beige: Beige background, dark text, brown links&lt;/li>
&lt;li>sky: Blue background, thin dark text, blue links&lt;/li>
&lt;/ul>
&lt;hr>
&lt;ul>
&lt;li>night: Black background, thick white text, orange links&lt;/li>
&lt;li>serif: Cappuccino background, gray text, brown links&lt;/li>
&lt;li>simple: White background, black text, blue links&lt;/li>
&lt;li>solarized: Cream-colored background, dark green text, blue links&lt;/li>
&lt;/ul>
&lt;hr>
&lt;section data-noprocess data-shortcode-slide
data-background-image="/img/boards.jpg"
>
&lt;h2 id="custom-slide">Custom Slide&lt;/h2>
&lt;p>Customize the slide style and background&lt;/p>
&lt;pre>&lt;code class="language-markdown">{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="custom-css-example">Custom CSS Example&lt;/h2>
&lt;p>Let&amp;rsquo;s make headers navy colored.&lt;/p>
&lt;p>Create &lt;code>assets/css/reveal_custom.css&lt;/code> with:&lt;/p>
&lt;pre>&lt;code class="language-css">.reveal section h1,
.reveal section h2,
.reveal section h3 {
color: navy;
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h1 id="questions">Questions?&lt;/h1>
&lt;p>&lt;a href="https://spectrum.chat/academic" target="_blank" rel="noopener">Ask&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://sourcethemes.com/academic/docs/managing-content/#create-slides" target="_blank" rel="noopener">Documentation&lt;/a>&lt;/p></description></item><item><title>Super-Releaser</title><link>https://open-neuroscience.com/post/super_releaser/</link><pubDate>Mon, 21 Mar 2016 10:00:12 +0000</pubDate><guid>https://open-neuroscience.com/post/super_releaser/</guid><description>&lt;p>Ever thought about making soft robots? The folks at &lt;a href="http://superreleaser.com/" target="_blank" rel="noopener">Super-Releaser&lt;/a> have, and they are doing very cool projects! Some for &lt;a href="http://superreleaser.com/project-profiles/" target="_blank" rel="noopener">medical applications and some for research&lt;/a> purposes. Check one of their cool robots below:&lt;/p>
&lt;div class="ytp-html5-clipboard">
&lt;span class="embed-youtube" style="text-align:center; display: block;">&lt;/span>
&lt;/div></description></item><item><title>5 Dollar PCR machine</title><link>https://open-neuroscience.com/post/5_dollar_pcr/</link><pubDate>Tue, 09 Jun 2015 09:53:14 +0000</pubDate><guid>https://open-neuroscience.com/post/5_dollar_pcr/</guid><description>&lt;p>The 5 dollar PCR machine is a project from &lt;a href="https://hackaday.io/dnhkng" target="_blank" rel="noopener">David Ng&lt;/a>.&lt;/p>
&lt;p>he created a very interesting design for the PCR machine. Instead of using eppendorfs, he is using teflon tubes and three different heating elements, which allows for cheaper (he has a working PCR machine for 5 dollars!) and faster DNA amplifications.&lt;/p>
&lt;p>&lt;a href="https://hackaday.io/project/1864-5-dna-replicator" target="_blank" rel="noopener">Here you can find the project page, with nice description and instructions&lt;/a>&lt;/p>
&lt;p>Below is a video from David explaining the project:&lt;/p>
&lt;iframe width="790" height="481" src="https://www.youtube.com/embed/S9Fq5CGj9Kg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe></description></item><item><title>Open bionics</title><link>https://open-neuroscience.com/post/open-bionics/</link><pubDate>Sat, 31 Jan 2015 22:56:41 +0000</pubDate><guid>https://open-neuroscience.com/post/open-bionics/</guid><description>&lt;p>&lt;a href="http://www.openbionics.org/" target="_blank" rel="noopener">The Open bionics project&lt;/a> was inspired by the Yale open hand project, aiming to develop light, affordable, and modular robot hands and myoelectric prosthesis. Also they want to make them easy to replicate using off the shelf materials. On the video below taken from their website you can see the hands in action, either as a prosthesis, or attached to a small drone being operated remotely.&lt;/p></description></item><item><title>Open prosthetics and robotics</title><link>https://open-neuroscience.com/post/prosthetics-and-robotics/</link><pubDate>Sat, 31 Jan 2015 22:08:16 +0000</pubDate><guid>https://open-neuroscience.com/post/prosthetics-and-robotics/</guid><description>&lt;p>With the rise of low cost 3D printers, and other cheap manufacturing tools, the field of robotics and prosthetics has been gaining quite a few open source projects. Two very nice compilations can be found at &lt;a href="//openrobothardware.org/">openrobot hardware&lt;/a> and at &lt;a href="http://softroboticstoolkit.com/" target="_blank" rel="noopener">Soft robotics toolkit&lt;/a>. Below are some related to neuroscience:&lt;/p>
&lt;p>&lt;a href="http://openeuroscience.com/hardware-projects/open-prosthetics-and-robotics/open-hand-project/" title="Open Hand Project" target="_blank" rel="noopener">The open hand project&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://openeuroscience.com/hardware-projects/open-prosthetics-and-robotics/the-yale-open-hand-project/" title="The Yale open hand project" target="_blank" rel="noopener">The Yale open hand project&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://openeuroscience.com/hardware-projects/open-prosthetics-and-robotics/open-bionics/" title="Open bionics" target="_blank" rel="noopener">Openbionics&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://openeuroscience.com/hardware-projects/open-prosthetics-and-robotics/fingertip-laser-sensor/" title="Fingertip laser sensor" target="_blank" rel="noopener">Fingertip laser sensor&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://openeuroscience.com/hardware-projects/open-prosthetics-and-robotics/takktile/" title="Takktile" target="_blank" rel="noopener">takktile&lt;/a>&lt;/p></description></item><item><title>Backyard Brains</title><link>https://open-neuroscience.com/post/backyard_brains/</link><pubDate>Tue, 29 Jan 2013 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/backyard_brains/</guid><description>&lt;p>&lt;a href="https://backyardbrains.com/" target="_blank" rel="noopener">Backyard brains&lt;/a> started out producing low cost, portable, electrophysiology systems to bring neuroscience to classrooms and help promote it.&lt;/p>
&lt;p>“Backyard brains wants to be for neuroscience, what the telescope is for astronomers” – meaning that the idea is that with a couple of hundred dollars anyone can get one of these recording systems and start doing experiments, like amateur astronomers can buy telescopes and start observing the cosmos.&lt;/p>
&lt;iframe width="790" height="444" src="https://www.youtube.com/embed/-mKen7tCDCs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe></description></item><item><title>10$ smartphone microscope</title><link>https://open-neuroscience.com/post/10_smartphone_microscope/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/10_smartphone_microscope/</guid><description>&lt;p>This &lt;a href="http://www.instructables.com/id/10-Smartphone-to-digital-microscope-conversion/%20how%20to%20use%20a%20smartphone%20for%20big%20amplifications" target="_blank" rel="noopener">neat little project&lt;/a> uses some plexi-glass, lens extracted from a laser pointer to harvest the power of smartphone cameras for some very big amplifications! Yoshinok manged to see cell plasmolysis and some other cool features with it.&lt;/p>
&lt;p>&lt;img src="https://i2.wp.com/www.instructables.com/files/deriv/FX0/QLMO/HMMF5O43/FX0QLMOHMMF5O43.MEDIUM.jpg?w=800" alt="Vegetal slice" data-recalc-dims="1" />&lt;/p></description></item><item><title>Addgene</title><link>https://open-neuroscience.com/post/addgene/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/addgene/</guid><description>&lt;p>&lt;a href="http://www.addgene.org/" target="_blank" rel="noopener">Addgene&lt;/a> is a non-profit company that makes the share of plasmids easier by making a plasmid database and linking them to the papers where they were described. In this way they take on the job of maintaining plasmids and shipping them to requesting scientists.&lt;/p></description></item><item><title>Allen Brain Map</title><link>https://open-neuroscience.com/post/allen-brain-map/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/allen-brain-map/</guid><description>&lt;p>&lt;a href="http://www.brain-map.org/" target="_blank" rel="noopener">The Brain Map&lt;/a> is one of the initiatives of the &lt;a href="http://www.alleninstitute.org/about/" target="_blank" rel="noopener">Allen Institute&lt;/a>.&lt;/p>
&lt;p>It is a data portal that encompasses different projects:&lt;/p>
&lt;blockquote>
&lt;p>the Allen Institute has created a set of large-scale programs to understand the fundamentals of the cortex. We will be focusing our understanding through simultaneous study of the brain&amp;rsquo;s components, computation and cognition.&lt;/a>&lt;/p>
&lt;p>the Allen Institute has produced a collection of open science resources that give users a powerful way to explore gene expression data, neural connections, single cell characterization and neuroanatomy. All of our resources are openly accessible via the Allen Brain Atlas data portal.&lt;/p>
&lt;/blockquote></description></item><item><title>Attys</title><link>https://open-neuroscience.com/post/attys/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/attys/</guid><description>&lt;p>&lt;a href="https://www.attys.tech/" target="_blank" rel="noopener">Attys&lt;/a> is an wearable data acquisition device with a special focus on biomedical signals such as heart activity (ECG), muscle activity (EMG) and brain activity (EEG). It’s open firmware, open API and has open source applications on github in C++ and JAVA to encourage people to create their own custom versions for mobile devices, tablets and PC.&lt;/p>
&lt;p>The story of the Attys started when Dr. Bernd Porr filmed numerous youTube clips to educate the public about the possibilities and limits of biosignal measurement (&lt;a href="http://biosignals.berndporr.me.uk">http://biosignals.berndporr.me.uk&lt;/a>) which are featured here: &lt;a href="http://openeuroscience.com/hardware-projects/human-electrophysiology/bio-signal/" target="_blank" rel="noopener">BPM link&lt;/a>&lt;/p>
&lt;p>The site has been very popular ever since and visitors have been asking if a ready made bio-amp could be made available. This year Dr. Porr then decided to make one. This was the birth of the Attys.&lt;/p>
&lt;p>Attys is also a general educational tool to measure any physical quantity such as temperature, pressure or light intensity. It works with Google’s open source Science Journal and turns every Android phone or tablet into an electronic lab book / oscilloscope. Of course one can measure biosignals with it, too.&lt;/p>
&lt;p>Vasso Georgiadou has been the main presenter for our biosignal channel. Here, she shows off the Attys:&lt;/p>
&lt;iframe width="790" height="444" src="https://www.youtube.com/embed/TG5cRvgFEDA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe></description></item><item><title>BB LED Matrix</title><link>https://open-neuroscience.com/post/bb_led_matrix/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/bb_led_matrix/</guid><description>&lt;p>This project uses a 32X32 LED array (1024 LEDs in total) and a beagle bone black board. &lt;a href="https://bikerglen.com/projects/lighting/led-panel-1up/" target="_blank" rel="noopener">The page describing the project&lt;/a> has very nice explanations on how the whole system works (and LED displays in general).&lt;/p>
&lt;p>From this project, the creator &lt;a href="https://twitter.com/bikerglen" target="_blank" rel="noopener">Glen Akins&lt;/a>, went on to construct a 3X2 matrix of 32X32 LEDS, or a total of 6144 RGB LEDs that have a 200Hz refresh rate! Check out the video below of the panel in action:&lt;/p>
&lt;iframe width="790" height="444" src="https://www.youtube.com/embed/LBeVMGOgWvY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe></description></item><item><title>Big Neuron</title><link>https://open-neuroscience.com/post/big-neuron/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/big-neuron/</guid><description>&lt;p>Big Neuron wants to create a standard for the field of single neuron reconstruction. Because the data available comes from different structures, different organisms, using different collection and analyses algorithms and is in the range of petabytes (according to the project site), there is a strong need for standards that will allow this huge amount of data to be compared.&lt;/p>
&lt;p>The project aims to develop a common platform and algorithms for data analysis to benchmark as many open source neuronal reconstruction models as possible.&lt;/p>
&lt;p>&lt;img src="https://avatars1.githubusercontent.com/u/15747935?s=200&amp;amp;v=4" alt="">&lt;/p>
&lt;p>For more information visit their &lt;a href="https://github.com/BigNeuron" target="_blank" rel="noopener">GitHub repository&lt;/a>.&lt;/p></description></item><item><title>Blinkenschild</title><link>https://open-neuroscience.com/post/blinkeschild/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/blinkeschild/</guid><description>&lt;p>&lt;a href="https://hackaday.io/project/363-blinkenschild" target="_blank" rel="noopener">Blinkenschild&lt;/a> is a portable sign consisting of 960 RGB LEDs. The images/movies to be displayed are stored in a SD card in a Teensy3 board and controlled via bluetooth. Resolution is not as high as LCD monitors but the refresh rate is much higher:&lt;/p>
&lt;pre>&lt;code>This is done in realtime and pixelvalues are recalculated before display.
This is still too fast so i had to add 30 ms delay between the frames or we would not perceive it as a fluid animation but rather just blinking bright light.
&lt;/code>&lt;/pre>
&lt;iframe width="790" height="593" src="https://www.youtube.com/embed/VX14pmky07Q" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe></description></item><item><title>Boinc</title><link>https://open-neuroscience.com/post/boinc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/boinc/</guid><description>&lt;p>Boinc is a platform for &lt;a href="http://boinc.berkeley.edu/trac/wiki/VolunteerComputing" target="_blank" rel="noopener">volunteer computing.&lt;/a> Briefly, volunteer computer is a system where computer processor&amp;rsquo;s idle time (those periods where your computer is on, but not being used for anything) is turned into calculation time via a custom written software.&lt;/p>
&lt;p>This idea got a lot of attention with the &lt;a href="http://setiathome.ssl.berkeley.edu/" target="_blank" rel="noopener">seti@home&lt;/a> project, where the computers of volunteers were transformed into a sort of supercomputer to analyse radio telescope date.&lt;/p>
&lt;p>The Boinc project provides a platform where different projects can be created and launched on the &lt;a href="http://boinc.berkeley.edu/" target="_blank" rel="noopener">platform&amp;rsquo;s website.&lt;/a> Once online, volunteers can decide to which project they want to spare their computer cycles and help crushing numbers. Currently (11/12/14) the platform has about 229,396 active volunteers on 751,864 computers. with a 24-hour average of 8.024 PetaFLOPS. (not bad!)&lt;/p>
&lt;p>Projects come from universities as well as private sector and range from medicine and mathematics to games.&lt;/p></description></item><item><title>BPM Biosignal</title><link>https://open-neuroscience.com/post/bpm_biosignal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/bpm_biosignal/</guid><description>&lt;p>BPM Biosignal is a two stage amplifier created mainly for educational purposes.&lt;/p>
&lt;p>Check their &lt;a href="https://www.youtube.com/c/BPMbiosignals" target="_blank" rel="noopener">YouTube Channel&lt;/a>.&lt;/p></description></item><item><title>Brain Map</title><link>https://open-neuroscience.com/post/brain_map/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/brain_map/</guid><description>&lt;p>&lt;a href="https://people.ece.cornell.edu/land/courses/ece4760/FinalProjects/s2012/pmd68_mab448/pmd68_mab448/index.html" target="_blank" rel="noopener">BrainMap&lt;/a> expands the accessible DIY projects for brain activity measurements.&lt;/p>
&lt;p>This is the conclusion project of Patrick Dear and Mark Bunney Jr. at Cornell university where they used infrared leds to measure differences in blood flow at the scalp and map the motor cortex.&lt;/p></description></item><item><title>BrainBrowser</title><link>https://open-neuroscience.com/post/brainbrowser/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/brainbrowser/</guid><description>&lt;p>BrainBrowser is a collection of open source, web-based 3D data visualization tools, mainly for neuroimaging studies. It is built using open technologies such as WebGL and HTML5. It allows exploration of cortical surface models (MNI and Wavefront OBJ, as well as FreeSurfer ASCII surface format) and volumetric MINC data. This project is currently maintained by &lt;a href="http://www.tareksherif.ca/" target="_blank" rel="noopener">Tarek Sherif&lt;/a> at McGill University, and the source code is available on &lt;a href="https://github.com/aces/brainbrowser" target="_blank" rel="noopener">GitHub&lt;/a>.&lt;/p>
&lt;p>You can find more info on the &lt;a href="https://brainbrowser.cbrain.mcgill.ca/" target="_blank" rel="noopener">website&lt;/a>&lt;/p></description></item><item><title>Computer Vision and motion tracking software</title><link>https://open-neuroscience.com/post/computer-vision-and-motion-tracking-software/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/computer-vision-and-motion-tracking-software/</guid><description>&lt;p>Motion tracking can be really useful in neurosciences, for automatic measurements of behaviour, among other things. Here you’ll find a small list of tracking softwares or libraries used to build such softwares:&lt;/p>
&lt;p> &lt;/p>
&lt;p>&lt;strong>Complete softwares:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="http://ctrax.sourceforge.net/index.html" target="_blank" rel="noopener">Ctrax&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;figure style="width: 128px" class="wp-caption alignnone">[&lt;img src="https://i0.wp.com/ctrax.sourceforge.net/images/ctrax-logo2b_128.png?resize=128%2C128" alt="" width="128" height="128" data-recalc-dims="1" />](https://i0.wp.com/ctrax.sourceforge.net/images/ctrax-logo2b_128.png)&lt;figcaption class="wp-caption-text">taken from: http://ctrax.sourceforge.net/index.html&lt;/figcaption>&lt;/figure>&lt;/li>
&lt;ul>
&lt;li>
&lt;blockquote>
&lt;p>Ctrax is an open-source, freely available, machine vision program for estimating the positions and orientations of many walking flies, maintaining their individual identities over long periods of time. It was designed to allow high-throughput, quantitative analysis of behavior in freely moving flies.&lt;/ul>&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;p> &lt;/p>
&lt;ul>
&lt;li>The bio tracking project, designed for multiple object tracking, developed at Georgia tech:&lt;/li>
&lt;li>&lt;a href="http://www.bio-tracking.org/" target="_blank" rel="noopener">&lt;a href="http://www.bio-tracking.org/">http://www.bio-tracking.org/&lt;/a>&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;span class="embed-youtube" style="text-align:center; display: block;">&lt;/span>&lt;/p>
&lt;p> &lt;/p>
&lt;p> &lt;/p>
&lt;ul>
&lt;li>Community Core Vision: Built with computer vision and machine sensing in mind, they mention multi touch applications as one of their focus on the website.&lt;/li>
&lt;li>&lt;a href="http://ccv.nuigroup.com/">http://ccv.nuigroup.com/&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;span class="embed-youtube" style="text-align:center; display: block;">&lt;/span>&lt;/p>
&lt;p> &lt;/p>
&lt;p> &lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://derek.simkowiak.net/motion-tracking-with-python/" target="_blank" rel="noopener">Motion Tracking using python&lt;/a>: Independent developed software by Derek Simkowiak, in a project he ran a couple of years back with his daughter, to track Gerbills&lt;/li>
&lt;/ul>
&lt;p>&lt;span class="embed-youtube" style="text-align:center; display: block;">&lt;/span>&lt;/p>
&lt;p> &lt;/p>
&lt;p> &lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://personal.ee.surrey.ac.uk/Personal/Z.Kalal/tld.html" target="_blank" rel="noopener">Tracking-Learning-Detection&lt;/a>: Developed by &lt;a href="http://personal.ee.surrey.ac.uk/Personal/Z.Kalal/index.html" target="_blank" rel="noopener">Zdenek Kalal&lt;/a> this software intends to track pretty much anything (object determination can be done via mouse) in real time and to learn features from the object as tracking goes on.&lt;/li>
&lt;/ul>
&lt;p>&lt;span class="embed-youtube" style="text-align:center; display: block;">&lt;/span>&lt;/p>
&lt;p> &lt;/p>
&lt;p> &lt;/p>
&lt;ul>
&lt;li> &lt;a href="http://openvisionc.sourceforge.net/" target="_blank" rel="noopener">Open Vision Control&lt;/a>: Developed on top of OpenCV (see below) in Python, it is a general purpose tracking software with several applications&lt;/li>
&lt;/ul>
&lt;p>&lt;span class="embed-youtube" style="text-align:center; display: block;">&lt;/span>&lt;/p>
&lt;p> &lt;/p>
&lt;p> &lt;/p>
&lt;ul>
&lt;li>SwisTrack: Developed at EPFL, it is also a tracking system for multiple objects&lt;figure id="attachment_744" style="width: 300px" class="wp-caption aligncenter">&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://i0.wp.com/openeuroscience.com/wp-content/uploads/2014/04/800px-swistrack4-ubuntu.png" target="_blank" rel="noopener">&lt;img class="size-medium wp-image-744" src="https://i0.wp.com/openeuroscience.com/wp-content/uploads/2014/04/800px-swistrack4-ubuntu.png?resize=300%2C211" alt="From http://en.wikibooks.org/wiki/Swistrack" width="300" height="211" srcset="https://i0.wp.com/openeuroscience.com/wp-content/uploads/2014/04/800px-swistrack4-ubuntu.png?w=800 800w, https://i0.wp.com/openeuroscience.com/wp-content/uploads/2014/04/800px-swistrack4-ubuntu.png?resize=300%2C212 300w, https://i0.wp.com/openeuroscience.com/wp-content/uploads/2014/04/800px-swistrack4-ubuntu.png?resize=768%2C541 768w" sizes="(max-width: 300px) 100vw, 300px" data-recalc-dims="1" />&lt;/a>&lt;figcaption class="wp-caption-text">From &lt;a href="http://en.wikibooks.org/wiki/Swistrack">http://en.wikibooks.org/wiki/Swistrack&lt;/a>&lt;/figcaption>&lt;/figure>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://infoscience.epfl.ch/record/85929">http://infoscience.epfl.ch/record/85929&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://infoscience.epfl.ch/record/125704">http://infoscience.epfl.ch/record/125704&lt;/a>&lt;/li>
&lt;/ul>
&lt;p> &lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0042247" target="_blank" rel="noopener">Tracking software for Drosophila&lt;/a>, by Colomb &lt;em>et al&lt;/em>.&lt;/li>
&lt;/ul>
&lt;p> &lt;/p>
&lt;p>&lt;strong>Computer vision/tracking libraries:&lt;/strong>&lt;/p>
&lt;p>&lt;a href="http://opencv.org/" target="_blank" rel="noopener">Open CV&lt;/a> is a library for machine learning and computer vision. It is written for different computer languages and different operational systems.&lt;/p>
&lt;blockquote>
&lt;p>The library has more than 2500 optimized algorithms, which includes a comprehensive set of both classic and state-of-the-art computer vision and machine learning algorithms. These algorithms can be used to detect and recognize faces, identify objects, classify human actions in videos, track camera movements, track moving objects, extract 3D models of objects, produce 3D point clouds from stereo cameras, stitch images together to produce a high resolution image of an entire scene, find similar images from an image database, remove red eyes from images taken using flash, follow eye movements, recognize scenery and establish markers to overlay it with augmented reality, etc.&lt;/p>
&lt;/blockquote>
&lt;p> &lt;/p>
&lt;p>&lt;a href="http://www.simplecv.org/" target="_blank" rel="noopener">Simple CV&lt;/a> is a framework that tries to simplify the development of software that require computer vision/machine learning, since a lot of researchers have the necessity of building on such concepts, but sometimes don’t have the time/training necessary to do so.&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Crowd funding</title><link>https://open-neuroscience.com/post/crowd-funding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/crowd-funding/</guid><description>&lt;p>As many other things that are being decentralized with the advent of the internet, so is research. One of the very things being decentralized is the funding source for research projects. This movement is called crowdfunding, and it is already present and strong for other areas, such as funding of technological, and social projects. They are organized via specialised websites that puts people needing funding with people who are willing to support it:&lt;/p>
&lt;p>The idea is somewhat simple. Researchers make a video of what their project/research idea is and ask for a certain value to fund the project. If people find the idea interesting they make a donation via the website hosting that project.&lt;/p>
&lt;p>There are normally two types of funding options, fixed, where the researchers only get the money pledged if the donations reach that value (or pass it, and if the minimum is not reached the money is returned to the pledgers), or variable where the researchers get to keep the money raised even if it didn’t reach the amount pledged for. In both cases the website keeps a percentage of the money raised in case the funding is successful.&lt;/p>
&lt;p>Articles about it can be founded &lt;a href="http://www.theguardian.com/higher-education-network/blog/2013/nov/11/science-research-funding-crowdfunding-excellence" target="_blank" rel="noopener">here&lt;/a> and &lt;a href="http://www.nyas.org/publications/EBriefings/Detail.aspx?cid=82c4e4b4-f200-49b3-b333-c41e1e2f46aa" target="_blank" rel="noopener">here&lt;/a>&lt;/p>
&lt;p>Some crowdfunding sources are listed below:&lt;/p>
&lt;p>&lt;a href="http://www.sciencestarter.de/">http://www.sciencestarter.de/&lt;/a> – Research crowdfunding portal based in germany&lt;/p>
&lt;p>&lt;a href="https://experiment.com/" target="_blank" rel="noopener">https://www.microryza.com/&lt;/a> – Research crowdFunding portal based in the US. (update – this is now called experiment.com)&lt;/p>
&lt;p>&lt;a href="https://www.kickstarter.com/?ref=nav" target="_blank" rel="noopener">www.kickstarter.com&lt;/a> – A Crowdfunding portal that also works for science projects. Currently (as in 02.02.14) only for projects based on the US, Canada, UK, Australia and New Zealand.&lt;/p>
&lt;p>&lt;a href="http://www.indiegogo.com/" target="_blank" rel="noopener">www.indiegogo.com&lt;/a> Also a crowdfunding portal that has science related projects. Differently from kickstarter, they support projects from all over the world, except countries in the US OFAC sanctions list&lt;/p>
&lt;p>&lt;a href="https://www.crowdsupply.com/">https://www.crowdsupply.com/&lt;/a> is another crowdfunding portal, based in the US, that focus on product development.&lt;/p>
&lt;p>&lt;a href="http://www.ulule.com/">http://www.ulule.com/&lt;/a> – A crowdfunding portal based in Europe. They have a nice post on &lt;a href="http://blog.ulule.com/post/700805254/a-brief-history-of-crowdfunding?_ga=1.104969667.1799200825.1396358648" target="_blank" rel="noopener">crowdfunding history&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://www.petridish.org/" target="_blank" rel="noopener">www.petridish.org&lt;/a> Research crowdfunding portal, but it seems that they haven’t been taking new projects for a while (since February 2013)&lt;/p>
&lt;p>&lt;del>&lt;a href="http://sciflies.org/about">&lt;a href="http://www.sciflies.org">www.sciflies.org&lt;/a>&lt;/a> – Research crowdfunding portal based in Florida, their differential is that all projects posted there have to be approved by an anonymous peer review process, currently done by the American Association for Advancement of science. And 100% of the money goes to the project, there are no administrative fees. It was not clear from the website if only US based projects are funded&lt;/del>&lt;/p>
&lt;p>&lt;a href="http://www.rockethub.com/">http://www.rockethub.com/&lt;/a> – General crowdfunding portal, that has also a science division where everyone can start a project.&lt;/p>
&lt;p>&lt;a href="http://scifundchallenge.org/" target="_blank" rel="noopener">scifundchallenge.org&lt;/a> – Built and maintained by the &lt;a href="http://opensciencefederation.com/" target="_blank" rel="noopener">open science federation&lt;/a>, this project has three main “departments”, all trying to bridge the gap in between science and the general public: Teach and encourage scientists to outreach, connect the public directly with scientists and science crowdfund.&lt;/p></description></item><item><title>Data repositories</title><link>https://open-neuroscience.com/post/data-repositories/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/data-repositories/</guid><description>&lt;p>In here are some examples of tools that can be used to share/store data collected. Published a paper and think that people would benefit from looking at the raw data? Want to make that data that has been stored for years useful? Here you’ll find some options on how to do it.&lt;/p>
&lt;p>&lt;a href="http://figshare.com/about" target="_blank" rel="noopener">FigShare&lt;/a>: Free repository that allows storage of any sort of files (data, code, schematics) and gives each of them a digital object identifier (also keeping track to any changes made), which makes them citable. Also the website has tools to share the data.&lt;figure id="attachment_1270" style="width: 167px" class="wp-caption aligncenter">&lt;/p>
&lt;p>&lt;img src="https://upload.wikimedia.org/wikipedia/commons/d/df/Figshare_logo.svg" alt="">&lt;/p>
&lt;p> &lt;/p>
&lt;p>&lt;a href="http://datadryad.org/" target="_blank" rel="noopener">Dryad&lt;/a>: Repository for data, works in a similar way to Figshare, but Dryad also has a data submission system integrated with a (growing) number of journals, so that paper submissions are synchronized with the data sharing.&lt;/p>
&lt;p> &lt;/p>
&lt;p> &lt;/p></description></item><item><title>DIY PCR</title><link>https://open-neuroscience.com/post/diy_pcr/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/diy_pcr/</guid><description>&lt;p>&lt;a href="https://hackaday.io/hacker/24043-katherina-baranova" target="_blank" rel="noopener">Katharina&lt;/a> and &lt;a href="https://hackaday.io/hacker/24028-alex-bondarekno" target="_blank" rel="noopener">Alex&lt;/a> are developing a classic PCR machine: 16 samples and a heated lid.&lt;/p>
&lt;p>&lt;a href="https://hackaday.io/project/2548-open-source-thermal-cycler" target="_blank" rel="noopener">You can find more details of their project here&lt;/a>&lt;/p>
&lt;p>Here is a demo video:&lt;/p>
&lt;iframe width="500" height="281" src="https://www.youtube.com/embed/R7leQlkBKJw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe></description></item><item><title>DremelFuge</title><link>https://open-neuroscience.com/post/dremelfuge/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/dremelfuge/</guid><description>&lt;p>&lt;a href="https://www.thingiverse.com/thing:1483" target="_blank" rel="noopener">DremelFuge&lt;/a> is a very simple and clever centrifuge, buit perhaps not the safest one (be careful if you end up using it!).&lt;/p>
&lt;p>It takes advantage of 3d printing technology to print an adaptor that goes on to a Dremel (a precision tool that has really high rotation rates). It was created by &lt;a href="https://www.thingiverse.com/cathalgarvey/about" target="_blank" rel="noopener">Cathal&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://cdn.thingiverse.com/renders/ff/74/4c/b2/c4/2009-12-30-023824_display_large_preview_featured.jpg" alt="3d printed dremel attachment" title="DremelFuge">&lt;/p></description></item><item><title>Fiji</title><link>https://open-neuroscience.com/post/fiji/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/fiji/</guid><description>&lt;p>&lt;a href="http://fiji.sc/Fiji" target="_blank" rel="noopener">Fiji&lt;/a> is a distribution of ImageJ. The idea of the developers is to make the life of scientists easier by bundling ImageJ with nicely organised plugins and auto update function.&lt;/p>
&lt;blockquote>
&lt;p>Fiji compares to ImageJ as Ubuntu compares to Linux.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="http://openeuroscience.wordpress.com/software/imagej/" title="ImageJ" target="_blank" rel="noopener"> &lt;img src="https://i1.wp.com/rsbweb.nih.gov/ij/images/imagej-logo.gif?w=800" alt="imagej logo" data-recalc-dims="1" />&lt;/a>&lt;/p></description></item><item><title>Interesting projects</title><link>https://open-neuroscience.com/post/other-interesting-projects/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/other-interesting-projects/</guid><description>&lt;p>It is great that there are other interesting projects out there that are also concerned with making science available to more people! Here is a short list of projects I came across. They are not necessarily focusing on open source, but worth getting to know anyhow:&lt;/p>
&lt;p>&lt;a href="http://trendinafrica.org/who-are-we/our-mission/" target="_blank" rel="noopener">TReND in Africa&lt;/a>: Where TReND stands for Teaching and Research in Neuroscience for Development, is a initiative to stop the brain drain in sub-Saharan Africa. They are a non-profit organisation led by a small group of researchers that are training and teaching local African scientists. On top of that they also coordinate the collection of money and equipment donation to establish permanent research facilities on African universities.&lt;/p>
&lt;p>&lt;a href="https://github.com/cbonsig/open-stent" target="_blank" rel="noopener">Open Stent project&lt;/a>: Although more into medicine rather than neuroscience is the open stent project is developed by &lt;a href="http://www.nitinol.com/" target="_blank" rel="noopener">NDC&lt;/a>. Their stent was first designed to aid customer interaction. It seems that when giving examples on design improvements, they would always bump into proprietary issues, therefore they developed their own design and made the blueprints available for everyone.&lt;/p>
&lt;p>&lt;a href="https://github.com/GliaX/Stethoscope" target="_blank" rel="noopener">Open source stethoscope:&lt;/a> This is a 3D printed stethoscope, developed by Tarek Loubani, a doctor that works in Gaza and with a 3D printer and 5 dollars worth of materials (tubes, ear piece and plastic to be printed) he and his group were able to outperform the Littmann Cardiology 3, a market leader, that sells for over 20X the price of the printed one.&lt;/p>
&lt;p>&lt;a href="http://hackteria.org/wiki/index.php/Main_Page" target="_blank" rel="noopener">Hackteria&lt;/a> – Is a wiki page that collects several DIY projects related to Biology and Open Source Art Projects that use Biology, LifeSciences, Biotechnology. Among the projects listed are centrifuges, water baths, field microscopes.&lt;/p>
&lt;p>&lt;a href="http://www.appropedia.org/Open-source_Lab" target="_blank" rel="noopener">Open Source Lab:&lt;/a>   A project by Prof. Joshua Pearce of Michigan University. It advocates in favour of researchers building their own lab equipment using 3D printers and other “off the shelf” available items. Although the main focus of the lab are environmental problems, a lot of the solutions there stated can easily be harvested/modified for neuroscience purposes.&lt;/p>
&lt;p>&lt;a href="https://www.cooking-hacks.com/documentation/tutorials/ehealth-biometric-sensor-platform-arduino-raspberry-pi-medical" target="_blank" rel="noopener">e-Health sensor platform&lt;/a>: A device created at the open source division of Libelium, called cooking hacks. It allows integration of several health related sensors (blood pressure, oxygen level, glucose level, muscle activity, airflow, galvanic response) into arduino and raspberry pi. Which can be used to make real time monitoring of patients and/or test subjects.&lt;/p>
&lt;p>&lt;a href="http://www.bitalino.com/" target="_blank" rel="noopener">Bitalino&lt;/a>: On the same lines as e-health (above), the Bitalino is a complete platform for measurements of biosignals, but this project is more focused on learning and prototyping. It also has free software for data visualization.&lt;/p>
&lt;p>&lt;a href="http://littledevices.org/research/" target="_blank" rel="noopener">Little Devices&lt;/a>: develops tools to improve health care and diagnostics. They are open source, and DIY.&lt;/p>
&lt;p>&lt;a href="http://publiclab.org/" target="_blank" rel="noopener">Public lab&lt;/a>: Involved with environmental issues, Public lab is a platform that empowers communities to measure environmental variables around them. This way hard data concerning water, air and soil pollution can be used to put pressure on governments.&lt;/p>
&lt;blockquote>
&lt;p>&lt;a href="https://arxiv.org/abs/1606.01196" target="_blank" rel="noopener">Open source Muon Detector:&lt;/a> an undergraduate-level physics project that incorporates various aspects of machine- and electronics-shop technical development. The desktop muon detector is a self-contained apparatus that employs plastic scintillator as a detection medium and a silicon photomultiplier for light collection. These detectors can be used in conjunction with the provided software to make interesting physics measurements. The total cost of each counter is approximately $100.&lt;/p>
&lt;/blockquote></description></item><item><title>NeuroElectro</title><link>https://open-neuroscience.com/post/neuroelectro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/neuroelectro/</guid><description>&lt;p>&lt;a href="http://neuroelectro.org/" target="_blank" rel="noopener">NeuroElectro&lt;/a> wants to extract information about neuron types, morphology, electrophysiology properties from papers, using text mining algorithms and gathers them in a database.&lt;/p>
&lt;blockquote>
&lt;p>Our goal is to facilitate the discovery of &lt;a href="http://neuroelectro.org/neuroelectro/neuron/clustering" target="_blank" rel="noopener">neuron-to-neuron relationships&lt;/a> and better understand the role of functional diversity across neuron types.&lt;/p>
&lt;p> &lt;/p>
&lt;/blockquote>
&lt;p>&lt;span class="embed-youtube" style="text-align:center; display: block;">&lt;/span>&lt;/p>
&lt;p> &lt;/p>
&lt;p> &lt;/p></description></item><item><title>Open BCI</title><link>https://open-neuroscience.com/post/bio_amp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/bio_amp/</guid><description>&lt;p>BioAmp is a biopotential acquisition device (EEG, ECG, EMG, EOG, etc.) developed in the Prototyping Laboratory at the School of Engineering of the National University of Entre Rios (Argentina).&lt;/p>
&lt;p>&lt;img src="./bio_amp_frontal.jpg" alt="Frontal view">&lt;/p>
&lt;p>Main features:&lt;/p>
&lt;pre>&lt;code>8 independent acquisition channels
24 bits of resolution per channel
2000 Hz is the maximum sampling frequency
USB connection (power and data transmission)
inputs for trigger signal
designed under electrical safety standards for medical use (electrical insulation, touch-proof connectors, etc.)
&lt;/code>&lt;/pre>
&lt;p>A very interesting feature of the BioAmp is the possibility of combining two amplifiers to double the number of recording channels. It is also possible to program each channel individually, offering the possibility of registering different types of signal simultaneously. For example, EEG, EOG, and EMG could be recorded during a sleep study, or EMG and ECG during a physical activity study, etc.&lt;/p>
&lt;p>&lt;img src="./bio_amp_back.jpg" alt="Posterior view">&lt;/p>
&lt;p>This project is currently in evolution and development, continually changes and updates are made to improve the product. Both the hardware source files (PCB and cabinet for 3D printing) and firmware are available in the project repository.&lt;/p>
&lt;p>For more information on this project and other projects carried out in the Prototyping Laboratory, visit the laboratory website.&lt;/p>
&lt;iframe id="video-2209-1_youtube_iframe" allowfullscreen="1" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" title="YouTube video player" src="https://www.youtube.com/embed/F7R7IxtyfGw?controls=0&amp;amp;rel=0&amp;amp;disablekb=1&amp;amp;showinfo=0&amp;amp;modestbranding=0&amp;amp;html5=1&amp;amp;iv_load_policy=3&amp;amp;autoplay=0&amp;amp;end=0&amp;amp;loop=0&amp;amp;playsinline=0&amp;amp;start=0&amp;amp;nocookie=false&amp;amp;enablejsapi=1&amp;amp;origin=https%3A%2F%2Fopeneuroscience.com&amp;amp;widgetid=1" width="829" height="466.3125" frameborder="0">&lt;/iframe></description></item><item><title>Open BCI</title><link>https://open-neuroscience.com/post/open-bci/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open-bci/</guid><description>&lt;p>&lt;a href="https://openbci.com/" target="_blank" rel="noopener">OpenBCI&lt;/a> is a complete open source EEG system that can be built either on top of an Arduino (8-bit system), or on top of chipKIT (32-bit system), which gives the system more local memory and allows for faster speeds.&lt;/p>
&lt;p>All software code and hardware (including a model for a 3D printable headset) plans can be found freely available at their &lt;a href="https://openbci.com/index.php/downloads" target="_blank" rel="noopener">download section&lt;/a> or at &lt;a href="https://github.com/OpenBCI" target="_blank" rel="noopener">GitHub&lt;/a>.&lt;/p></description></item><item><title>Open EEG</title><link>https://open-neuroscience.com/post/open_eeg/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open_eeg/</guid><description>&lt;p>&lt;a href="http://openeeg.sourceforge.net/doc/index.html" target="_blank" rel="noopener">The openEEG&lt;/a> project aims at describing and putting manuals for building a two channel EEG system for about U$200.
More on instructions on how to build one, can be found &lt;a href="http://openeeg.sourceforge.net/doc/SimpleEEG/" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>The latest update on the page seems to be a bit old, but Olimex sells the necessary PCB boards and accessories to &lt;a href="https://www.olimex.com/Products/EEG/OpenEEG/" target="_blank" rel="noopener">build the device&lt;/a>. They also sell the &lt;a href="https://www.olimex.com/Products/EEG/OpenEEG/EEG-SMT/open-source-hardware" target="_blank" rel="noopener">openEEG completely assembled&lt;/a>.&lt;/p></description></item><item><title>Open Ephys</title><link>https://open-neuroscience.com/post/open-ephys/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open-ephys/</guid><description>&lt;p>Open Ephys is a great initiative to create a suite that encompasses hardware for LFP and spiking recording, optogenetics combined with custom written software for microstimulation, environmental stimuli, extracellular recording and optogen. perturbations. Their ultimate goal is to create a system optimized for tetrodes and optogenetics where one is able to record and analyse data in real-time. On the &lt;a href="http://www.open-ephys.org/" target="_blank" rel="noopener">project’s website&lt;/a> one can download plans on how to build the devices and estimate on part cost (which is much, much lower than commercially available systems out there).&lt;/p></description></item><item><title>Open ExG</title><link>https://open-neuroscience.com/post/open_exg/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open_exg/</guid><description>&lt;p>OpenHardwareExG: is a project that provides both open source hardware and software for the measurement and analysis of different types of biosignals&lt;/p>
&lt;p>From the &lt;a href="http://openelectronicslab.github.io/OpenHardwareExG/" target="_blank" rel="noopener">project page&lt;/a>:&lt;/p>
&lt;pre>&lt;code>About the OpenHardwareExG project
Project goals
The main goal of the project is to build a device that allows the creation of electrophysiologic signal processing applications. In addition:
Hardware and software that we develop will have a free/open source license. We also prefer to use hardware and software that are free/open source.
We would like to keep the hardware &amp;quot;DIY compatible&amp;quot; (hand solderable, with parts that are readily available in small quantities, etc.)
For us, this is a hobby and learning project. It's important to keep it fun, and take the time to learn along the way.
&lt;/code>&lt;/pre></description></item><item><title>Open lab notebooks</title><link>https://open-neuroscience.com/post/open-lab-notebooks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open-lab-notebooks/</guid><description>&lt;p>&lt;a href="http://en.wikipedia.org/wiki/Open_notebook_science" target="_blank" rel="noopener">Open notebooks&lt;/a> are opening up science in the very first steps, making records of ideas, plans that didn’t work and protocols that failed available publicly. This allows others to avoid trailing the same dead end roads, saving time, money and human power. Some examples are listed below, but unfortunately there weren’t enough examples in the neuroscience field (until 22/01/14), so examples from all fields are listed:&lt;/p>
&lt;p>&lt;a href="http://carlboettiger.info/lab-notebook.html" target="_blank" rel="noopener">Carl Boettiger’s notebook&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://www.jeremiahfaith.com/open_notebook_science/" target="_blank" rel="noopener">Jeremiah Faith’s notebook&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://github.com/michaelbarton" target="_blank" rel="noopener">Michael Barton’s github repository&lt;/a>&lt;/p></description></item><item><title>Open PCR</title><link>https://open-neuroscience.com/post/open_pcr/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open_pcr/</guid><description>&lt;p>&lt;a href="https://openpcr.org/" target="_blank" rel="noopener">Open PCR&lt;/a> is an open source PCR machine with heated lid and space for 12 samples&lt;/p></description></item><item><title>Open science framework</title><link>https://open-neuroscience.com/post/open-science-framework/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open-science-framework/</guid><description>&lt;p>From the &lt;a href="http://openscienceframework.org/" target="_blank" rel="noopener">Open science framework&lt;/a> webpage:&lt;/p>
&lt;blockquote>
&lt;p>The Open Science Framework (OSF) is part network of research materials, part version control system, and part collaboration software. The purpose of the software is to support the scientist’s workflow and help increase the alignment between scientific values and scientific practices. &lt;/p>
&lt;/blockquote>
&lt;p>The project offers cloud space for uploading of project outline, materials, workflow, individual contributions and their extent to a specific project. All of that with the option of having all data publicly or privately available. The idea is to allow a more transparent and innovative system where people can see what is being done in “real-time”, contribute and even take up on ideas from other people so that the wheel doesn’t have to be reinvented, the system also allows for proper citation, so that the “wheel inventors” won’t go uncredited.&lt;/p>
&lt;p>As an example of what can be done through the Open Science Framework, one can cite the &lt;a href="http://openscienceframework.org/project/EZcUj/wiki/home" target="_blank" rel="noopener">Reproducibility Project&lt;/a> which is a project that aims at checking the reproducibility of ~150 sample studies from cognitive and psychological sciences. Contributors are welcomed!&lt;/p></description></item><item><title>OpenSpritzer</title><link>https://open-neuroscience.com/post/openspritzer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/openspritzer/</guid><description>&lt;p>A very neat picospritzer initially created by Joe (PI at &lt;a href="http://raimondolab.com/" target="_blank" rel="noopener">Raimondo Lab&lt;/a>) using basically a solenoid valve, microcontroller and a power source.&lt;/p>
&lt;p>Was later further developed by &lt;a href="https://chrisjforman.com/" target="_blank" rel="noopener">Chris&lt;/a> at the &lt;a href="badenlab.org">Baden Lab&lt;/a>, and collaboratively published as &lt;a href="https://www.nature.com/articles/s41598-017-02301-2" target="_blank" rel="noopener">a peer reviewed article&lt;/a>.&lt;/p>
&lt;p>Details on how to build it, can be found on the &lt;a href="https://github.com/BadenLab/Openspritzer/" target="_blank" rel="noopener">project&amp;rsquo;s Git repository&lt;/a>.&lt;/p></description></item><item><title>OpenStage</title><link>https://open-neuroscience.com/post/openstage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/openstage/</guid><description>&lt;p>&lt;a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0088977" target="_blank" rel="noopener">Open stage&lt;/a> is a low-cost motorised microscope stage capable of movement in the micrometer range.&lt;/p>
&lt;p>It features manual control via a control-pad, different movement velocities and pc communication through the serial port.&lt;/p>
&lt;p>The authors also state that due to its simplicity, the system could be used to drive micromanipulators and other devices&lt;/p></description></item><item><title>Parallela</title><link>https://open-neuroscience.com/post/parallela/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/parallela/</guid><description>&lt;p>&lt;a href="http://www.parallella.org/Introduction/" target="_blank" rel="noopener">Parallela&lt;/a>, an open source, open access card sized supercomputer, has the mission of bringing parallel computing to the masses by combining multiple RISC processors and very low power consumption. Produced by the &lt;a href="http://www.adapteva.com/" target="_blank" rel="noopener">Adapteva company&lt;/a>.&lt;/p></description></item><item><title>PySpace</title><link>https://open-neuroscience.com/post/pyspace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pyspace/</guid><description>&lt;p>&lt;a href="https://pyspace.github.io/pyspace/" target="_blank" rel="noopener">PySpace&lt;/a> is a signal processing and classificiation environment for Python.&lt;/p>
&lt;p>Modular software for processing of large data streams that has been specifically designed to enable distributed execution and empirical evaluation of signal processing chains. Various signal processing algorithms are available within the software, from finite impulse response filters over data-dependent spatial filters (e.g. CSP, xDAWN) to established classifiers (e.g. SVM, LDA). pySPACE incorporates the concept of node and node chains of the Modular Toolkit for Data Processing (MDP) framework.&lt;/p>
&lt;p>A paper about PySpace can be found &lt;a href="http://journal.frontiersin.org/article/10.3389/fninf.2013.00040/full" target="_blank" rel="noopener">here&lt;/a>&lt;/p></description></item><item><title>Python for Neurosciences (Frontiers collection)</title><link>https://open-neuroscience.com/post/python-for-neuroscience-frontiers-collection/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/python-for-neuroscience-frontiers-collection/</guid><description>&lt;p>Frontiers has created not one but two nice collections about open source software for neurosciences written in Python.&lt;/p>
&lt;p>&lt;a href="http://journal.frontiersin.org/researchtopic/8/python-in-neuroscience" target="_blank" rel="noopener">Here is collection 1&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://journal.frontiersin.org/researchtopic/1591/python-in-neuroscience-ii" target="_blank" rel="noopener">Here is collection 2&lt;/a>&lt;/p>
&lt;p>In these collections the readers will find a lot of nice resources, ranging from stimulus generation, to data formatting and analysis.&lt;/p></description></item><item><title>Signal Generators</title><link>https://open-neuroscience.com/post/signal-generators/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/signal-generators/</guid><description>&lt;p>Every lab needs a signal generator once in a while. They are useful to see if your acquisition program is working properly, to test why a certain piece of equipment is not working properly or to generate cues and targets at behavioural paradigms. Listed below are different generators, built using arduinos and other microcontrollers. They have different degrees of complexity and capabilities, so it would be wise to briefly look through them and see what fits you best!&lt;/p>
&lt;hr>
&lt;p>&lt;a href="http://www.instructables.com/id/Arduino-Waveform-Generator/" target="_blank" rel="noopener">The arduino waveform generator&lt;/a> is a pretty straight forward project that is able to generate four different waveforms from 1Hz to 50kHz. Gain, frequency, modulation and waveform type are controlled by nobs.&lt;/p>
&lt;iframe width="800" height="422" src="https://www.youtube.com/embed/gz_gVKWFN8E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe>
&lt;hr>
&lt;p>&lt;a href="http://www.instructables.com/id/Atmel-Xmega-USBSerial-Arbitrary-Waveform-Generato/?ALLSTEPS" target="_blank" rel="noopener">Atmel Xmega USB/Serial Arbitrary Waveform Generator&lt;/a> runs using a boston android XMEGA evaluation board and is able to deliver square, sine, triangular and arbitrary waveforms in between 5Hz and 20kHz. This one is not a stand alone system, which means that to set a new waveform type, one would have to have the board connect to a computer at all times.&lt;/p>
&lt;p>&lt;img src="https://i1.wp.com/www.instructables.com/files/deriv/FWF/PWX4/G79D44SM/FWFPWX4G79D44SM.LARGE.jpg?w=800" alt="arbitrary waveform generator" data-recalc-dims="1" />&lt;/p>
&lt;hr>
&lt;p>&lt;a href="http://arduino.cc/en/Tutorial/DueSimpleWaveformGenerator" target="_blank" rel="noopener">Simple waveform generator&lt;/a> seems to be the most straight forward of all projects, requiring only a potentiometer, a couple of resistors and push buttons. The trade off is that with the present sketch, waveforms of only up to 170Hz can be generated. It generates sawtooth, square, triangular and sine waveforms.&lt;/p>
&lt;p>&lt;img src="https://i0.wp.com/arduino.cc/en/uploads/Tutorial/DueSimpleWaveform_fritzing.png?w=800" alt="arduino due waveform generator" data-recalc-dims="1" />&lt;/p></description></item><item><title>Simulations</title><link>https://open-neuroscience.com/post/simulation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/simulation/</guid><description>&lt;div id="pl-1067" class="panel-layout" >
&lt;div id="pg-1067-0" class="panel-grid panel-no-style" >
&lt;div id="pgc-1067-0-0" class="panel-grid-cell" >
&lt;div id="panel-1067-0-0-0" class="so-panel widget widget_sow-editor panel-first-child panel-last-child" data-index="0" >
&lt;div class="so-widget-sow-editor so-widget-sow-editor-base">
&lt;div class="siteorigin-widget-tinymce textwidget">
&lt;p>
Ever thought about playing with a virtual worm? or interacting with a simulated bee brain? Sounds interesting no? These are just two projects that offer anyone the opportunity to play around with brain/neuronal simulations and models. Some of them are hardware based, and some completely software:
&lt;/p>
&lt;pre>&lt;code> &amp;lt;p&amp;gt;
&amp;lt;a href=&amp;quot;http://openeuroscience.com/open-source-simulations-and-models/open-worm/&amp;quot;&amp;gt;OpenWorm&amp;lt;/a&amp;gt;
&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;
&amp;lt;a href=&amp;quot;http://openeuroscience.com/open-source-simulations-and-models/green-brain/&amp;quot;&amp;gt;GreenBrain&amp;lt;/a&amp;gt;
&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;
&amp;lt;a href=&amp;quot;http://openeuroscience.com/open-source-simulations-and-models/neuronsneuronsneurons/&amp;quot;&amp;gt;Neurons,Neurons,Neurons&amp;lt;/a&amp;gt;
&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;
&amp;lt;a href=&amp;quot;http://openeuroscience.com/open-source-simulations-and-models/big-neuron/&amp;quot;&amp;gt;Big Neuron&amp;lt;/a&amp;gt;
&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;section class="blog">
&lt;div class="container">
&lt;div class="post-list" itemscope="" itemtype="http://schema.org/Blog">
{% for page in site.pages %}
{% for category in page.categories %}
{% if category == "Simulation" %}
{% include card_page.html %}
{% endif %}
{% endfor %}
{% endfor %}
&lt;pre>&lt;code>&amp;lt;/div&amp;gt;
&lt;/code>&lt;/pre>
&lt;/div>
&lt;/section></description></item><item><title>Skinner Box with RPi+Python</title><link>https://open-neuroscience.com/post/skinnerbox_rpi_python/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/skinnerbox_rpi_python/</guid><description>&lt;p>This project was developed by &lt;a href="http://www.kscottz.com/about/" target="_blank" rel="noopener">Katherine Scott&lt;/a> to be presented at the PyCon 2014. She developed a skinner box for her pet rats using a raspberry pi and some 3D printed parts. The setup contain a food dispenser, a buzzer, levers, a camera to observe the animals and it is hooked in a way that everything can be controlled over the internet!&lt;/p>
&lt;p>You can find the files for 3D parts &lt;a href="http://www.thingiverse.com/thing:296335" target="_blank" rel="noopener">here&lt;/a> and a better description of the project &lt;a href="http://www.kscottz.com/open-skinner-box-pycon-2014/" target="_blank" rel="noopener">here&lt;/a>&lt;/p>
&lt;p> &lt;/p>
&lt;iframe width="790" height="444" src="https://www.youtube.com/embed/grMfIoDgn9M" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe>
&lt;p> &lt;/p></description></item><item><title>Stereo microscope</title><link>https://open-neuroscience.com/post/stereo_microscope/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/stereo_microscope/</guid><description>&lt;p>Although stereo microscopes are an essential piece of hardware in biology labs, sometimes we wish they had more features, like the possibility to record the magnified images with a camera, or have a better lighting system to enhance contrast on those small samples.&lt;/p>
&lt;p>One person has taken those issues to heart and tackled them all in a very brilliant way. Below you&amp;rsquo;ll find links to &lt;a href="http://www.tangentaudio.com/about/" target="_blank" rel="noopener">Steve&amp;rsquo;s blog&lt;/a>, where he describes, in a very detailed way, three projects to enhance the all familiar stereo microscope:&lt;/p>
&lt;p>&lt;a href="http://www.tangentaudio.com/mechanical/microscope-camera-output/" target="_blank" rel="noopener">Camera eye piece adaptor.&lt;/a>&lt;/p>
&lt;p>&lt;img src="featured.jpg" alt="">&lt;/p>
&lt;p>&lt;a href="http://www.tangentaudio.com/2013/03/aziz-light/" target="_blank" rel="noopener">AZIZ a ring lighting system.&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://i1.wp.com/www.tangentaudio.com/wp-content/uploads/2013/03/DSC_6828-modified-1024x680.jpg?resize=800%2C531" data-recalc-dims="1" />&lt;/p>
&lt;p>&lt;a href="http://www.tangentaudio.com/2013/02/epic-builds-articulated-stereo-microscope-arm/" target="_blank" rel="noopener">Articulated stereo microscope mount.&lt;/a>&lt;/p>
&lt;p>&lt;span class="embed-youtube" style="text-align:center; display: block;">&lt;/span>&lt;/p>
&lt;p>Some of them are not that easy to reproduce, but maybe be a good starting point for other DIY versions.&lt;/p></description></item><item><title>Syringe Pump</title><link>https://open-neuroscience.com/post/syringe_pump/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/syringe_pump/</guid><description>&lt;p>From the &lt;a href="http://www.mse.mtu.edu/~pearce/Index.html" target="_blank" rel="noopener">Pearce lab&lt;/a>, this syringe pump was &lt;a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0107216" target="_blank" rel="noopener">published in Plos One&lt;/a> and is built using 3d printed parts, stepper motors and a raspberry pi, costing 5% or less than commercial available systems. Can be calibrated and customized for different applications.&lt;/p></description></item><item><title>Takktile</title><link>https://open-neuroscience.com/post/takktile/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/takktile/</guid><description>&lt;p>&lt;a href="http://www.takktile.com/" target="_blank" rel="noopener">Takktile&lt;/a>, is a tactile sensor to be used on robotic applications. The developers want to make it move away from the closed walls of research institutions by making it open source and cheap. It is built based on MEMs barometers and can sense 1 gram loads as well as coping with hammer blows (see video from their website below).&lt;/p>
&lt;p>&lt;span class="embed-youtube" style="text-align:center; display: block;">&lt;/span>&lt;/p></description></item><item><title>tDCS</title><link>https://open-neuroscience.com/post/tdcs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/tdcs/</guid><description>&lt;p>Although of simple complexity and using low currents, this tDCS machine is still to be considered a piece of equipment that could be dangerous both in the assembly and in the operation phases, so please inform yourself as best as you can before either of these steps! Also remember that the openeuroscience website cannot be held responsible for any injuries that might occur from improper use of this tool.&lt;/p>
&lt;p>&lt;a href="https://www.instructables.com/id/Build-a-Human-Enhancement-Device-Basic-tDCS-Suppl/" target="_blank" rel="noopener">DIY tDCS instructables&lt;/a>&lt;/p></description></item><item><title>Tensor Flow</title><link>https://open-neuroscience.com/post/tensor-flow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/tensor-flow/</guid><description>&lt;p>Google has packaged their deeplearning machine learning tools and made it open source. The project is called tensorflow, and is available &lt;a href="http://www.tensorflow.org" target="_blank" rel="noopener">here.&lt;/a> Some nice tutorials on the website, so that with a bit of patience, people can start to deep their toes into machine learning!&lt;/p>
&lt;p>Be sure to check the video below for more details!&lt;/p>
&lt;p>&lt;span class="embed-youtube" style="text-align:center; display: block;">&lt;/span>&lt;/p></description></item><item><title>The Visible Human project</title><link>https://open-neuroscience.com/post/the-visible-human-project/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/the-visible-human-project/</guid><description>&lt;p>&lt;a href="https://www.nlm.nih.gov/research/visible/visible_human.html" target="_blank" rel="noopener">The Visible Human Project&lt;/a> is a database of anatomical images (MR, CT and radiography) from male and female bodies. Information about the database is translated into a couple of different languages. Although an license needs to be signed and sent over to the NIH, the procedure seems to be straightforward and cost free.&lt;/p></description></item><item><title>The Yale open hand project</title><link>https://open-neuroscience.com/post/the_yale_open_hand_project/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/the_yale_open_hand_project/</guid><description>&lt;p>&lt;a href="http://www.eng.yale.edu/grablab/openhand/" target="_blank" rel="noopener">The Yale open hand project&lt;/a>, has a similar purpose of the open hand project, that is, to make prosthetic hands more widely available through the lowering of costs. They have a different design from the open hand project. Additionally the project wants to take advantage of the lowered costs to speed up the development cycle and provide, together with input from the user community, several different useful hand designs.&lt;/p></description></item><item><title>Vision Egg</title><link>https://open-neuroscience.com/post/vision-egg/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/vision-egg/</guid><description>&lt;p>&lt;a href="http://visionegg.org/" target="_blank" rel="noopener">Vision Egg&lt;/a> is a Python library for generating visual stimuli.&lt;/p>
&lt;p>In more detail, it is a high level interface in between Python and OpenGL, and can use inexpensive consumer grade graphics cards to generate precise visual stimuli. A paper with more details can be found here &lt;a href="http://journal.frontiersin.org/article/10.3389/neuro.11.004.2008/full">http://journal.frontiersin.org/article/10.3389/neuro.11.004.2008/full&lt;/a>&lt;/p></description></item><item><title>Web portals</title><link>https://open-neuroscience.com/post/webportals/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/webportals/</guid><description>&lt;p>Here are some open learning sources, they go from sites that interactively teach one how to code, to efforts in publishing free college textbooks.&lt;/p>
&lt;p>&lt;a href="https://www.khanacademy.org/" target="_blank" rel="noopener">Khan Academy&lt;/a>: Is composed of a series of lectures and exercises on a wide range of topics from basic multiplication to linear algebra and information theory. A great place to learn those things you missed in high school.&lt;/p>
&lt;p>&lt;a href="http://openstaxcollege.org/" target="_blank" rel="noopener">Openstax College:&lt;/a> is an initiative that is producing free, downloadable college level textbooks initially in sociology, physics, biology and anatomy and Physiology.&lt;/p>
&lt;p>&lt;a href="http://www.codecademy.com/#!/exercises/0" target="_blank" rel="noopener">CodeAcademy:&lt;/a> Teaches several programming languages, including python, javascript, etc. in an interactive manner. From the first lesson one is already writing meaningful code to solve exercises.&lt;/p>
&lt;p>&lt;a href="http://software-carpentry.org/" target="_blank" rel="noopener">Software Carpentry:&lt;/a> Is a project that helps scientists to write better code and increase their productivity by teaching them basic computing skills, such as version control, database systems, code with proper documentation and so on. They offer bootcamps, so check their website to see if there are any near you!&lt;/p>
&lt;p>&lt;a href="http://www.code.org/" target="_blank" rel="noopener">Code.org:&lt;/a> Aims to teach programming to everyone, by putting learning sources together, such as codeacademy, khanacademy and so on. Considering that mostly everything now days is run by a computer, this is a great idea.&lt;/p>
&lt;p>&lt;a href="https://www.edx.org/" target="_blank" rel="noopener">edx.org&lt;/a>: An initiative from Harvard and MIT to make their lectures (and from other universities) available free online, the nice thing is that they range from humanities to computer science, meaning that this is useful event if it is only for you to go a little bit depeer into that “old forgotten hobby/interest in something that is not neuroscience” you once had.&lt;/p>
&lt;p>&lt;a href="https://www.coursera.org/" target="_blank" rel="noopener">Coursera&lt;/a>: Aggregates online courses from several universities. It offers certificates for people who complete the courses.&lt;/p>
&lt;p>&lt;a href="https://www.sparkfun.com/static/about" target="_blank" rel="noopener">Sparkfun&lt;/a> is a retail store that sells eletronic components for hobbyists and DIY enthusiasts. But they also keep a very useful &lt;a href="https://learn.sparkfun.com/tutorials" target="_blank" rel="noopener">tutorial section&lt;/a> where one can find lessons on basic eletronics, installing arduino libraries, infrared communication, and designing PCB boards.&lt;/p>
&lt;p>&lt;a href="http://www.openoptogenetics.org/index.php?title=Main_Page" target="_blank" rel="noopener">OpenOptogenetics:&lt;/a> is a wiki page designed to promote knowledge and know-how exchange for optogenetic applications.&lt;/p>
&lt;p>&lt;a href="http://openwetware.org/wiki/Main_Page" target="_blank" rel="noopener">Open Wetware wiki&lt;/a> is a page dedicated to gathering information and know-how in biology and biological engineering. They provide a place to organize your own information, store labnotebooks and collaborate with other individuals. There is an article released in Nature (2008) about &lt;a href="http://www.nature.com/news/2008/080903/full/455022a.html" target="_blank" rel="noopener">this project.&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://arxiv.org/abs/1403.7439" target="_blank" rel="noopener">OpenPicoAmp&lt;/a>: is an open source planar lipid bilayer amplifier designed to teach undergrad students about electro-chemical properties of membranes. A paper describing the project, together with bill of materials and more can be found &lt;a href="http://arxiv.org/abs/1403.7439" target="_blank" rel="noopener">here&lt;/a> (for some reason this link did not open on Firefox. Try Chromium or Chrome instead). Also, a description on thingiverse can found &lt;a href="http://www.thingiverse.com/thing:292678" target="_blank" rel="noopener">here.&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://www.pyroelectro.com/" target="_blank" rel="noopener">PyroElectro&lt;/a>: a page for electronics and robotics enthusiasts, they have &lt;a href="http://www.pyroelectro.com/edu/" target="_blank" rel="noopener">tutorials on several topics&lt;/a>, such as modern electronics. microcontrollers and FPGA.&lt;/p>
&lt;p> &lt;/p></description></item><item><title>YouTube as a resource for Open Science Hardware</title><link>https://open-neuroscience.com/post/youtube_as_a_resource_for_open_science_hardware/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/youtube_as_a_resource_for_open_science_hardware/</guid><description>&lt;p>When working with Open Hardware, Google search will become your friend, whether you want it or not. Other search engines, such as DuckDuckGo won’t cut it (it is much harder to find what you are looking for, especially in cases where you don’t know all specific terms). Google is a fantastic tool to find answers. But if you are new in electronic development, and in programming, or even if you are going to work in a new subfield, you may not even know what you don’t know. What to search for if you don’t know what the problem or component is called? Here I’d like to suggest YouTube. You want to get involved with Arduinos and a interesting sensor. Go and search for ‘Arduino &lt;em>interesting sensor&lt;/em>’ on YouTube. There are many videos, long and short, explaining in more or less depth, more or less funny, what you can expect, and more importantly what the used components  are called. I normally start by watching a few short videos to make sure that system is what I’m looking for. Afterwards, I look into several longer, deeper videos. You will get a great overview of components and alternatives regarding code and hardware.&lt;/p>
&lt;p>Here I suggest some YouTube channels that constantly output good content, going across all kind of sensors, wireless communication as well as science and hacking in a more general sense. They can also be seen as evening entertainment, for me much better than dozing off to some soap opera.&lt;/p>
&lt;p> &lt;/p>
&lt;h2 id="electroboom">ElectroBOOM&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/channel/UCJ0-OtVpF0wOKEqT2Z1HEtA">https://www.youtube.com/channel/UCJ0-OtVpF0wOKEqT2Z1HEtA&lt;/a>&lt;/p>
&lt;p>Mehdi Sadaghdar, the protagonist of this channel explains in a very unique way how not to get electrocuted in any possible way. It trains the viewer what to keep in mind when working with electronics. There are many ways to fry your circuit, he will show them all in a entertaining way. It seems repetitive after a whole but that will just help your brain to keep those bullet points!&lt;/p>
&lt;p> &lt;/p>
&lt;h2 id="greatscott">GreatScott&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/channel/UC6mIxFTvXkWQVEHPsEdflzQ">https://www.youtube.com/channel/UC6mIxFTvXkWQVEHPsEdflzQ&lt;/a>&lt;/p>
&lt;p>Basic arduino and electronic projects, mosty day to day useful and very reproducible. His series  ‘Electronics Basics’ explains fantastically electronic components like transistors and diodes to name a few. Easy to digest and a lot can be learned.&lt;/p>
&lt;p> &lt;/p>
&lt;h2 id="codys-lab">Cody’s Lab&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/user/theCodyReeder/videos">https://www.youtube.com/user/theCodyReeder/videos&lt;/a>&lt;/p>
&lt;p>Cody is the Nr 1 in entertaining education on YouTube. He does all kind of experiments from geology, chemistry biology to gardening and everything in between. His passion is infectious and his style not teachy at all, you just tag along a fun quarter hour of science and ideas.&lt;/p>
&lt;p> &lt;/p>
&lt;h2 id="afrotechmods">Afrotechmods&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/user/Afrotechmods">https://www.youtube.com/user/Afrotechmods&lt;/a>&lt;/p>
&lt;p>A nice source for Arduino knowledge and inspiration! Great videos about basic electronics and some basic elements like power supplies, LEDs and transistors as well. There are well explained circuits about filters and op amps for example.&lt;/p>
&lt;p> &lt;/p>
&lt;h2 id="marco-reps">Marco Reps&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/user/reppesis/videos">https://www.youtube.com/user/reppesis/videos&lt;/a>&lt;/p>
&lt;p>A very diverse set of videos about electronics and hacking with great explanations. A good source for inspiration and entertainment, oscilloscopes, lasers and soldering are some of his main topics.&lt;/p>
&lt;p> &lt;/p>
&lt;h2 id="applied-science">Applied Science&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/user/bkraz333/videos">https://www.youtube.com/user/bkraz333/videos&lt;/a>&lt;/p>
&lt;p>IMHO the most advanced scientist on YouTube. X-ray, water cutter, electron microscopy and others, in depth explanation how he build or hacked several machines that seem way to complex / delicate to open up. His reverse engineering skills are helpful, it applies to how we could approach devices when we want to fix or understand them.&lt;/p>
&lt;p> &lt;/p>
&lt;h2 id="the-thought-emporium">The Thought Emporium&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/user/TheChemlife/videos">https://www.youtube.com/user/TheChemlife/videos&lt;/a>&lt;/p>
&lt;p>A group of scientists hacking advanced scientific machinery with household items, listening to satellites, dry freezing, fluorescence dyes and how to become a cyborg.&lt;/p>
&lt;p> &lt;/p>
&lt;h2 id="the-post-apocalyptic-inventor">The Post Apocalyptic Inventor&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/channel/UCDbWmfrwmzn1ZsGgrYRUxoA/videos">https://www.youtube.com/channel/UCDbWmfrwmzn1ZsGgrYRUxoA/videos&lt;/a>&lt;/p>
&lt;p>He hacks household items to harvest their motors and other hard to come by components. Lots of useful explanations on how to use metal working machines and how to build your own. A lot of entertainment around basic electronics.&lt;/p>
&lt;p> &lt;/p>
&lt;h2 id="andreas-spiess">Andreas Spiess&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/channel/UCu7_D0o48KbfhpEohoP7YSQ/videos">https://www.youtube.com/channel/UCu7_D0o48KbfhpEohoP7YSQ/videos&lt;/a>&lt;/p>
&lt;p>THE source of information if you want to learn wireless communication with Arduinos. Andreas gives very well structured and explained lectures about the ESP32, an Arduino with WiFi and other wireless capabilities. He enlightens every aspect about hardware and software on this topic. There are also many videos about all kind of interesting sensors.&lt;/p>
&lt;p> &lt;/p>
&lt;h2 id="eevblog">EEVblog&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/user/EEVblog/videos">https://www.youtube.com/user/EEVblog/videos&lt;/a>&lt;/p>
&lt;p>I will be honest, the videos are really long and very deep and technical. But if you need some information on a specific topic and his channel has a relevant video, that video will contain all the information you will need. If you are already trying to get into PCB design, watch his videos, all of them, over and over again, you will pick up all the rules and tricks to keep im mind when designing yourself.&lt;/p>
&lt;p> &lt;/p>
&lt;h2 id="bitluni8217s-lab">bitluni’s lab&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/user/bitlunislab/videos">https://www.youtube.com/user/bitlunislab/videos&lt;/a>&lt;/p>
&lt;p>Good videos about Arduino in the world of internet of things, making electronics in your house remote controlled and automatic will nicely translate into the lab as well.&lt;/p>
&lt;p> &lt;/p>
&lt;h2 id="ave">AvE&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/user/arduinoversusevil/videos">https://www.youtube.com/user/arduinoversusevil/videos&lt;/a>&lt;/p>
&lt;p>Entertainment in the first place, but be aware of his potty mouth, not everyone likes it. If you do, keep on watching. He mostly dissembles power tools and evaluates their build with focus on the machine engineering side – is the housing sufficiently stable, and electronically, are the components capable of what the label advertises? After a good amount of videos you get a good grasp on how engineering works in general, how they think. He helps to understand that you don’t need to be afraid to open up devices if they break or if you want to interfere with them. there are patterns and regularities that make all those machines somewhat self explanatory.&lt;/p>
&lt;p> &lt;/p>
&lt;h2 id="forcetronics">ForceTronics&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/channel/UCNd_fNspAczm8UoE2ay7K1Q/videos">https://www.youtube.com/channel/UCNd_fNspAczm8UoE2ay7K1Q/videos&lt;/a>&lt;/p>
&lt;p>From easy to advanced and mostly around Arduino and PCB design, also really good videos about soldering including helpful tips on SMD soldering. All ind of tutorials about sensors and times, protocols and wifi stuff.&lt;/p>
&lt;p> &lt;/p>
&lt;h2 id="tom-scott">Tom Scott&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/user/enyay/videos">https://www.youtube.com/user/enyay/videos&lt;/a>&lt;/p>
&lt;p>No Arduino, no electronics just science and curiosity. Short professionally produced videos about various topics space, computers, technology, science and more.&lt;/p>
&lt;p> &lt;/p>
&lt;h2 id="if-you-know-all-of-that-already-8230">If you know all of that already …&lt;/h2>
&lt;h3 id="kerrywong">KerryWong&lt;/h3>
&lt;p>&lt;a href="https://www.youtube.com/user/KerryWongBlog/videos">https://www.youtube.com/user/KerryWongBlog/videos&lt;/a>&lt;/p>
&lt;p>Lots of interesting and exotic laboratory electronics testing equipment and some more basic videos.&lt;/p>
&lt;p> &lt;/p>
&lt;h3 id="thesignalpath">TheSignalPath&lt;/h3>
&lt;p>&lt;a href="https://www.youtube.com/user/TheSignalPathBlog/videos">https://www.youtube.com/user/TheSignalPathBlog/videos&lt;/a>&lt;/p>
&lt;p>Really professional high frequency stuff, loang and in depth. if you are into that or you just want to see realy sexy PCBs give it a go!&lt;/p></description></item></channel></rss>
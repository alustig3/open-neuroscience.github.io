<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Hardware | Open Neuroscience</title><link>https://open-neuroscience.com/tag/hardware/</link><atom:link href="https://open-neuroscience.com/tag/hardware/index.xml" rel="self" type="application/rss+xml"/><description>Hardware</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>CC BY SA 4.0</copyright><lastBuildDate>Fri, 09 Jul 2021 00:00:00 +0000</lastBuildDate><image><url>https://open-neuroscience.com/media/openneuroscience_logo_dark.svg</url><title>Hardware</title><link>https://open-neuroscience.com/tag/hardware/</link></image><item><title>BioAmp EXG Pill</title><link>https://open-neuroscience.com/post/bioamp_exg_pill/</link><pubDate>Fri, 09 Jul 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/bioamp_exg_pill/</guid><description>&lt;p>BioAmp EXG Pill is a small, powerful Analog Front End (AFE) biopotential signal acquisition board that can be paired with any 5 V Micro Controller Unit (MCU) with an ADC. It is capable of recording publication-quality biopotential signals like ECG, EMG, EOG, and EEG, without the inclusion of any dedicated hardware or software filters. It’s small size allows easy integration into mobile and space-constrained projects, and it’s powerful noise rejection makes it usable even when the device is close to the AC mains supply. Any 1.5 mm diameter wire can be used as a strain-relieving electrode cable, making BioAmp EXG Pill very cost-effective in comparison to other options.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Deepak Khatri&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/upsidedownlabs/BioAmp-EXG-Pill">https://github.com/upsidedownlabs/BioAmp-EXG-Pill&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=-G3z9fvQnuw">https://www.youtube.com/watch?v=-G3z9fvQnuw&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Deepak Khatri&lt;/p>
&lt;hr></description></item><item><title>OpenSpritzer</title><link>https://open-neuroscience.com/post/openspritzer/</link><pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/openspritzer/</guid><description>&lt;p>Designed for ease of use, robustness and low-cost, the “Openspritzer” is an open hardware “Picospritzer” as routinely used in biological labs around the world. The performance of Openspritzer and commercial alternatives is effectively indistinguishable.&lt;/p>
&lt;p>The system is based on a solenoid valve connected to a pressure gauge. Control can be attained directly via an external TTL pulse or internally through an Arduino set by a rotary encoder. The basic setup can be put together for 3-400€, or substantially less if you are prepare to shop around.&lt;/p>
&lt;p>We anticipate that due to its high performance and low cost Openspritzer will be of interest to a broad range of researchers working in the life and physical sciences.&lt;/p>
&lt;p>Source: CJ. Forman et al, 2017 &amp;ndash; &lt;a href="https://www.nature.com/articles/s41598-017-02301-2">https://www.nature.com/articles/s41598-017-02301-2&lt;/a>&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>C.J. Forman, H. Tomes, B. Mbobo, R.J. Burman, M. Jacobs, T. Baden, J.V. Raimondo, M.J.Y. Zimmermann&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/BadenLab/Openspritzer">https://github.com/BadenLab/Openspritzer&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Maxime Zimmermann&lt;/p>
&lt;hr></description></item><item><title>Mobilefuge: Low-cost, Open-source 3D-printed centrifuge</title><link>https://open-neuroscience.com/post/mobilefuge_low_cost_open_source_3d_printed_centrifuge/</link><pubDate>Wed, 21 Apr 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/mobilefuge_low_cost_open_source_3d_printed_centrifuge/</guid><description>&lt;p>We made a low-cost centrifuge that can be useful for carrying out low-cost LAMP based detection of SARS-Cov2 virus in saliva. The 3D printed centrifuge (Mobilefuge) is portable, robust, stable, safe, easy to build and operate. The Mobilefuge doesn’t require soldering or programming skills and can be built without any specialised equipment, yet practical enough for high throughput use. More importantly, Mobilefuge can be powered from widely available USB ports, including mobile phones and associated power supplies. This allows the Mobilefuge to be used even in off-grid and resource limited settings.&lt;/p>
&lt;hr>
&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/svDpDSwnS3A" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
lJpWyzvg4Zk&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Chinna Devarapu&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.medrxiv.org/content/10.1101/2021.01.06.21249280v1">https://www.medrxiv.org/content/10.1101/2021.01.06.21249280v1&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/YwBiC7-dR-g" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;hr>
&lt;p>This post was automatically generated by
Chinna Devarapu&lt;/p>
&lt;hr></description></item><item><title>Building a Simple and Versatile Illumination System for Optogenetic Experiments</title><link>https://open-neuroscience.com/post/building_a_simple_and_versatile_illumination_system_for_optogenetic_experiments/</link><pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/building_a_simple_and_versatile_illumination_system_for_optogenetic_experiments/</guid><description>&lt;p>Controlling biological processes using light has increased the accuracy and speed with which researchers can manipulate many biological processes. Optical control allows for an unprecedented ability to dissect function and holds the potential for enabling novel genetic therapies. However, optogenetic experiments require adequate light sources with spatial, temporal, or intensity control, often a bottleneck for researchers. Here we detail how to build a low-cost and versatile LED illumination system that is easily customizable for different available optogenetic tools. This system is configurable for manual or computer control with adjustable LED intensity. We provide an illustrated step-by-step guide for building the circuit, making it computer-controlled, and constructing the LEDs. To facilitate the assembly of this device, we also discuss some basic soldering techniques and explain the circuitry used to control the LEDs. Using our open-source user interface, users can automate precise timing and pulsing of light on a personal computer (PC) or an inexpensive tablet. This automation makes the system useful for experiments that use LEDs to control genes, signaling pathways, and other cellular activities that span large time scales. For this protocol, no prior expertise in electronics is required to build all the parts needed or to use the illumination system to perform optogenetic experiments.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Phillip Kyriakakis1, Lourdes Fernandez de Cossio2, Patrick Wade Howard1, Sivleng Kouv1, Marianne Catanho1, Vincent J. Hu3, Robert Kyriakakis1, Molly E. Allen1, Yunhan Ma4, Marcelo Aguilar-Rivera1, Todd P. Coleman1&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/BreakLiquid/LED-Control-User-Interfaces">https://github.com/BreakLiquid/LED-Control-User-Interfaces&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Phillip Kyriakakis&lt;/p>
&lt;hr></description></item><item><title>Non-Telecentric 2P microscopy for 3D random access mesoscale imaging (nTCscope)</title><link>https://open-neuroscience.com/post/non_telecentric_2p_microscopy_for_3d_random_access_mesoscale_imaging_ntcscope_/</link><pubDate>Fri, 19 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/non_telecentric_2p_microscopy_for_3d_random_access_mesoscale_imaging_ntcscope_/</guid><description>&lt;p>Ultra-low-cost, easily implemented and flexible two-photon scanning microscopy modification offering a several-fold expanded three-dimensional field of view that also maintains single-cell resolution. Application of our system for imaging neuronal activity has been demonstrated on mice, zebrafish and fruit flies&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Filip Janiak; Philipp Bartel; Michael Bale; Takeshi Yoshimatsu; Emilia Komulainen; Mingyi Zhou; Kevin Staras; Lucia Prieto-Godino; Thomas Euler; Miguel Maravall; Tom Baden&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/BadenLab/nTCscope">https://github.com/BadenLab/nTCscope&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/3dXE6LjEcd8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;hr>
&lt;p>This post was automatically generated by
Filip Janiak&lt;/p>
&lt;hr></description></item><item><title>An Open-source Anthropomorphic Robot Hand System: HRI Hand</title><link>https://open-neuroscience.com/post/an_open_source_anthropomorphic_robot_hand_system_hri_hand/</link><pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/an_open_source_anthropomorphic_robot_hand_system_hri_hand/</guid><description>&lt;p>We present an open-source anthropomorphic robot hand system called HRI hand. Our robot hand system was developed with a focus on the end-effector role of the collaborative robot manipulator. HRI hand is a research platform that can be built at a lower price (approximately $500, using only 3D printing) than commercial end-effectors. Moreover, it was designed as a two four-bar linkage for the under-actuated mechanism and provides pre-shaping motion similar to the human hand prior to touching an object. A URDF, python node, and rviz package is also provided to support the Robot Operating System (ROS). All hardware CAD design files and software source codes have been released and can be easily assembled and modified. The system proposed in this paper is developed with a five-finger structure, but each finger is modularized, so it can be developed with end-effectors of various shapes depending on the shape of the palm.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Hyeonjun Park; Donghan Kim&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/MrLacuqer/HRI-hand-firmware.git">https://github.com/MrLacuqer/HRI-hand-firmware.git&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://youtu.be/c5Ry3tl9FVw">https://youtu.be/c5Ry3tl9FVw&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Andre M Chagas&lt;/p>
&lt;hr></description></item><item><title>Feeding Experimentation Device ver3 (FED3)</title><link>https://open-neuroscience.com/post/feeding_experimentation_device_ver3_fed3_/</link><pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/feeding_experimentation_device_ver3_fed3_/</guid><description>&lt;p>FED3 is an open-source battery-powered device for home-cage training of mice in operant tasks. FED3 can be 3D printed and the control code is open-source and can be modified. The code is written in the Arduino language and is run on an Adafruit Feather M0 Adalogger microcontroller inside of FED3. Mice interact with FED3 through two nose-pokes and FED3 responds with visual stimuli, auditory stimuli, and by dispensing pellets. FED3 also has an analog output that allows it to synchronize with and control external equipment such as lasers or brain recording systems. A screen provides feedback to the user, and all behavioral events are logged to an on-board microSD card. The default code includes multiple built-in programs but FED3 is open-source and hackable, and can be easily modified to perform other tasks.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Lex Kravitz&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/KravitzLabDevices/FED3">https://github.com/KravitzLabDevices/FED3&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=3kBaFsYON4U">https://www.youtube.com/watch?v=3kBaFsYON4U&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Lex Kravitz&lt;/p>
&lt;hr></description></item><item><title>Head-Mounted Mesoscope</title><link>https://open-neuroscience.com/post/head_mounted_mesoscope/</link><pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/head_mounted_mesoscope/</guid><description>&lt;p>The advent of genetically encoded calcium indicators, along with surgical preparations such as thinned skulls or refractive index matched skulls, have enabled mesoscale cortical activity imaging in head-fixed mice. Such imaging studies have revealed complex patterns of coordinated activity across the cortex during spontaneous behaviors, goal-directed behavior, locomotion, motor learning,and perceptual decision making. However, neural activity during unrestrained behavior significantly differs from neural activity in head-fixed animals. Whole-cortex imaging in freely behaving mice will enable the study of neural activity in a larger, more complex repertoire of behaviors not possible in head-fixed animals. Here we present the “Mesoscope,” a wide-field miniaturized, head-mounted fluorescence microscope compatible with transparent polymer skulls recently developed by our group. With afield of view of 8 mm x 10 mm and weighing less than 4 g, the Mesoscope can image most of the mouse dorsal cortex with resolution ranging from 39 to 56μm. Stroboscopic illumination with blue and green LEDs allows fort he measurement of both fluorescence changes due to calcium activity and reflectance signals to capture hemodynamic changes. We have used the Mesoscope to successfully record mesoscale calcium activity across the dorsal cortex during sensory-evoked stimuli, open field behaviors, and social interactions.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Biosensing and Biorobotics Lab&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.biorxiv.org/content/10.1101/2020.05.25.114892v1.full.pdf">https://www.biorxiv.org/content/10.1101/2020.05.25.114892v1.full.pdf&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>LED Zappelin'</title><link>https://open-neuroscience.com/post/led_zappelin_/</link><pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/led_zappelin_/</guid><description>&lt;p>Two-photon (2P) microscopy is a cornerstone technique in neuroscience research. However, combining 2P imaging with spectrally arbitrary light stimulation can be challenging due to crosstalk between stimulation light and fluorescence detection. To overcome this limitation, we present a simple and low-cost electronic solution based on an ESP32 microcontroller and a TLC5947 LED driver to rapidly time-interleave stimulation and detection epochs during scans. Implemented for less than $100, our design can independently drive up to 24 arbitrary spectrum LEDs to meet user requirements. We demonstrate the utility of our stimulator for colour vision experiments on the in vivo tetrachromatic zebrafish retina and for optogenetic circuit mapping in Drosophila.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Maxime Zimmermann; Andre Maia Chagas; Philipp Bartel; Sinzi Pop, Lucia Pierto Godino; Tom Baden&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/BadenLab/LED-Zappelin">https://github.com/BadenLab/LED-Zappelin&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Maxime Zimmermann&lt;/p>
&lt;hr></description></item><item><title>An open-source experimental framework for automation of high-throughput cell biology experiments</title><link>https://open-neuroscience.com/post/an_open_source_experimental_framework_for_automation_of_high_throughput_cell_biology_experiments/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/an_open_source_experimental_framework_for_automation_of_high_throughput_cell_biology_experiments/</guid><description>&lt;p>Modern Biology methods require a large number of high quality experiments to be conducted, which requires a high degree of automation. Our solution is an open-source hardware that allows for automatic high-throughput generation of large amounts of cell biology data. The hardware consists of an automatic XY-stage for moving a multiwell plate containing growing cells; a perfusion manifold allowing application of up to 8 different solutions; and a small epifluorescent microscope. It is extremely cheap (approximately £400 without and £2500 with a fluorescent microscope) and is easily customizable for individual experimental needs. We demonstrate the usability of this platform with high-throughput Ca2+ imaging and large-scale labelling experiments. All building instructions, software and PCB Gerber file are provided.&lt;/p>
&lt;hr>
&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/lJpWyzvg4Zk" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Pavel Katunin; Anton Nikolaev&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/frescolabs/FrescoM">https://github.com/frescolabs/FrescoM&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/RerEV6oOm4s" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;hr>
&lt;p>This post was automatically generated by
Anton Nikolaev&lt;/p>
&lt;hr></description></item><item><title>Computer-controlled dog treat dispenser</title><link>https://open-neuroscience.com/post/computer_controlled_dog_treat_dispenser/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/computer_controlled_dog_treat_dispenser/</guid><description>&lt;p>When performing canine operant conditioning studies, the delivery of the reward can be a limiting factor of the study. While there are a few commercially available options for automatically delivering rewards, they generally require manual input, such as using a remote control, in accordance with the experiment script. This means that human reaction times and transmission distances can cause interruptions to the flow of the experiment. The potential for development of non-supervised conditioning studies is limited by this same factor. To remedy this, we retrofitted an off-the-shelf treat dispenser with new electronics that allow it to be remotely controllable as well as act as an experiment computation, data storage, and networking center. We present a fully integrated dispenser driver board with a complementary Raspberry Pi. With rather simple modifications, the commercial treat dispenser can be modified into a computer-controlled dispenser for canine cognition experiments or for other forms of canine training or games.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Jeffrey R. Stevens; Walker Arce&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/unl-cchil/canine_treat_dispenser">https://github.com/unl-cchil/canine_treat_dispenser&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=veKvqE5ipu4">https://www.youtube.com/watch?v=veKvqE5ipu4&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Jeffrey R. Stevens&lt;/p>
&lt;hr></description></item><item><title>openEyeTrack - An open source high-speed eyetracker</title><link>https://open-neuroscience.com/post/openeyetrack_an_open_source_high_speed_eyetracker/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/openeyetrack_an_open_source_high_speed_eyetracker/</guid><description>&lt;p>Vision is one of the primary senses, and tracking eye gaze can offer insight into the cues that affect decision-making behavior. Thus, to study decision-making and other cognitive processes, it is fundamentally necessary to track eye position accurately. However, commercial eye trackers are 1) often very expensive, and 2) incorporate proprietary software to detect the movement of the eye. Closed source solutions limit the researcher’s ability to be fully informed regarding the algorithms used to track the eye and to incorporate modifications tailored to their needs. Here, we present our software solution, openEyeTrack, a low-cost, high-speed, low-latency, open-source video-based eye tracker. Video-based eye trackers can perform nearly as well as classical scleral search coil methods and are suitable for most applications.&lt;/p>
&lt;p>openEyeTrack is a video-based eye-tracker that takes advantage of OpenCV, a low-cost, high-speed infrared camera and GigE-V APIs for Linux provided by Teledyne DALSA, the graphical user interface toolkit QT5 and cvui, the OpenCV based GUI. All of the software components are freely available. The only costs are from the hardware components such as the camera (Genie Nano M640 NIR, Teledyne DALSA, ~$450, ~730 frames per second) and infrared light source, an articulated arm to position the camera (Manfrotto: $130), a computer with one or more gigabit network interface cards, and a power over ethernet switch to power and receive data from the camera.&lt;/p>
&lt;p>By using the GigE-V Framework to capture the frames from the DALSA camera and the OpenCV simple blob detector, openEyeTrack can accurately estimate the position and area of the pupil. We include pupil size calculations because of its putative link to arousal levels and emotions of the subject.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Chandramouli Chandrasekaran; Jorge Paolo Casas&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/chand-lab/openEyeTrack">https://github.com/chand-lab/openEyeTrack&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Chandramouli Chandrasekaran&lt;/p>
&lt;hr></description></item><item><title>PocketPCR-Pocket size USB powered PCR Thermo Cycler</title><link>https://open-neuroscience.com/post/pocketpcr_pocket_size_usb_powered_pcr_thermo_cycler/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pocketpcr_pocket_size_usb_powered_pcr_thermo_cycler/</guid><description>&lt;p>The PocketPCR is a so called thermocycler used to activate biological reactions. To do so the device raises and lowers the temperature of the liquid in the small tubes. The polymerase chain reaction (PCR) is a method widely used in molecular biology to make copies of a specific DNA segment. Applications of the technique include DNA cloning for sequencing, analysis of genetic fingerprints, amplification of ancient DNA and gene cloning.&lt;/p>
&lt;p>Simpler, smaller and more affordable. This ultra portable and compact thermocycler was designed with the goal to bring the cost down to affordable price for anyone who wants to do some Do-It-Your-Self biology with a DNA starter kit in his kitchen. The PocketPCR can be run from a simple USB power adapter. The device can be operated stand alone and all parameters can be set without the need of a computer or smartphone.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Urs Gaudenz; Yanwu Guo&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://gaudi.ch/PocketPCR/">http://gaudi.ch/PocketPCR/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://youtu.be/0tXwAAMCetI">https://youtu.be/0tXwAAMCetI&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Urs Gaudenz&lt;/p>
&lt;hr></description></item><item><title>KineMouse Wheel</title><link>https://open-neuroscience.com/post/kinemouse_wheel/</link><pubDate>Tue, 23 Feb 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/kinemouse_wheel/</guid><description>&lt;p>Who says you can&amp;rsquo;t reinvent the wheel?! This running wheel for head-fixed mice allows 3D reconstruction of body kinematics using a single camera and DeepLabCut (or similar) software. A lightweight, transparent polycarbonate floor and a mirror mounted on the inside allow two views to be captured simultaneously. All parts are commercially available or laser cut. See example usage here: &lt;a href="https://elifesciences.org/articles/63596">https://elifesciences.org/articles/63596&lt;/a>&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Richard Warren&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://hackaday.io/project/160744-kinemouse-wheel">https://hackaday.io/project/160744-kinemouse-wheel&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://elifesciences.org/articles/63596">https://elifesciences.org/articles/63596&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Richard Warren&lt;/p>
&lt;hr></description></item><item><title>SignalBuddy</title><link>https://open-neuroscience.com/post/signalbuddy/</link><pubDate>Tue, 23 Feb 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/signalbuddy/</guid><description>&lt;p>SignalBuddy is an easy-to-make, easy-to-use signal generator for scientific applications. Making friends is hard, but making SignalBuddy is easy. All you need is an Arduino Uno! SignalBuddy replaces more complicated and (much) more expensive signal generators in laboratory settings where one millisecond resolution is sufficient.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Richard Warren&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://hackaday.io/project/167649-signalbuddy">https://hackaday.io/project/167649-signalbuddy&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://github.com/richard-warren/SignalBuddy/raw/master/images/SignalBuddy3D.gif">https://github.com/richard-warren/SignalBuddy/raw/master/images/SignalBuddy3D.gif&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Richard Warren&lt;/p>
&lt;hr></description></item><item><title>Open Source Tools for Temporally Controlled Rodent Behavior Suitable for Electrophysiology and Optogenetic Manipulations</title><link>https://open-neuroscience.com/post/open_source_tools_for_temporally_controlled_rodent_behavior_suitable_for_electrophysiology_and_optogenetic_manipulations/</link><pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open_source_tools_for_temporally_controlled_rodent_behavior_suitable_for_electrophysiology_and_optogenetic_manipulations/</guid><description>&lt;p>Understanding how the brain controls behavior requires observing and manipulating neural activity in awake behaving animals. Neuronal firing is timed at millisecond precision. Therefore, to decipher temporal coding, it is necessary to monitor and control animal behavior at the same level of temporal accuracy. However, it is technically challenging to deliver sensory stimuli and reinforcers as well as to read the behavioral responses they elicit with millisecond precision. Presently available commercial systems often excel in specific aspects of behavior control, but they do not provide a customizable environment allowing flexible experimental design while maintaining high standards for temporal control necessary for interpreting neuronal activity. Moreover, delay measurements of stimulus and reinforcement delivery are largely unavailable. We combined microcontroller-based behavior control with a sound delivery system for playing complex acoustic stimuli, fast solenoid valves for precisely timed reinforcement delivery and a custom-built sound attenuated chamber using high-end industrial insulation materials. Together this setup provides a physical environment to train head-fixed animals, enables calibrated sound stimuli and precisely timed fluid and air puff presentation as reinforcers. We provide latency measurements for stimulus and reinforcement delivery and an algorithm to perform such measurements on other behavior control systems. Combined with electrophysiology and optogenetic manipulations, the millisecond timing accuracy will help interpret temporally precise neural signals and behavioral changes. Additionally, since software and hardware provided here can be readily customized to achieve a large variety of paradigms, these solutions enable an unusually flexible design of rodent behavioral experiments.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Nicola Solari; Katalin Sviatkó; Tamás Laszlovszky; Panna Hegedüs; Balázs Hangya&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/hangyabalazs/Rodent_behavior_setup">https://github.com/hangyabalazs/Rodent_behavior_setup&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Balazs Hangya&lt;/p>
&lt;hr></description></item><item><title>EmotiBit</title><link>https://open-neuroscience.com/post/emotibit/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/emotibit/</guid><description>&lt;p>EmotiBit is a wearable sensor to capture high-quality emotional, physiological, and movement data from just about anywhere on the body.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Sean Montgomery&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.emotibit.com/">https://www.emotibit.com/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=jbcL2jzyWj4&amp;amp;ab_channel=EmotiBit">https://www.youtube.com/watch?v=jbcL2jzyWj4&amp;amp;ab_channel=EmotiBit&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Sean Montgomery&lt;/p>
&lt;hr></description></item><item><title>Mouse VR</title><link>https://open-neuroscience.com/post/mouse_vr/</link><pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/mouse_vr/</guid><description>&lt;p>Harvey Lab miniaturized mouse VR rig for head-fixed virtual navigation and decision-making tasks.&lt;/p>
&lt;p>The VR setup is comprised of several independent assemblies:&lt;/p>
&lt;p>The screen assembly: a laser projector projects onto a parabolic screen surrounding the mouse. This is the basis for the visual virtual reality.&lt;/p>
&lt;p>Ball cup assembly: an air-supported 8&amp;quot; styrofoam ball that the mouse can run on, with associated ball cup, sensors, and electronics&lt;/p>
&lt;p>Reward delivery system and lick sensor: lick spout, liquid reward reservoir, solenoid, and associated electronics&lt;/p>
&lt;p>Enclosure: A box surrounding the behavioral setup.&lt;/p>
&lt;p>Each of these components is independent of the others: i.e. just the screen could be used in combination with a different treadmill and reward delivery system. The electronics for the ball sensors, reward delivery, and lick detection are all mounted on the same PCB. If only one or two of these functions are needed, you do not need to populate the entire PCB.&lt;/p>
&lt;p>The screen assembly is designed to be small enough to be mounted within a standard 19&amp;quot; server rack, which could easily fit 3 rigs stacked vertically (or two + monitor and keyboard station).&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Noah Pettit; Matthias Minderer; Selmaan Chettih; Charlotte Arlt; Jim Bohnslav; Pavel Gorelick; Ofer Mazor; Christopher Harvey&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/HarveyLab/mouseVR">https://github.com/HarveyLab/mouseVR&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Noah Pettit&lt;/p>
&lt;hr></description></item><item><title>culture_shock</title><link>https://open-neuroscience.com/post/culture_shock/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/culture_shock/</guid><description>&lt;p>Culture Shock is an open-source electroporator that was developed
through internet based collaboration, starting on the DIYbio Google
Group. It is an evolution on the traditional capacitive discharge
circuit topology, instead using pulsed induction to enable a
programmable waveform as well as reduce the size, weight, and cost of
the equipment. With all these benefits, we hope to reduce the burden
of laboratory consumables for DNA transformation and electrofusion
procedures, where chemical supplies are currently relied on. The added
benefit of programmability allows many cell types to be manipulated by
altering the voltage level, or even giving the voltage profile a
particular shape.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>John Griessen; Nathan McCorkle; Bryan Bishop&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/kanzure/culture_shock">https://github.com/kanzure/culture_shock&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Nathan McCorkle&lt;/p>
&lt;hr></description></item><item><title>PiDose</title><link>https://open-neuroscience.com/post/pidose/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pidose/</guid><description>&lt;p>PiDose is an open-source tool for scientists performing drug administration experiments with mice. It allows for automated daily oral dosing of mice over long time periods (weeks to months) without the need for experimenter interaction and handling. To accomplish this, a small 3D-printed chamber is mounted adjacent to a regular mouse home-cage, with an opening in the cage to allow animals to freely access the chamber.&lt;/p>
&lt;p>The chamber is supported by a load cell, and does not contact the cage but sits directly next to the entrance opening. Prior to treatment, mice have a small RFID capsule implanted subcutaneously, and when they enter the chamber they are detected by an RFID reader. While the mouse is in the chamber, readings are taken from the load cell in order to determine the mouse&amp;rsquo;s bodyweight. At the opposite end of the chamber from the entrance, a nose-poke port accesses a spout which dispenses drops from two separate liquid reservoirs. This spout is wired to a capacitive touch sensor controller in order to detect licks, and delivers liquid drops in response to licking.&lt;/p>
&lt;p>Each day, an average weight is calculated for each mouse and a drug dosage is determined based on this. When a mouse licks at the spout it dispenses either regular drinking water or a drop of drug solution depending on if they have received their daily dosage or not. All components are controlled by a Python script running on a Raspberry Pi.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Cameron Woodard; Wissam Nasrallah; Bahram Samiei; Tim Murphy; Lynn Raymond&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://osf.io/rpyfm/">https://osf.io/rpyfm/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Cameron Woodard&lt;/p>
&lt;hr></description></item><item><title>pyControl</title><link>https://open-neuroscience.com/post/pycontrol/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pycontrol/</guid><description>&lt;p>pyControl is a system of open source hardware and software for controlling behavioural experiments, built around the Micropython microcontroller.&lt;/p>
&lt;p>pyControl makes it easy to program complex behavioural tasks using a clean, intuitive, and flexible syntax for specifying tasks as state machines. User created task definition files, written in Python, run directly on the microcontroller, supported by pyControl framework code. This gives users the power and simplicity of Python for specifying task behaviour, while allowing advanced users low-level access to the microcontroller hardware.&lt;/p>
&lt;p>pyControl hardware consists of a breakout board and a set of devices such as nose-pokes, audio boards, LED drivers, rotary encoders and stepper motor controllers that are connected to the breakout board to create behavioural setups.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Thomas Akam&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://pycontrol.readthedocs.io">https://pycontrol.readthedocs.io&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Thomas Akam&lt;/p>
&lt;hr></description></item><item><title>pyPhotometry</title><link>https://open-neuroscience.com/post/pyphotometry/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pyphotometry/</guid><description>&lt;p>pyPhotometry is system of open source, Python based, hardware and software for neuroscience fiber photometry data acquisition, consisting of an acquisition board and graphical user interface.&lt;/p>
&lt;p>pyPhotometry supports data aquisition from two analog and two digital inputs, and control of two LEDs via built in LED drivers with an adjustable 0-100mA output. The system supports time-division multiplexed illumination which allows fluoresence evoked by different excitation wavelengths to be independenly readout from a single photoreciever signal.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Thomas Akam&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://pyphotometry.readthedocs.io">https://pyphotometry.readthedocs.io&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Thomas Akam&lt;/p>
&lt;hr></description></item><item><title>PiVR</title><link>https://open-neuroscience.com/post/pivr/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pivr/</guid><description>&lt;p>PiVR is a system that allows experimenters to immerse small animals into virtual realities. The system tracks the position of the animal and presents light stimulation according to predefined rules, thus creating a virtual landscape in which the animal can behave. By using optogenetics, we have used PiVR to present fruit fly larvae with virtual olfactory realities, adult fruit flies with a virtual gustatory reality and zebrafish larvae with a virtual light gradient.&lt;/p>
&lt;p>PiVR operates at high temporal resolution (70Hz) with low latencies (&amp;lt;30 milliseconds) while being affordable (&amp;lt;US$500) and easy to build (&amp;lt;6 hours). Through extensive documentation (&lt;a href="http://www.PiVR.org">www.PiVR.org&lt;/a>), this tool was designed to be accessible to a wide public, from high school students to professional researchers studying systems neuroscience in academia.&lt;/p>
&lt;p>The project is open source (BSD-3) and the documented code written in the freely available programming language Python. We hope that PiVR will be adapted by advanced users for their particular needs, for example to create closed-loop experiments involving other sensory modalities (e.g., sound/vibration) through the use of PWM controllable devices. We envision PiVR to be used as the central module when creating virtual realities for a variety of sensory modalities. This ‘PiVR module’ takes care of detecting the animal and presenting the appropriate PWM signal that is then picked up by the PWM controllable device installed by the user, for example to produce a sound whenever an animal enters a pre-defined region.&lt;/p>
&lt;p>In short, PiVR is a powerful and affordable experimental platform allowing experimenters to create a wide array of virtual reality experiments. Our hope is that PiVR will be adapted by several labs to democratize closed-loop experiments and, by standardizing image quality and the animal detection algorithm, increase reproducibility.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>David Tadres; Matthieu Louis&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://www.PiVR.org">http://www.PiVR.org&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=w5tIG6B6FWo">https://www.youtube.com/watch?v=w5tIG6B6FWo&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
David Tadres&lt;/p>
&lt;hr></description></item><item><title>OpenDrop Digital Microfluidics Platform</title><link>https://open-neuroscience.com/post/opendrop_digital_microfluidics_platform/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/opendrop_digital_microfluidics_platform/</guid><description>&lt;p>OpenDrop a modular, open source digital microfludics platform for research purposes. The device uses recent electro-wetting technology to control small droplets of liquids. Potential applications are lab on a chip devices for automating processes of digital biology.&lt;/p>
&lt;p>The OpenDrop V4 is modular electrowetting controller. The driver board is equipped with a connector that can host a circuit board cartridge with a 14×8 electrode array and 4 reservoirs. The liquids stay on a thin, hydrophobic foil laminated to the circuit board . The device is powered from USB though an included USB-C cable. All the voltage level are generated on the device and can be set with the built in soft menu from 150-300 Volts, DC or AC.&lt;/p>
&lt;p>OpenDrop Cartridges
The modular concepts of the OpenDrop V4 allows different configurations of cartridges: A gold coated electrode array board that can be coated with any dielectric layer and hydrophobic coating to make cartridges for topless digital microfluidic applications using readily available materials.The OpenDrop V4 Cartridge is a close-cell cartridge capable of “move, mix, split and reservoir dispensing”. The 4×8 electrode array and 4 reservoirs are laminated with a 15um ETFE foil, hydrophobic coating and ITO top cover.&lt;/p>
&lt;p>Programming
The OpenDrop V4 can be operated standalone and droplets can be moved through the built in joystick. A control software to program sequences of patterns from a computer is available as a free download. The board is also compatible with Adafruit Feather M0 controller boards and can be reprogrammed through the free Arduino IDE for custom specific applications. A sample code with the instruction to activate electrodes can be found on the OpenDrop GitHub.&lt;/p>
&lt;p>Features:&lt;/p>
&lt;ul>
&lt;li>Modular Cartridge System
- Connector to connect electrode board with up to 128 channels
- Gold coated 14×8 electrodes array, 2.75 mm x 2.75 mm in size, 4mil gaps&lt;/li>
&lt;li>Reservoirs – the new electrode array features 4 CT-type reservoirs&lt;/li>
&lt;li>AC and DC voltage generated on the device form USB power. True AC voltage driving capability (up to 300VAC).&lt;/li>
&lt;li>32bit AVR SAMD21G18 microprocessor with plenty of memory and power
- Electronic settings for voltage level, frequency and AC/DC selection
- Electronic reading of actual voltage level
- One connector for communication and powering (USB-C)
- Optical isolation of the high-voltage electronics trough opto-couplers and PhotoMOS
- New polyphonic audio amplifier and speaker (it’s a synth!)
- Cartridge presence detection
- Feedback amplifier
- Super flat OLED Display
- Nice joystick and 2 buttons, 3 LEDs
- Reset button
- All files open source, designed on KiCAD&lt;/li>
&lt;/ul>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>MSc Urs Gaudenz&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://www.gaudi.ch/OpenDrop/">http://www.gaudi.ch/OpenDrop/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=TY97QfWY6J4">https://www.youtube.com/watch?v=TY97QfWY6J4&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Anonymous&lt;/p>
&lt;hr></description></item><item><title>Stytra</title><link>https://open-neuroscience.com/post/stytra/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/stytra/</guid><description>&lt;p>Stytra, a flexible, open-source software package, written in Python and designed to cover all the general requirements involved in larval zebrafish behavioral experiments.&lt;/p>
&lt;p>It provides timed stimulus presentation, interfacing with external devices and simultaneous real-time tracking of behavioral parameters such as position, orientation, tail and eye motion in both freely-swimming and head-restrained preparations.&lt;/p>
&lt;p>Stytra logs all recorded quantities, metadata, and code version in standardized formats to allow full provenance tracking, from data acquisition through analysis to publication.&lt;/p>
&lt;p>The package is modular and expandable for different experimental protocols and setups. We also provide complete documentation with examples for extending the package to new stimuli and hardware, as well as a schema and parts list for behavioural setups.&lt;/p>
&lt;p>The software can be used in the context of calcium imaging experiments by interfacing with other acquisition devices.&lt;/p>
&lt;p>Our aims are to enable more laboratories to easily implement behavioral experiments, as well as to provide a platform for sharing stimulus protocols that permits easy reproduction of experiments and straightforward validation.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Vilim Stih; Luigi Petrucco&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/portugueslab/stytra">https://github.com/portugueslab/stytra&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Anonymous&lt;/p>
&lt;hr></description></item><item><title>Open Source Automated Western Blot Processor</title><link>https://open-neuroscience.com/post/open_source_automated_western_blot_processor/</link><pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open_source_automated_western_blot_processor/</guid><description>&lt;p>Researchers in the biomedical area are always involved in methodologies comprising several processes that are repetitive and time-consuming; these researchers can take advantage of this time for other more important things.&lt;/p>
&lt;p>For many years, the trend for this type of problem has been automation. One of the routine methodologies used by researchers in broad areas of basic investigation is the Western blot technique.&lt;/p>
&lt;p>This method allows the detection through specific antibodies and the eventual quantification of the protein of interest in different biological lysates transferred onto a suitable membrane. This methodology involves several repetitive processes; one of them is the washing of blots after incubations with primary and secondary antibodies.&lt;/p>
&lt;p>The present device has been designed to automate this process at a low cost. Researchers must use several tools to carry out the same task at a much higher price, and more importantly, in a time-consuming process. Although it is designed for the Western blot, it can be optimized for other cyclic tasks.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Jorge Bravo-Martinez&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://dx.doi.org/10.17632/xcvckyc9mh.1">http://dx.doi.org/10.17632/xcvckyc9mh.1&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Jorge Bravo-Martinez&lt;/p>
&lt;hr></description></item><item><title>Small cost efficient 3D printed peristaltic pumps</title><link>https://open-neuroscience.com/post/small_cost_efficient_3d_printed_peristaltic_pumps/</link><pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/small_cost_efficient_3d_printed_peristaltic_pumps/</guid><description>&lt;p>The project overall aim is to provide cost efficient solution to drive microfluidics systems for e.g. cell culture and organ on a chip applications. Pumps, valves and other accessories are ofter expensive to buy or very expensive to custom made. The 8-channel FAST pump is a 3D printed pump that uses some off the shelf parts (steel pins and ball bearings) and is easily fabricated and assembled. A step by step protocol is published (&lt;a href="https://www.sciencedirect.com/science/article/pii/S2468067220300249)">https://www.sciencedirect.com/science/article/pii/S2468067220300249)&lt;/a>. The link to the picture is from this publication. The pump is far cheaper and smaller than commercial pumps and still has at least as good as or better pump performance. The 8-channel pump is excellent for use in parallel cell culture applications.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Alexander Jönsson; Arianna Toppi; Martin Dufva&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.sciencedirect.com/science/article/pii/S2468067220300249">https://www.sciencedirect.com/science/article/pii/S2468067220300249&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Martin Dufva&lt;/p>
&lt;hr></description></item><item><title>A Cartesian Coordinate Robot for Dispensing Fruit Fly Food</title><link>https://open-neuroscience.com/post/a_cartesian_coordinate_robot_for_dispensing_fruit_fly_food/</link><pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/a_cartesian_coordinate_robot_for_dispensing_fruit_fly_food/</guid><description>&lt;p>The fruit fly, Drosophila melanogaster, continues to be one of the most widely used model organisms in biomedical research.&lt;/p>
&lt;p>Though chosen for its ease of husbandry, maintaining large numbers of stocks of fruit flies, as done by many laboratories, is labour-intensive.&lt;/p>
&lt;p>One task which lends itself to automation is the production of the vials of food in which the flies are reared. Fly facilities typically have to generate several thousand vials of fly food each week to sustain their fly stocks.&lt;/p>
&lt;p>The system presented here combines a cartesian coordinate robot with a peristaltic pump. The design of the robot is based on an open hardware CNC (computer numerical control) machine, and uses belt and pulley actuators for the X and Y axes, and a leadscrew actuator for the Z axis.&lt;/p>
&lt;p>CNC motion and operation of the peristaltic pump are controlled by grbl (&lt;a href="https://github.com/gnea/grbl),">https://github.com/gnea/grbl),&lt;/a> an open source, embedded, G-code parser. Grbl is written in optimized C and runs directly on an Arduino. A Raspberry Pi is used to generate and stream G-code instructions to Grbl.&lt;/p>
&lt;p>A touch screen on the Raspberry Pi provides a graphical user interface to the system. Whilst the robot was built for the express purpose of filling vials of fly food, it could potentially be used for other liquid handling tasks in the laboratory.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Matt Wayland; Matthias Landgraf&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/WaylandM/fly-food-robot">https://github.com/WaylandM/fly-food-robot&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://doi.org/10.6084/m9.figshare.5175223.v1">https://doi.org/10.6084/m9.figshare.5175223.v1&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matt Wayland&lt;/p>
&lt;hr></description></item><item><title>Ethoscopes</title><link>https://open-neuroscience.com/post/ethoscopes/</link><pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/ethoscopes/</guid><description>&lt;p>Ethoscopes are machines for high-throughput analysis of behavior in Drosophila and other animals.&lt;/p>
&lt;p>Ethoscopes provide a software and hardware solution that is reproducible and easily scalable.&lt;/p>
&lt;p>They perform, in real-time, tracking and profiling of behavior by using a supervised machine learning algorithm, are able to deliver behaviorally triggered stimuli to flies in a feedback-loop mode, and are highly customizable and open source.&lt;/p>
&lt;p>Ethoscopes can be built easily by using 3D printing technology and rely on Raspberry Pi microcomputers and Arduino boards to provide affordable and flexible hardware.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Quentin Geissmann; Luis Garcia; Giorgio Gilestro&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://lab.gilest.ro/ethoscope">http://lab.gilest.ro/ethoscope&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=5oWGBUMJON8&amp;amp;feature=emb_title">https://www.youtube.com/watch?v=5oWGBUMJON8&amp;amp;feature=emb_title&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Giorgio Gilestro&lt;/p>
&lt;hr></description></item><item><title>Poseidon</title><link>https://open-neuroscience.com/post/poseidon/</link><pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/poseidon/</guid><description>&lt;p>The Poseidon is an open-source syringe pump and microscope system. It uses 3D printed parts and common components that can be easily purchased. It can be used in microfluidics experiments or other applications. You can assemble it in a short-time for under $400. The system is modular and highly customizable. Examples of applications are: control the chemical environment of a bioreactor, purify proteins and precisely add reagents to chemical reactions over time.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Pachter Lab&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://pachterlab.github.io/poseidon">https://pachterlab.github.io/poseidon&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>FishCam</title><link>https://open-neuroscience.com/post/fishcam/</link><pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/fishcam/</guid><description>&lt;p>We describe the “FishCam”, a low-cost (500 USD) autonomous camera package to record videos and images underwater. The system is composed of easily accessible components and can be programmed to turn ON and OFF on customizable schedules. Its 8-megapixel camera module is capable of taking 3280 × 2464-pixel images and videos. An optional buzzer circuit inside the pressure housing allows synchronization of the video data from the FishCam with passive acoustic recorders. Ten FishCam deployments were performed along the east coast of Vancouver Island, British Columbia, Canada, from January to December 2019. Field tests demonstrate that the proposed system can record up to 212 h of video data over a period of at least 14 days. The FishCam data collected allowed us to identify fish species and observe species interactions and behaviors. The FishCam is an operational, easily-reproduced and inexpensive camera system that can help expand both the temporal and spatial coverage of underwater observations in ecological research. With its low cost and simple design, it has the potential to be integrated into educational and citizen science projects, and to facilitate learning the basics of electronics and programming.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Xavier Mouy&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.sciencedirect.com/science/article/pii/S2468067220300195">https://www.sciencedirect.com/science/article/pii/S2468067220300195&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>OpenTrons</title><link>https://open-neuroscience.com/post/opentrons/</link><pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/opentrons/</guid><description>&lt;p>Today, biologists spend too much time pipetting by hand. We think biologists should have robots to do pipetting for them. People doing science should be free of tedious benchwork and repetitive stress injuries. They should be able to spend their time designing experiments and analyzing data.&lt;/p>
&lt;p>That&amp;rsquo;s why we started Opentrons.&lt;/p>
&lt;p>We make robots for biologists. Our mission is to provide the scientific community with a common platform to easily share protocols and reproduce each other&amp;rsquo;s results. Our robots automate experiments that would otherwise be done by hand, allowing our community to spend more time pursuing answers to some of the 21st century’s most important questions&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Opentrons&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://opentrons.com/about">https://opentrons.com/about&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/channel/UCvMRmXIxnHs3AutkVhuqaQg">https://www.youtube.com/channel/UCvMRmXIxnHs3AutkVhuqaQg&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Fingertip laser sensor</title><link>https://open-neuroscience.com/post/fingertip_laser_sensor/</link><pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/fingertip_laser_sensor/</guid><description>&lt;p>&lt;a href="http://toychest.ai.uni-bremen.de/wiki/projects:fingertip#fingertip_laser_sensor" target="_blank" rel="noopener">The fingertip laser project&lt;/a> makes use of the sensor used in an Avago ADNS-9500 laser mouse, to improve the capabilities of robotic hands, giving them the capability to detect distance, surface type and slippage of grasped objects. Very elegant hack of a mouse sensor!&lt;/p></description></item><item><title>UC2</title><link>https://open-neuroscience.com/post/uc2/</link><pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/uc2/</guid><description>&lt;p>The open-source optical toolbox UC2 [YouSeeToo] simplifies the process of building optical setups, by combining 3D-printed cubes, each holding a specific component (e.g. lens, mirror) on a magnetic square-grid baseplate. The use of widely available consumables and 3D printing, together with documentation and software, offers an extremely low-cost and accessible alternative for both education and research areas. In order to reduce the entry barrier, we provide a fully comprehensive toolbox called TheBOX. A paper describing the scientific application in detail can be found &lt;a href="https://www.biorxiv.org/content/10.1101/2020.03.02.973073v1" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Benedict Diederich; René Lachmann; Barbora Marsikova&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://useetoo.org">https://useetoo.org&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=ey4uEFEG6MY">https://www.youtube.com/watch?v=ey4uEFEG6MY&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Barbora Marsikova&lt;/p>
&lt;hr></description></item><item><title>Open Source Eye Tracking</title><link>https://open-neuroscience.com/post/open_source_eye_tracking/</link><pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open_source_eye_tracking/</guid><description>&lt;p>The purpose of this project is to convey a location in 3 dimensional space to a machine, hands free and in real time.&lt;/p>
&lt;p>Currently it is very difficult to control machines without making the user provide input with their hands. Additionally it can be very difficult to specify a location in space without a complex input device. This system provides a novel solution to this problem by allowing the user to specify a location simply by looking at it.&lt;/p>
&lt;p>Normally eyetracking solutions are prohibitively expensive and not open source, limiting their use for creators to integrate them into new projects. This solution is fully open source, easy to build and will provide a huge variety of options for makers interested in using this fascinating and powerful technology.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>John Evans&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://hackaday.io/project/153293-low-cost-open-source-eye-tracking">https://hackaday.io/project/153293-low-cost-open-source-eye-tracking&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Open Source Syringe Pump Controller</title><link>https://open-neuroscience.com/post/open_source_syringe_pump_controller/</link><pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open_source_syringe_pump_controller/</guid><description>&lt;p>Syringe pumps are a necessary piece of laboratory equipment that are used for fluid delivery in behavioral neuroscience laboratories. Many experiments provide rodents and primates with fluid rewards such as juice, water, or liquid sucrose. Current commercialized syringe pumps are not customizable and do not have the ability to deliver multiple volumes of fluid based on different inputs to the pump. Additionally, many syringe pumps are expensive and cannot be used in experiments with paired neurophysiological recordings due to electrical noise. We developed an open source syringe pump controller using commonly available parts. The controller adjusts the acceleration and speed of the motor to deliver three different volumes of fluid reward within one common time epoch. This syringe pump controller is cost effective and has been successfully implemented in rodent behavioral experiments with paired neurophysiological recordings in the rat frontal cortex while rats lick for different volumes of liquid sucrose rewards. Our syringe pump controller will enable new experiments to address the potential confound of temporal information in studies of reward signaling by fluid magnitude.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Laubach Lab&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/LaubachLab/OpenSourceSyringePump">https://github.com/LaubachLab/OpenSourceSyringePump&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Sample Rotator Mixer and Shaker</title><link>https://open-neuroscience.com/post/sample_rotator_mixer_and_shaker/</link><pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/sample_rotator_mixer_and_shaker/</guid><description>&lt;p>An open-source 3-D printable laboratory sample rotator mixer is developed here in two variants that allow users to opt for the level of functionality, cost saving and associated complexity needed in their laboratories. First, a laboratory sample rotator is designed and demonstrated that can be used for tumbling as well as gentle mixing of samples in a variety of tube sizes by mixing them horizontally, vertically, or any position in between. Changing the mixing angle is fast and convenient and requires no tools. This device is battery powered and can be easily transported to operate in various locations in a lab including desktops, benches, clean hoods, chemical hoods, cold rooms, glove boxes, incubators or biological hoods. Second, an on-board Arduino-based microcontroller is incorporated that adds the functionality of a laboratory sample shaker. These devices can be customized both mechanically and functionally as the user can simply select the operation mode on the switch or alter the code to perform custom experiments. The open source laboratory sample rotator mixer can be built by non-specialists for under US$30 and adding shaking functionality can be done for under $20 more. Thus, these open source devices are technically superior to the proprietary commercial equipment available on the market while saving over 90% of the costs.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>MOST&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.appropedia.org/Open_Source_Laboratory_Sample_Rotator_Mixer_and_Shaker">https://www.appropedia.org/Open_Source_Laboratory_Sample_Rotator_Mixer_and_Shaker&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=Ta2ACV1oIjI&amp;amp;feature=emb_logo">https://www.youtube.com/watch?v=Ta2ACV1oIjI&amp;amp;feature=emb_logo&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Automated Operant Conditioning</title><link>https://open-neuroscience.com/post/automated_operant_conditioning/</link><pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/automated_operant_conditioning/</guid><description>&lt;p>Operant conditioning (OC) is a classical paradigm and a standard technique used in experimental psychology in which animals learn to perform an action to achieve a reward. By using this paradigm, it is possible to extract learning curves and measure accurately reaction times (RTs). Both these measurements are proxy of cognitive capabilities and can be used to evaluate the effectiveness of therapeutic interventions in mouse models of disease. Here, we describe a fully 3D printable device that is able to perform OC on freely moving mice, while performing real-time tracking of the animal position. We successfully trained six mice, showing stereotyped learning curves that are highly reproducible across mice and reaching &amp;gt;70% of accuracy after 2 d of conditioning. Different products for OC are commercially available, though most of them do not provide customizable features and are relatively expensive. This data demonstrate that this system is a valuable alternative to available state-of-the-art commercial devices, representing a good balance between performance, cost, and versatility in its use.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Raffaele Mazziotti, Giulia Sagona, Leonardo Lupori, Virginia Martini and Tommaso Pizzorusso&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/raffaelemazziotti/oc_chamber">https://github.com/raffaelemazziotti/oc_chamber&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Craniobot</title><link>https://open-neuroscience.com/post/craniobot/</link><pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/craniobot/</guid><description>&lt;p>The Craniobot is a cranial microsurgery platform that combines automated skull surface profiling with a computer numerical controlled (CNC) milling machine to perform a variety of cranial microsurgical procedures in mice. The Craniobot utilizes a low force contact sensor to profile the skull surface and uses this information to perform micrometer-scale precise milling operations within minutes. The procedure of removing the sub-millimeter thick mouse skull precisely without damaging the underlying brain can be technically challenging and often takes significant skill and practice. This can now be overcome using the Craniobot.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Mathew Rynes, Leila Ghanbari, Micheal Laroque, Greg Johnson, Daniel Sousa Schulman, Suhasa Kodandaramaiah&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.labmaker.org/products/craniobot">https://www.labmaker.org/products/craniobot&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Labmaker</title><link>https://open-neuroscience.com/post/labmaker/</link><pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/labmaker/</guid><description>&lt;p>LabMaker is a maker and assembly service for OPEN SCIENCE instruments. OPEN SCIENCE initiatives provide part lists or &amp;ldquo;Bill Of Materials&amp;rdquo; (BOM) for openly available scientific instruments. LabMaker bridges the gap between the BOM and the ready-to-use instrument for those not wanting to build by themselves. LabMaker is based in Berlin, Germany and ships worldwide. Berlin, as a city, is not only amongst the frontrunners for the title &amp;ldquo;start-up capital of Europe&amp;rdquo;, but also home to a large diversity of companies rooted in traditional precision manufacturing.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Labmaker&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.labmaker.org/">https://www.labmaker.org/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>MicroscoPy</title><link>https://open-neuroscience.com/post/microscopy/</link><pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/microscopy/</guid><description>&lt;p>An open-source, motorized, and modular microscope built using LEGO bricks, Arduino, Raspberry Pi and 3D printing. The microscope uses a Raspberry Pi mini-computer with an 8MP camera to capture images and videos. Stepper motors and the illumination are controlled using a circuit board comprising an Arduino microcontroller, six stepper motor drivers and a high-power LED driver. All functions can be controlled from a keyboard connected to the Raspberry Pi or a separate custom-built Arduino joystick connected to the mainboard. LEGO bricks are used to construct the main body of the microscope to achieve a modular and easy-to-assemble design concept.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Yuksel Temiz and IBM&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/IBM/MicroscoPy">https://github.com/IBM/MicroscoPy&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=PBSYnk9T4o4&amp;amp;feature=youtu.be">https://www.youtube.com/watch?v=PBSYnk9T4o4&amp;amp;feature=youtu.be&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Autoreward 2</title><link>https://open-neuroscience.com/post/autoreward2/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/autoreward2/</guid><description>&lt;p>The &lt;strong>motivation&lt;/strong> to start this project arises when we started to include a new behavioral paradigm in the lab, an alternation T-mace with return arms (like the one in Wood e_t al._ 2000). We wanted a clean performance, as well as a clean video record, so we consider necessary to interfere neither with the animal attention (mice, how they are!) nor the camera’s field of view. I decided then to give a try to the new hobby I was getting into, “Do-It-Yourself” (DIY) stuff.&lt;/p>
&lt;p>In my head, it was pictured very simple. At the end of the day, I just needed a) something to detect the animal passing by, b) something to deliver a drop of water and c) something to make it happen in a coordinated way. And that’s what Autoreward2 is, no more, no less.&lt;/p>
&lt;p>Well perhaps it is a bit more. &lt;strong>So far, the project can&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Detect&lt;/strong> when the animal reaches the end of any of the two arms.&lt;/li>
&lt;li>&lt;strong>Deliver&lt;/strong> a small drop of fluid through the corresponding licking port (easy to make it happen in the opposite, if wanted).&lt;/li>
&lt;li>Give visual cues to the experimenter, indicating which arm has been reached.&lt;/li>
&lt;li>Allow to &lt;strong>select&lt;/strong> different modes of working for different working protocols: ‘Waiting for selection’, ‘Habituation’, ‘Training’, ‘Experimental’ and “Filling and cleaning” modes (and is ready to include more!).&lt;/li>
&lt;/ul>
&lt;p>To achieve it, I decided for very &lt;strong>simple approach&lt;/strong>. A couple of cheap infrared emitters are continuously read by an UNO R3 board. Breaking any of the beams triggers the signal to open the corresponding solenoid valve, connected to the fluid tank. That lets the liquid flow by gravity for around 75 milliseconds, resulting in a single drop at the tip of the licking port.&lt;/p>
&lt;div align="center">
&lt;p>&lt;img src="./featured2.jpg" alt="">&lt;/p>
&lt;/div>
&lt;p>There is a delay after each detection, to avoid repetitive delivery if animals don’t leave the area. A couple LEDs mounted in the bare-board (out of animal sight) light up when the process is triggered, one for each side. They also work as indicators for the ‘Waiting for selection’ mode, when they are continuously on, meanwhile no option is choose or the ‘return to waiting mode action’ is pressed.&lt;/p>
&lt;p>&lt;strong>The selection&lt;/strong> is made through a 4×4 membrane keypad. Right now, only options 1 to 4 are programmed, making up to 12 more programs available! When any section is made, the in-built LED blinks the corresponding times and the system is ready to work. At any moment, pressing any key makes the system reset to the waiting mode. As easy as that.&lt;/p>
&lt;p>Everything is &lt;strong>powered&lt;/strong> by a regular 9V wall adapter, giving 3.3V to the LEDs and Infrared detectors, and 9V to the solenoids. Of course, it is possible to use a 9V batterie to power it. To avoid damage coming from the solenoid discharges, the circuit is protected by a couple of diodes at this level.&lt;/p>
&lt;p>And that’s all, &lt;strong>it’s simple&lt;/strong>. The most important thing: it &lt;strong>works&lt;/strong>. The other most important thing: it costs around &lt;strong>80€&lt;/strong>. Here is the to-buy list (or equivalent):&lt;/p>
&lt;ul>
&lt;li>Elegoo UNO R3 (I found them for &lt;strong>10€&lt;/strong>, with USB cable)&lt;/li>
&lt;li>BreadBoard + Acrylic base (&lt;strong>7€&lt;/strong>)&lt;/li>
&lt;li>9V 1A Wall power supply (&lt;strong>9€&lt;/strong>)&lt;/li>
&lt;li>2x InfraRed beams, 5mm (&lt;strong>15€&lt;/strong> both, the 3mm ones are even cheaper)&lt;/li>
&lt;li>2x Mini-Solenoid valves (&lt;strong>10€&lt;/strong> both)&lt;/li>
&lt;li>2x red LEDs&lt;/li>
&lt;li>4x 1 KΩ resistors&lt;/li>
&lt;li>2x TIP120 Transistors&lt;/li>
&lt;li>2x 1N4001 diodes&lt;/li>
&lt;li>Wiring (set of jumpers for less than &lt;strong>10€&lt;/strong>)&lt;/li>
&lt;li>‘Velcro’ to attach the acrylic base where the boards are mounted.&lt;/li>
&lt;li>Plastic tubing and laboratory sample tubes, modified with turning siringe tips to attach/deattach the tubing easily.&lt;/li>
&lt;li>2x or 4x weak magnets to fix the tubes to the walls.&lt;/li>
&lt;/ul>
&lt;p> &lt;/p>
&lt;p>Feel free to access the &lt;a href="https://github.com/jjballesteros/Arduino-AutoReward" target="_blank" rel="noopener">Github&lt;/a> page or the &lt;a href="http://forum.arduino.cc/index.php?topic=476643.0" target="_blank" rel="noopener">Arduino forum post&lt;/a> to obtain the &lt;strong>code&lt;/strong>, check for the circuit &lt;strong>sketch&lt;/strong>, and see some &lt;strong>pictures&lt;/strong>.&lt;/p>
&lt;p>PD: If someone is scandalized by the code, I am getting better on it, it is not my main strength. Please, improve it! Of course, I have in mind many possible upgrades such as a screen, a SD card port, to change the Keypad for a wireless interface (tactile?) … Did someone say smartphone plus Bluetooth? Going fancy, a barcode reader to easily introduce subjects’ data… And here is where I relay in the open-access idea, I offer it and hopefully someone implement any of the ideas. If so, remember to share!&lt;/p>
&lt;p>Jesús J. Ballesteros&lt;/p>
&lt;p>Contact me:&lt;/p>
&lt;p>&lt;a href="https://twitter.com/jjballesterosc" target="_blank" rel="noopener">Twitter&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.researchgate.net/profile/J_J_Ballesteros" target="_blank" rel="noopener">ResearchGate&lt;/a>&lt;/p></description></item><item><title>OpenFlexure</title><link>https://open-neuroscience.com/post/openflexure/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/openflexure/</guid><description>&lt;p>OpenFlexure is a 3D printed flexure translation stage, developed by a group at the Bath University. The stage is capable of sub-micron-scale motion, with very small drift over time. Which makes it quite good, among other things, for time-lapse protocols that need to be done over days/weeks time, and under space restricted areas, such as fume hoods. A paper describing it in detail can be found &lt;a href="http://arxiv.org/abs/1509.05394" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>Adding a camera and servo motors, turns the stage into an automated microscope. More details about the project can be found &lt;a href="https://openflexure.org/" target="_blank" rel="noopener">here&lt;/a>.&lt;/p></description></item><item><title>OpenFuge</title><link>https://open-neuroscience.com/post/openfuge/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/openfuge/</guid><description>&lt;p>&lt;a href="https://www.thingiverse.com/thing:151406" target="_blank" rel="noopener">OpenFuge&lt;/a> describes all the materials and gives step by step instructions to the assembly of a centrifuge that is able to deliver 6000 G’s of force and to rotate at 9000 RPM, while being able to hold 4 eppendorf tubes. Developed by &lt;a href="https://www.thingiverse.com/CopabX/about" target="_blank" rel="noopener">CopabX&lt;/a>&lt;/p></description></item><item><title>Pulse Pal</title><link>https://open-neuroscience.com/post/pulse-pal/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pulse-pal/</guid><description>&lt;blockquote>
&lt;p>Pulse Pal is an open and inexpensive (~$210) alternative to pulse generators used in neurophysiology research, and is most often used to create precisely timed light trains in optogenetics assays. Pulse Pal generates four channels of configurable square pulse trains ranging in voltage from +10 to -10V using a bipolar DAC. Two digital trigger channels can be used to start and stop playback. APIs are available in C++, Python and MATLAB, and the hardware designs and firmware are fully open source.&lt;/p>
&lt;/blockquote>
&lt;p>Be sure to check the &lt;a href="http://journal.frontiersin.org/article/10.3389/fneng.2014.00043/abstract" target="_blank" rel="noopener">paper&lt;/a> about it and their &lt;a href="https://sites.google.com/site/pulsepalwiki/home" target="_blank" rel="noopener">wiki page.&lt;/a>&lt;/p></description></item><item><title>Red Pitaya</title><link>https://open-neuroscience.com/post/red-pitaya/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/red-pitaya/</guid><description>&lt;p>&lt;a href="https://www.redpitaya.com/?skip_intro=yes" target="_blank" rel="noopener">Red Pitaya&lt;/a> is an computer+FPGA that has digital input and outputs and really fast analog inputs and outputs. It allows connection over ethernet and programming of custom routines. The system is powerful enough to have application in mostly all branches of neuroscience labs: oscilloscopes, signal generators and even a candidate for recording systems.&lt;/p></description></item><item><title>GogoFuge</title><link>https://open-neuroscience.com/post/gogofuge/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/gogofuge/</guid><description>&lt;p>&lt;a href="https://diybio.org/2012/06/12/gogofuge/" target="_blank" rel="noopener">GogoFuge&lt;/a> is a good example of the power of opensource designs. IT was based on the idea of the DremelFuge and altered to be a tabletop centrifuge with vortex capability. It was created by &lt;a href="fablabatschool.org/profile/KeeganCooke">Keegan Cooke&lt;/a>&lt;/p>
&lt;iframe width="474" height="360" src="https://www.youtube.com/embed/Qcl04sqXqY4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe></description></item><item><title>Intelligent hearing aid</title><link>https://open-neuroscience.com/post/intelligent-hearing-aid/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/intelligent-hearing-aid/</guid><description>&lt;p>Ojoshi at instructables.com has posted a manual on how to build this arduino based &lt;a href="http://www.instructables.com/id/Intelligent-Hearing-Aid/?ALLSTEPS" target="_blank" rel="noopener">hearing aid system&lt;/a>.&lt;/p>
&lt;p>From his &lt;a href="http://www.instructables.com/id/Intelligent-Hearing-Aid/?ALLSTEPS" target="_blank" rel="noopener">instructables page&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>it has tuning functionality that allows the wearer to tune the amplification to his or her needs. It has a conversational mode which recognizes voice input and amplifies it while reducing background noise. It saves all data to memory so that the device can be quickly powered up and ready to use. This device also has a very easy user interface to keep operation quick and simple.&lt;/p>
&lt;/blockquote>
&lt;p> &lt;/p>
&lt;p>If you are going to try and build this, take maximum care and do it at your own risk!&lt;/p>
&lt;p> &lt;/p>
&lt;p> &lt;figure style="width: 703px" class="wp-caption aligncenter">&lt;/p>
&lt;p>&lt;img src="https://i1.wp.com/cdn.instructables.com/F5D/JQVI/HOUFWUWY/F5DJQVIHOUFWUWY.LARGE.jpg?resize=703%2C937" alt="" width="703" height="937" data-recalc-dims="1" />&lt;figcaption class="wp-caption-text">from: &lt;a href="http://cdn.instructables.com/F5D/JQVI/HOUFWUWY/F5DJQVIHOUFWUWY.LARGE.jpg">http://cdn.instructables.com/F5D/JQVI/HOUFWUWY/F5DJQVIHOUFWUWY.LARGE.jpg&lt;/a>&lt;/figcaption>&lt;/figure>&lt;/p></description></item><item><title>NeuroTinker</title><link>https://open-neuroscience.com/post/neurotinker/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/neurotinker/</guid><description>&lt;p>&lt;a href="https://hackaday.io/project/3339-neurons-neurons-neurons" target="_blank" rel="noopener">NeuroTinker project&lt;/a> is all about hardware emulated neurons. The creators made them in a way that each hardware neuron has excitatory and inhibitory inputs and one output that can be split up to affect dowsntream neurons. They are also cheap enough so that one can build several of them and wire them together to see which properties will emerge in the system. Design files are available on the &lt;a href="https://github.com/neurotinker" target="_blank" rel="noopener">project&amp;rsquo;s GitHub organization&lt;/a>&lt;/p></description></item><item><title>Super-Releaser</title><link>https://open-neuroscience.com/post/super_releaser/</link><pubDate>Mon, 21 Mar 2016 10:00:12 +0000</pubDate><guid>https://open-neuroscience.com/post/super_releaser/</guid><description>&lt;p>Ever thought about making soft robots? The folks at &lt;a href="http://superreleaser.com/" target="_blank" rel="noopener">Super-Releaser&lt;/a> have, and they are doing very cool projects! Some for &lt;a href="http://superreleaser.com/project-profiles/" target="_blank" rel="noopener">medical applications and some for research&lt;/a> purposes. Check one of their cool robots below:&lt;/p>
&lt;div class="ytp-html5-clipboard">
&lt;span class="embed-youtube" style="text-align:center; display: block;">&lt;/span>
&lt;/div></description></item><item><title>5 Dollar PCR machine</title><link>https://open-neuroscience.com/post/5_dollar_pcr/</link><pubDate>Tue, 09 Jun 2015 09:53:14 +0000</pubDate><guid>https://open-neuroscience.com/post/5_dollar_pcr/</guid><description>&lt;p>The 5 dollar PCR machine is a project from &lt;a href="https://hackaday.io/dnhkng" target="_blank" rel="noopener">David Ng&lt;/a>.&lt;/p>
&lt;p>he created a very interesting design for the PCR machine. Instead of using eppendorfs, he is using teflon tubes and three different heating elements, which allows for cheaper (he has a working PCR machine for 5 dollars!) and faster DNA amplifications.&lt;/p>
&lt;p>&lt;a href="https://hackaday.io/project/1864-5-dna-replicator" target="_blank" rel="noopener">Here you can find the project page, with nice description and instructions&lt;/a>&lt;/p>
&lt;p>Below is a video from David explaining the project:&lt;/p>
&lt;iframe width="790" height="481" src="https://www.youtube.com/embed/S9Fq5CGj9Kg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe></description></item><item><title>Open bionics</title><link>https://open-neuroscience.com/post/open-bionics/</link><pubDate>Sat, 31 Jan 2015 22:56:41 +0000</pubDate><guid>https://open-neuroscience.com/post/open-bionics/</guid><description>&lt;p>&lt;a href="http://www.openbionics.org/" target="_blank" rel="noopener">The Open bionics project&lt;/a> was inspired by the Yale open hand project, aiming to develop light, affordable, and modular robot hands and myoelectric prosthesis. Also they want to make them easy to replicate using off the shelf materials. On the video below taken from their website you can see the hands in action, either as a prosthesis, or attached to a small drone being operated remotely.&lt;/p></description></item><item><title>Open prosthetics and robotics</title><link>https://open-neuroscience.com/post/prosthetics-and-robotics/</link><pubDate>Sat, 31 Jan 2015 22:08:16 +0000</pubDate><guid>https://open-neuroscience.com/post/prosthetics-and-robotics/</guid><description>&lt;p>With the rise of low cost 3D printers, and other cheap manufacturing tools, the field of robotics and prosthetics has been gaining quite a few open source projects. Two very nice compilations can be found at &lt;a href="//openrobothardware.org/">openrobot hardware&lt;/a> and at &lt;a href="http://softroboticstoolkit.com/" target="_blank" rel="noopener">Soft robotics toolkit&lt;/a>. Below are some related to neuroscience:&lt;/p>
&lt;p>&lt;a href="http://openeuroscience.com/hardware-projects/open-prosthetics-and-robotics/open-hand-project/" title="Open Hand Project" target="_blank" rel="noopener">The open hand project&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://openeuroscience.com/hardware-projects/open-prosthetics-and-robotics/the-yale-open-hand-project/" title="The Yale open hand project" target="_blank" rel="noopener">The Yale open hand project&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://openeuroscience.com/hardware-projects/open-prosthetics-and-robotics/open-bionics/" title="Open bionics" target="_blank" rel="noopener">Openbionics&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://openeuroscience.com/hardware-projects/open-prosthetics-and-robotics/fingertip-laser-sensor/" title="Fingertip laser sensor" target="_blank" rel="noopener">Fingertip laser sensor&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://openeuroscience.com/hardware-projects/open-prosthetics-and-robotics/takktile/" title="Takktile" target="_blank" rel="noopener">takktile&lt;/a>&lt;/p></description></item><item><title>Backyard Brains</title><link>https://open-neuroscience.com/post/backyard_brains/</link><pubDate>Tue, 29 Jan 2013 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/backyard_brains/</guid><description>&lt;p>&lt;a href="https://backyardbrains.com/" target="_blank" rel="noopener">Backyard brains&lt;/a> started out producing low cost, portable, electrophysiology systems to bring neuroscience to classrooms and help promote it.&lt;/p>
&lt;p>“Backyard brains wants to be for neuroscience, what the telescope is for astronomers” – meaning that the idea is that with a couple of hundred dollars anyone can get one of these recording systems and start doing experiments, like amateur astronomers can buy telescopes and start observing the cosmos.&lt;/p>
&lt;iframe width="790" height="444" src="https://www.youtube.com/embed/-mKen7tCDCs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe></description></item><item><title>10$ smartphone microscope</title><link>https://open-neuroscience.com/post/10_smartphone_microscope/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/10_smartphone_microscope/</guid><description>&lt;p>This &lt;a href="http://www.instructables.com/id/10-Smartphone-to-digital-microscope-conversion/%20how%20to%20use%20a%20smartphone%20for%20big%20amplifications" target="_blank" rel="noopener">neat little project&lt;/a> uses some plexi-glass, lens extracted from a laser pointer to harvest the power of smartphone cameras for some very big amplifications! Yoshinok manged to see cell plasmolysis and some other cool features with it.&lt;/p>
&lt;p>&lt;img src="https://i2.wp.com/www.instructables.com/files/deriv/FX0/QLMO/HMMF5O43/FX0QLMOHMMF5O43.MEDIUM.jpg?w=800" alt="Vegetal slice" data-recalc-dims="1" />&lt;/p></description></item><item><title>Attys</title><link>https://open-neuroscience.com/post/attys/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/attys/</guid><description>&lt;p>&lt;a href="https://www.attys.tech/" target="_blank" rel="noopener">Attys&lt;/a> is an wearable data acquisition device with a special focus on biomedical signals such as heart activity (ECG), muscle activity (EMG) and brain activity (EEG). It’s open firmware, open API and has open source applications on github in C++ and JAVA to encourage people to create their own custom versions for mobile devices, tablets and PC.&lt;/p>
&lt;p>The story of the Attys started when Dr. Bernd Porr filmed numerous youTube clips to educate the public about the possibilities and limits of biosignal measurement (&lt;a href="http://biosignals.berndporr.me.uk">http://biosignals.berndporr.me.uk&lt;/a>) which are featured here: &lt;a href="http://openeuroscience.com/hardware-projects/human-electrophysiology/bio-signal/" target="_blank" rel="noopener">BPM link&lt;/a>&lt;/p>
&lt;p>The site has been very popular ever since and visitors have been asking if a ready made bio-amp could be made available. This year Dr. Porr then decided to make one. This was the birth of the Attys.&lt;/p>
&lt;p>Attys is also a general educational tool to measure any physical quantity such as temperature, pressure or light intensity. It works with Google’s open source Science Journal and turns every Android phone or tablet into an electronic lab book / oscilloscope. Of course one can measure biosignals with it, too.&lt;/p>
&lt;p>Vasso Georgiadou has been the main presenter for our biosignal channel. Here, she shows off the Attys:&lt;/p>
&lt;iframe width="790" height="444" src="https://www.youtube.com/embed/TG5cRvgFEDA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe></description></item><item><title>BB LED Matrix</title><link>https://open-neuroscience.com/post/bb_led_matrix/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/bb_led_matrix/</guid><description>&lt;p>This project uses a 32X32 LED array (1024 LEDs in total) and a beagle bone black board. &lt;a href="https://bikerglen.com/projects/lighting/led-panel-1up/" target="_blank" rel="noopener">The page describing the project&lt;/a> has very nice explanations on how the whole system works (and LED displays in general).&lt;/p>
&lt;p>From this project, the creator &lt;a href="https://twitter.com/bikerglen" target="_blank" rel="noopener">Glen Akins&lt;/a>, went on to construct a 3X2 matrix of 32X32 LEDS, or a total of 6144 RGB LEDs that have a 200Hz refresh rate! Check out the video below of the panel in action:&lt;/p>
&lt;iframe width="790" height="444" src="https://www.youtube.com/embed/LBeVMGOgWvY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe></description></item><item><title>Blinkenschild</title><link>https://open-neuroscience.com/post/blinkeschild/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/blinkeschild/</guid><description>&lt;p>&lt;a href="https://hackaday.io/project/363-blinkenschild" target="_blank" rel="noopener">Blinkenschild&lt;/a> is a portable sign consisting of 960 RGB LEDs. The images/movies to be displayed are stored in a SD card in a Teensy3 board and controlled via bluetooth. Resolution is not as high as LCD monitors but the refresh rate is much higher:&lt;/p>
&lt;pre>&lt;code>This is done in realtime and pixelvalues are recalculated before display.
This is still too fast so i had to add 30 ms delay between the frames or we would not perceive it as a fluid animation but rather just blinking bright light.
&lt;/code>&lt;/pre>
&lt;iframe width="790" height="593" src="https://www.youtube.com/embed/VX14pmky07Q" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe></description></item><item><title>BPM Biosignal</title><link>https://open-neuroscience.com/post/bpm_biosignal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/bpm_biosignal/</guid><description>&lt;p>BPM Biosignal is a two stage amplifier created mainly for educational purposes.&lt;/p>
&lt;p>Check their &lt;a href="https://www.youtube.com/c/BPMbiosignals" target="_blank" rel="noopener">YouTube Channel&lt;/a>.&lt;/p></description></item><item><title>Brain Map</title><link>https://open-neuroscience.com/post/brain_map/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/brain_map/</guid><description>&lt;p>&lt;a href="https://people.ece.cornell.edu/land/courses/ece4760/FinalProjects/s2012/pmd68_mab448/pmd68_mab448/index.html" target="_blank" rel="noopener">BrainMap&lt;/a> expands the accessible DIY projects for brain activity measurements.&lt;/p>
&lt;p>This is the conclusion project of Patrick Dear and Mark Bunney Jr. at Cornell university where they used infrared leds to measure differences in blood flow at the scalp and map the motor cortex.&lt;/p></description></item><item><title>DIY PCR</title><link>https://open-neuroscience.com/post/diy_pcr/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/diy_pcr/</guid><description>&lt;p>&lt;a href="https://hackaday.io/hacker/24043-katherina-baranova" target="_blank" rel="noopener">Katharina&lt;/a> and &lt;a href="https://hackaday.io/hacker/24028-alex-bondarekno" target="_blank" rel="noopener">Alex&lt;/a> are developing a classic PCR machine: 16 samples and a heated lid.&lt;/p>
&lt;p>&lt;a href="https://hackaday.io/project/2548-open-source-thermal-cycler" target="_blank" rel="noopener">You can find more details of their project here&lt;/a>&lt;/p>
&lt;p>Here is a demo video:&lt;/p>
&lt;iframe width="500" height="281" src="https://www.youtube.com/embed/R7leQlkBKJw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe></description></item><item><title>DremelFuge</title><link>https://open-neuroscience.com/post/dremelfuge/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/dremelfuge/</guid><description>&lt;p>&lt;a href="https://www.thingiverse.com/thing:1483" target="_blank" rel="noopener">DremelFuge&lt;/a> is a very simple and clever centrifuge, buit perhaps not the safest one (be careful if you end up using it!).&lt;/p>
&lt;p>It takes advantage of 3d printing technology to print an adaptor that goes on to a Dremel (a precision tool that has really high rotation rates). It was created by &lt;a href="https://www.thingiverse.com/cathalgarvey/about" target="_blank" rel="noopener">Cathal&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://cdn.thingiverse.com/renders/ff/74/4c/b2/c4/2009-12-30-023824_display_large_preview_featured.jpg" alt="3d printed dremel attachment" title="DremelFuge">&lt;/p></description></item><item><title>Open BCI</title><link>https://open-neuroscience.com/post/bio_amp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/bio_amp/</guid><description>&lt;p>BioAmp is a biopotential acquisition device (EEG, ECG, EMG, EOG, etc.) developed in the Prototyping Laboratory at the School of Engineering of the National University of Entre Rios (Argentina).&lt;/p>
&lt;p>&lt;img src="./bio_amp_frontal.jpg" alt="Frontal view">&lt;/p>
&lt;p>Main features:&lt;/p>
&lt;pre>&lt;code>8 independent acquisition channels
24 bits of resolution per channel
2000 Hz is the maximum sampling frequency
USB connection (power and data transmission)
inputs for trigger signal
designed under electrical safety standards for medical use (electrical insulation, touch-proof connectors, etc.)
&lt;/code>&lt;/pre>
&lt;p>A very interesting feature of the BioAmp is the possibility of combining two amplifiers to double the number of recording channels. It is also possible to program each channel individually, offering the possibility of registering different types of signal simultaneously. For example, EEG, EOG, and EMG could be recorded during a sleep study, or EMG and ECG during a physical activity study, etc.&lt;/p>
&lt;p>&lt;img src="./bio_amp_back.jpg" alt="Posterior view">&lt;/p>
&lt;p>This project is currently in evolution and development, continually changes and updates are made to improve the product. Both the hardware source files (PCB and cabinet for 3D printing) and firmware are available in the project repository.&lt;/p>
&lt;p>For more information on this project and other projects carried out in the Prototyping Laboratory, visit the laboratory website.&lt;/p>
&lt;iframe id="video-2209-1_youtube_iframe" allowfullscreen="1" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" title="YouTube video player" src="https://www.youtube.com/embed/F7R7IxtyfGw?controls=0&amp;amp;rel=0&amp;amp;disablekb=1&amp;amp;showinfo=0&amp;amp;modestbranding=0&amp;amp;html5=1&amp;amp;iv_load_policy=3&amp;amp;autoplay=0&amp;amp;end=0&amp;amp;loop=0&amp;amp;playsinline=0&amp;amp;start=0&amp;amp;nocookie=false&amp;amp;enablejsapi=1&amp;amp;origin=https%3A%2F%2Fopeneuroscience.com&amp;amp;widgetid=1" width="829" height="466.3125" frameborder="0">&lt;/iframe></description></item><item><title>Open BCI</title><link>https://open-neuroscience.com/post/open-bci/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open-bci/</guid><description>&lt;p>&lt;a href="https://openbci.com/" target="_blank" rel="noopener">OpenBCI&lt;/a> is a complete open source EEG system that can be built either on top of an Arduino (8-bit system), or on top of chipKIT (32-bit system), which gives the system more local memory and allows for faster speeds.&lt;/p>
&lt;p>All software code and hardware (including a model for a 3D printable headset) plans can be found freely available at their &lt;a href="https://openbci.com/index.php/downloads" target="_blank" rel="noopener">download section&lt;/a> or at &lt;a href="https://github.com/OpenBCI" target="_blank" rel="noopener">GitHub&lt;/a>.&lt;/p></description></item><item><title>Open EEG</title><link>https://open-neuroscience.com/post/open_eeg/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open_eeg/</guid><description>&lt;p>&lt;a href="http://openeeg.sourceforge.net/doc/index.html" target="_blank" rel="noopener">The openEEG&lt;/a> project aims at describing and putting manuals for building a two channel EEG system for about U$200.
More on instructions on how to build one, can be found &lt;a href="http://openeeg.sourceforge.net/doc/SimpleEEG/" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>The latest update on the page seems to be a bit old, but Olimex sells the necessary PCB boards and accessories to &lt;a href="https://www.olimex.com/Products/EEG/OpenEEG/" target="_blank" rel="noopener">build the device&lt;/a>. They also sell the &lt;a href="https://www.olimex.com/Products/EEG/OpenEEG/EEG-SMT/open-source-hardware" target="_blank" rel="noopener">openEEG completely assembled&lt;/a>.&lt;/p></description></item><item><title>Open Ephys</title><link>https://open-neuroscience.com/post/open_ephys/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open_ephys/</guid><description>&lt;p>Open Ephys is a great initiative to create a suite that encompasses hardware for LFP and spiking recording, optogenetics combined with custom written software for microstimulation, environmental stimuli, extracellular recording and optogen. perturbations. Their ultimate goal is to create a system optimized for tetrodes and optogenetics where one is able to record and analyse data in real-time. On the &lt;a href="http://www.open-ephys.org/" target="_blank" rel="noopener">project’s website&lt;/a> one can download plans on how to build the devices and estimate on part cost (which is much, much lower than commercially available systems out there).&lt;/p></description></item><item><title>Open ExG</title><link>https://open-neuroscience.com/post/open_exg/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open_exg/</guid><description>&lt;p>OpenHardwareExG: is a project that provides both open source hardware and software for the measurement and analysis of different types of biosignals&lt;/p>
&lt;p>From the &lt;a href="http://openelectronicslab.github.io/OpenHardwareExG/" target="_blank" rel="noopener">project page&lt;/a>:&lt;/p>
&lt;pre>&lt;code>About the OpenHardwareExG project
Project goals
The main goal of the project is to build a device that allows the creation of electrophysiologic signal processing applications. In addition:
Hardware and software that we develop will have a free/open source license. We also prefer to use hardware and software that are free/open source.
We would like to keep the hardware &amp;quot;DIY compatible&amp;quot; (hand solderable, with parts that are readily available in small quantities, etc.)
For us, this is a hobby and learning project. It's important to keep it fun, and take the time to learn along the way.
&lt;/code>&lt;/pre></description></item><item><title>Open PCR</title><link>https://open-neuroscience.com/post/open_pcr/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open_pcr/</guid><description>&lt;p>&lt;a href="https://openpcr.org/" target="_blank" rel="noopener">Open PCR&lt;/a> is an open source PCR machine with heated lid and space for 12 samples&lt;/p></description></item><item><title>OpenStage</title><link>https://open-neuroscience.com/post/openstage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/openstage/</guid><description>&lt;p>&lt;a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0088977" target="_blank" rel="noopener">Open stage&lt;/a> is a low-cost motorised microscope stage capable of movement in the micrometer range.&lt;/p>
&lt;p>It features manual control via a control-pad, different movement velocities and pc communication through the serial port.&lt;/p>
&lt;p>The authors also state that due to its simplicity, the system could be used to drive micromanipulators and other devices&lt;/p></description></item><item><title>Parallela</title><link>https://open-neuroscience.com/post/parallela/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/parallela/</guid><description>&lt;p>&lt;a href="http://www.parallella.org/Introduction/" target="_blank" rel="noopener">Parallela&lt;/a>, an open source, open access card sized supercomputer, has the mission of bringing parallel computing to the masses by combining multiple RISC processors and very low power consumption. Produced by the &lt;a href="http://www.adapteva.com/" target="_blank" rel="noopener">Adapteva company&lt;/a>.&lt;/p></description></item><item><title>Signal Generators</title><link>https://open-neuroscience.com/post/signal-generators/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/signal-generators/</guid><description>&lt;p>Every lab needs a signal generator once in a while. They are useful to see if your acquisition program is working properly, to test why a certain piece of equipment is not working properly or to generate cues and targets at behavioural paradigms. Listed below are different generators, built using arduinos and other microcontrollers. They have different degrees of complexity and capabilities, so it would be wise to briefly look through them and see what fits you best!&lt;/p>
&lt;hr>
&lt;p>&lt;a href="http://www.instructables.com/id/Arduino-Waveform-Generator/" target="_blank" rel="noopener">The arduino waveform generator&lt;/a> is a pretty straight forward project that is able to generate four different waveforms from 1Hz to 50kHz. Gain, frequency, modulation and waveform type are controlled by nobs.&lt;/p>
&lt;iframe width="800" height="422" src="https://www.youtube.com/embed/gz_gVKWFN8E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe>
&lt;hr>
&lt;p>&lt;a href="http://www.instructables.com/id/Atmel-Xmega-USBSerial-Arbitrary-Waveform-Generato/?ALLSTEPS" target="_blank" rel="noopener">Atmel Xmega USB/Serial Arbitrary Waveform Generator&lt;/a> runs using a boston android XMEGA evaluation board and is able to deliver square, sine, triangular and arbitrary waveforms in between 5Hz and 20kHz. This one is not a stand alone system, which means that to set a new waveform type, one would have to have the board connect to a computer at all times.&lt;/p>
&lt;p>&lt;img src="https://i1.wp.com/www.instructables.com/files/deriv/FWF/PWX4/G79D44SM/FWFPWX4G79D44SM.LARGE.jpg?w=800" alt="arbitrary waveform generator" data-recalc-dims="1" />&lt;/p>
&lt;hr>
&lt;p>&lt;a href="http://arduino.cc/en/Tutorial/DueSimpleWaveformGenerator" target="_blank" rel="noopener">Simple waveform generator&lt;/a> seems to be the most straight forward of all projects, requiring only a potentiometer, a couple of resistors and push buttons. The trade off is that with the present sketch, waveforms of only up to 170Hz can be generated. It generates sawtooth, square, triangular and sine waveforms.&lt;/p>
&lt;p>&lt;img src="https://i0.wp.com/arduino.cc/en/uploads/Tutorial/DueSimpleWaveform_fritzing.png?w=800" alt="arduino due waveform generator" data-recalc-dims="1" />&lt;/p></description></item><item><title>Skinner Box with RPi+Python</title><link>https://open-neuroscience.com/post/skinnerbox_rpi_python/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/skinnerbox_rpi_python/</guid><description>&lt;p>This project was developed by &lt;a href="http://www.kscottz.com/about/" target="_blank" rel="noopener">Katherine Scott&lt;/a> to be presented at the PyCon 2014. She developed a skinner box for her pet rats using a raspberry pi and some 3D printed parts. The setup contain a food dispenser, a buzzer, levers, a camera to observe the animals and it is hooked in a way that everything can be controlled over the internet!&lt;/p>
&lt;p>You can find the files for 3D parts &lt;a href="http://www.thingiverse.com/thing:296335" target="_blank" rel="noopener">here&lt;/a> and a better description of the project &lt;a href="http://www.kscottz.com/open-skinner-box-pycon-2014/" target="_blank" rel="noopener">here&lt;/a>&lt;/p>
&lt;p> &lt;/p>
&lt;iframe width="790" height="444" src="https://www.youtube.com/embed/grMfIoDgn9M" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe>
&lt;p> &lt;/p></description></item><item><title>Stereo microscope</title><link>https://open-neuroscience.com/post/stereo_microscope/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/stereo_microscope/</guid><description>&lt;p>Although stereo microscopes are an essential piece of hardware in biology labs, sometimes we wish they had more features, like the possibility to record the magnified images with a camera, or have a better lighting system to enhance contrast on those small samples.&lt;/p>
&lt;p>One person has taken those issues to heart and tackled them all in a very brilliant way. Below you&amp;rsquo;ll find links to &lt;a href="http://www.tangentaudio.com/about/" target="_blank" rel="noopener">Steve&amp;rsquo;s blog&lt;/a>, where he describes, in a very detailed way, three projects to enhance the all familiar stereo microscope:&lt;/p>
&lt;p>&lt;a href="http://www.tangentaudio.com/mechanical/microscope-camera-output/" target="_blank" rel="noopener">Camera eye piece adaptor.&lt;/a>&lt;/p>
&lt;p>&lt;img src="featured.jpg" alt="">&lt;/p>
&lt;p>&lt;a href="http://www.tangentaudio.com/2013/03/aziz-light/" target="_blank" rel="noopener">AZIZ a ring lighting system.&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://i1.wp.com/www.tangentaudio.com/wp-content/uploads/2013/03/DSC_6828-modified-1024x680.jpg?resize=800%2C531" data-recalc-dims="1" />&lt;/p>
&lt;p>&lt;a href="http://www.tangentaudio.com/2013/02/epic-builds-articulated-stereo-microscope-arm/" target="_blank" rel="noopener">Articulated stereo microscope mount.&lt;/a>&lt;/p>
&lt;p>&lt;span class="embed-youtube" style="text-align:center; display: block;">&lt;/span>&lt;/p>
&lt;p>Some of them are not that easy to reproduce, but maybe be a good starting point for other DIY versions.&lt;/p></description></item><item><title>Syringe Pump</title><link>https://open-neuroscience.com/post/syringe_pump/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/syringe_pump/</guid><description>&lt;p>From the &lt;a href="http://www.mse.mtu.edu/~pearce/Index.html" target="_blank" rel="noopener">Pearce lab&lt;/a>, this syringe pump was &lt;a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0107216" target="_blank" rel="noopener">published in Plos One&lt;/a> and is built using 3d printed parts, stepper motors and a raspberry pi, costing 5% or less than commercial available systems. Can be calibrated and customized for different applications.&lt;/p></description></item><item><title>Takktile</title><link>https://open-neuroscience.com/post/takktile/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/takktile/</guid><description>&lt;p>&lt;a href="http://www.takktile.com/" target="_blank" rel="noopener">Takktile&lt;/a>, is a tactile sensor to be used on robotic applications. The developers want to make it move away from the closed walls of research institutions by making it open source and cheap. It is built based on MEMs barometers and can sense 1 gram loads as well as coping with hammer blows (see video from their website below).&lt;/p>
&lt;p>&lt;span class="embed-youtube" style="text-align:center; display: block;">&lt;/span>&lt;/p></description></item><item><title>tDCS</title><link>https://open-neuroscience.com/post/tdcs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/tdcs/</guid><description>&lt;p>Although of simple complexity and using low currents, this tDCS machine is still to be considered a piece of equipment that could be dangerous both in the assembly and in the operation phases, so please inform yourself as best as you can before either of these steps! Also remember that the openeuroscience website cannot be held responsible for any injuries that might occur from improper use of this tool.&lt;/p>
&lt;p>&lt;a href="https://www.instructables.com/id/Build-a-Human-Enhancement-Device-Basic-tDCS-Suppl/" target="_blank" rel="noopener">DIY tDCS instructables&lt;/a>&lt;/p></description></item><item><title>The Yale open hand project</title><link>https://open-neuroscience.com/post/the_yale_open_hand_project/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/the_yale_open_hand_project/</guid><description>&lt;p>&lt;a href="http://www.eng.yale.edu/grablab/openhand/" target="_blank" rel="noopener">The Yale open hand project&lt;/a>, has a similar purpose of the open hand project, that is, to make prosthetic hands more widely available through the lowering of costs. They have a different design from the open hand project. Additionally the project wants to take advantage of the lowered costs to speed up the development cycle and provide, together with input from the user community, several different useful hand designs.&lt;/p></description></item></channel></rss>
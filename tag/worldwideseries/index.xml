<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>worldwideseries | Open Neuroscience</title><link>https://open-neuroscience.com/tag/worldwideseries/</link><atom:link href="https://open-neuroscience.com/tag/worldwideseries/index.xml" rel="self" type="application/rss+xml"/><description>worldwideseries</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>CC BY SA 4.0</copyright><lastBuildDate>Wed, 21 Apr 2021 00:00:00 +0000</lastBuildDate><image><url>https://open-neuroscience.com/media/openneuroscience_logo_dark.svg</url><title>worldwideseries</title><link>https://open-neuroscience.com/tag/worldwideseries/</link></image><item><title>Mobilefuge: Low-cost, Open-source 3D-printed centrifuge</title><link>https://open-neuroscience.com/post/mobilefuge_low_cost_open_source_3d_printed_centrifuge/</link><pubDate>Wed, 21 Apr 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/mobilefuge_low_cost_open_source_3d_printed_centrifuge/</guid><description>&lt;p>We made a low-cost centrifuge that can be useful for carrying out low-cost LAMP based detection of SARS-Cov2 virus in saliva. The 3D printed centrifuge (Mobilefuge) is portable, robust, stable, safe, easy to build and operate. The Mobilefuge doesn’t require soldering or programming skills and can be built without any specialised equipment, yet practical enough for high throughput use. More importantly, Mobilefuge can be powered from widely available USB ports, including mobile phones and associated power supplies. This allows the Mobilefuge to be used even in off-grid and resource limited settings.&lt;/p>
&lt;hr>
&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/svDpDSwnS3A" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
lJpWyzvg4Zk&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Chinna Devarapu&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.medrxiv.org/content/10.1101/2021.01.06.21249280v1">https://www.medrxiv.org/content/10.1101/2021.01.06.21249280v1&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/YwBiC7-dR-g" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;hr>
&lt;p>This post was automatically generated by
Chinna Devarapu&lt;/p>
&lt;hr></description></item><item><title>The Virtual Macaque Brain</title><link>https://open-neuroscience.com/post/the_virtual_macaque_brain/</link><pubDate>Wed, 21 Apr 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/the_virtual_macaque_brain/</guid><description>&lt;p>A whole-cortex macaque structural connectome constructed from a combination of axonal tract-tracing and diffusion-weighted imaging data. Created for modeling brain dynamics using TheVirtualBrain (thevirtualbrain.org) platform. A detailed description and example usage can be found in the paper here: &lt;a href="https://www.nature.com/articles/s41597-019-0129-z">https://www.nature.com/articles/s41597-019-0129-z&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/kHAEc-5lT2o" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;hr>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Kelly Shen; Gleb Bezgin; Michael Schirner; Petra Ritter; Stefan Everling; Anthony R. McIntosh&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://zenodo.org/record/1471588#.YH8YcpNKg6d">https://zenodo.org/record/1471588#.YH8YcpNKg6d&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Kelly Shen&lt;/p>
&lt;hr></description></item><item><title>Non-Telecentric 2P microscopy for 3D random access mesoscale imaging (nTCscope)</title><link>https://open-neuroscience.com/post/non_telecentric_2p_microscopy_for_3d_random_access_mesoscale_imaging_ntcscope_/</link><pubDate>Fri, 19 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/non_telecentric_2p_microscopy_for_3d_random_access_mesoscale_imaging_ntcscope_/</guid><description>&lt;p>Ultra-low-cost, easily implemented and flexible two-photon scanning microscopy modification offering a several-fold expanded three-dimensional field of view that also maintains single-cell resolution. Application of our system for imaging neuronal activity has been demonstrated on mice, zebrafish and fruit flies&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Filip Janiak; Philipp Bartel; Michael Bale; Takeshi Yoshimatsu; Emilia Komulainen; Mingyi Zhou; Kevin Staras; Lucia Prieto-Godino; Thomas Euler; Miguel Maravall; Tom Baden&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/BadenLab/nTCscope">https://github.com/BadenLab/nTCscope&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/3dXE6LjEcd8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;hr>
&lt;p>This post was automatically generated by
Filip Janiak&lt;/p>
&lt;hr></description></item><item><title>An open-source experimental framework for automation of high-throughput cell biology experiments</title><link>https://open-neuroscience.com/post/an_open_source_experimental_framework_for_automation_of_high_throughput_cell_biology_experiments/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/an_open_source_experimental_framework_for_automation_of_high_throughput_cell_biology_experiments/</guid><description>&lt;p>Modern Biology methods require a large number of high quality experiments to be conducted, which requires a high degree of automation. Our solution is an open-source hardware that allows for automatic high-throughput generation of large amounts of cell biology data. The hardware consists of an automatic XY-stage for moving a multiwell plate containing growing cells; a perfusion manifold allowing application of up to 8 different solutions; and a small epifluorescent microscope. It is extremely cheap (approximately £400 without and £2500 with a fluorescent microscope) and is easily customizable for individual experimental needs. We demonstrate the usability of this platform with high-throughput Ca2+ imaging and large-scale labelling experiments. All building instructions, software and PCB Gerber file are provided.&lt;/p>
&lt;hr>
&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/lJpWyzvg4Zk" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Pavel Katunin; Anton Nikolaev&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/frescolabs/FrescoM">https://github.com/frescolabs/FrescoM&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/RerEV6oOm4s" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;hr>
&lt;p>This post was automatically generated by
Anton Nikolaev&lt;/p>
&lt;hr></description></item><item><title>NeuroFedora</title><link>https://open-neuroscience.com/post/neurofedora/</link><pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/neurofedora/</guid><description>&lt;p>NeuroFedora is an initiative to provide a ready to use Fedora Linux based Free/Open source software platform for neuroscience. We believe that similar to Free software, science should be free for all to use, share, modify, and study. The use of Free software also aids reproducibility, data sharing, and collaboration in the research community. By making the tools used in the scientific process easier to use, NeuroFedora aims to take a step to enable this ideal.&lt;/p>
&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/2xAK1tY0qro" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>NeuroFedora volunteers @ the Fedora project&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://neuro.fedoraproject.org">https://neuro.fedoraproject.org&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Ankur Sinha (NeuroFedora SIG member)&lt;/p>
&lt;hr></description></item><item><title>DeepLabStream</title><link>https://open-neuroscience.com/post/deeplabstream/</link><pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/deeplabstream/</guid><description>&lt;p>DeepLabStream is a python based multi-purpose tool that enables the realtime tracking of animals and manipulation of experiments. Our toolbox is adapted from the previously published DeepLabCut (Mathis et al., 2018) and expands on its core capabilities. DeepLabStreams core feature is the real-time analysis using any type of camera-based video stream (incl. multiple streams). Building onto that, we designed a full experimental closed-loop toolkit. It enables running experimental protocols that are dependent on a constant stream of bodypart positions and feedback activation of several input/output devices. It&amp;rsquo;s capabilities range from simple region of interest (ROI) based triggers to headdirection or behavior dependent stimulation.&lt;/p>
&lt;hr>
&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/ZspLDZb_kMI" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Schwarz Neurocon Lab&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/SchwarzNeuroconLab/DeepLabStream">https://github.com/SchwarzNeuroconLab/DeepLabStream&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item></channel></rss>
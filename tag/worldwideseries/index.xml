<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>worldwideseries | Open Neuroscience</title><link>https://open-neuroscience.com/tag/worldwideseries/</link><atom:link href="https://open-neuroscience.com/tag/worldwideseries/index.xml" rel="self" type="application/rss+xml"/><description>worldwideseries</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>CC BY SA 4.0</copyright><lastBuildDate>Wed, 21 Apr 2021 00:00:00 +0000</lastBuildDate><image><url>https://open-neuroscience.com/media/openneuroscience_logo_dark.svg</url><title>worldwideseries</title><link>https://open-neuroscience.com/tag/worldwideseries/</link></image><item><title>Mobilefuge: Low-cost, Open-source 3D-printed centrifuge</title><link>https://open-neuroscience.com/post/mobilefuge_low_cost_open_source_3d_printed_centrifuge/</link><pubDate>Wed, 21 Apr 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/mobilefuge_low_cost_open_source_3d_printed_centrifuge/</guid><description>&lt;p>We made a low-cost centrifuge that can be useful for carrying out low-cost LAMP based detection of SARS-Cov2 virus in saliva. The 3D printed centrifuge (Mobilefuge) is portable, robust, stable, safe, easy to build and operate. The Mobilefuge doesnâ€™t require soldering or programming skills and can be built without any specialised equipment, yet practical enough for high throughput use. More importantly, Mobilefuge can be powered from widely available USB ports, including mobile phones and associated power supplies. This allows the Mobilefuge to be used even in off-grid and resource limited settings.&lt;/p>
&lt;hr>
&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/svDpDSwnS3A" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Chinna Devarapu&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.medrxiv.org/content/10.1101/2021.01.06.21249280v1">https://www.medrxiv.org/content/10.1101/2021.01.06.21249280v1&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/YwBiC7-dR-g" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;hr>
&lt;p>This post was automatically generated by
Chinna Devarapu&lt;/p>
&lt;hr></description></item><item><title>The Virtual Macaque Brain</title><link>https://open-neuroscience.com/post/the_virtual_macaque_brain/</link><pubDate>Wed, 21 Apr 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/the_virtual_macaque_brain/</guid><description>&lt;p>A whole-cortex macaque structural connectome constructed from a combination of axonal tract-tracing and diffusion-weighted imaging data. Created for modeling brain dynamics using TheVirtualBrain (thevirtualbrain.org) platform. A detailed description and example usage can be found in the paper here: &lt;a href="https://www.nature.com/articles/s41597-019-0129-z">https://www.nature.com/articles/s41597-019-0129-z&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/kHAEc-5lT2o" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;hr>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Kelly Shen; Gleb Bezgin; Michael Schirner; Petra Ritter; Stefan Everling; Anthony R. McIntosh&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://zenodo.org/record/1471588#.YH8YcpNKg6d">https://zenodo.org/record/1471588#.YH8YcpNKg6d&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Kelly Shen&lt;/p>
&lt;hr></description></item><item><title>BrainGlobe</title><link>https://open-neuroscience.com/post/brainglobe/</link><pubDate>Thu, 25 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/brainglobe/</guid><description>&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/Ndsssf_gHns" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>BrainGlobe is a suite of Python-based computational neuroanatomy software tools. We provide software packages for the analysis and visualisation of neuroanatomical data, particularly from whole-brain microscopy. In addition, we provide tools for working with brain atlases, to simplify development of new tools and aid collaboration and cooperation by adopting common standards.&lt;/p>
&lt;p>Our tools include:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://open-neuroscience.com/post/brainglobe_atlas_api/" target="_blank" rel="noopener">BrainGlobe Atlas API&lt;/a> - A lightweight python module to interact with atlases for systems neuroscience&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://open-neuroscience.com/post/cellfinder/" target="_blank" rel="noopener">Cellfinder&lt;/a> - Automated 3D cell detection and registration of whole-brain microscopy images&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://open-neuroscience.com/post/brainrender/" target="_blank" rel="noopener">Brainrender&lt;/a> - A Python based software for visualization of neuroanatomical and morphological data.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Project Author(s): Adam Tyson; Federico Claudi; Luigi Petrucco&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/brainglobe">https://github.com/brainglobe&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://brainglobe.info/">https://brainglobe.info/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Adam Tyson&lt;/p>
&lt;hr></description></item><item><title>Kilosort</title><link>https://open-neuroscience.com/post/kilosort/</link><pubDate>Sun, 21 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/kilosort/</guid><description>&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/FlRql3g41iE" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>Kilosort is a software package for identifying neurons and their spikes in extracellular electrophysiology, a process known as &amp;ldquo;spike sorting&amp;rdquo;. Kilosort has been primarily developed and tested on the Neuropixels 1.0 probes, but is now used on data acquired with a wide variety of ephys methods, from Utah arrays to tetrodes and to the latest Neuropixels 2.0 probes.&lt;/p>
&lt;p>Spike sorting consists of several largely-independent steps: data preprocessing, drift correction, spike clustering, template matching, and results postprocessing. There have been several versions of Kilosort, improving on various aspects of these steps, and we are currently on version v3. In many cases, and especially for Neuropixels probes, the automated output of Kilosort3 requires minimal manual curation. The main change from v2.5 is a completely new and much more sophisticated clustering algorithm. To learn about Kilosort2.5, the primary reference is the Neuropixels 2.0 paper. Kilosort2.5 improves on Kilosort2 primarily in the type of drift correction we use. Where Kilosort2 modified templates as a function of time/drift (a drift tracking approach), Kilosort2.5 corrects the raw data directly via a sub-pixel registration process (a drift correction approach).&lt;/p>
&lt;p>All versions of Kilosort so far have been developed and released in Matlab. However, we are advancing towards Python releases of the codebase, including of older version like Kilosort 2 and 2.5 (available here: &lt;a href="https://github.com/MouseLand/pykilosort/tree/master/pykilosort">https://github.com/MouseLand/pykilosort/tree/master/pykilosort&lt;/a> ).&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Marius Pachitariu&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/MouseLand/Kilosort">https://github.com/MouseLand/Kilosort&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/clq50N7V_wA" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;hr>
&lt;p>This post was automatically generated by
Marius Pachitariu&lt;/p>
&lt;hr></description></item><item><title>Non-Telecentric 2P microscopy for 3D random access mesoscale imaging (nTCscope)</title><link>https://open-neuroscience.com/post/non_telecentric_2p_microscopy_for_3d_random_access_mesoscale_imaging_ntcscope_/</link><pubDate>Fri, 19 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/non_telecentric_2p_microscopy_for_3d_random_access_mesoscale_imaging_ntcscope_/</guid><description>&lt;p>Ultra-low-cost, easily implemented and flexible two-photon scanning microscopy modification offering a several-fold expanded three-dimensional field of view that also maintains single-cell resolution. Application of our system for imaging neuronal activity has been demonstrated on mice, zebrafish and fruit flies&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Filip Janiak; Philipp Bartel; Michael Bale; Takeshi Yoshimatsu; Emilia Komulainen; Mingyi Zhou; Kevin Staras; Lucia Prieto-Godino; Thomas Euler; Miguel Maravall; Tom Baden&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/BadenLab/nTCscope">https://github.com/BadenLab/nTCscope&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/3dXE6LjEcd8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;hr>
&lt;p>This post was automatically generated by
Filip Janiak&lt;/p>
&lt;hr></description></item><item><title>Addgene's AAV Data Hub</title><link>https://open-neuroscience.com/post/addgene_s_aav_data_hub/</link><pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/addgene_s_aav_data_hub/</guid><description>&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/U_P2CD746pI" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;br>
&lt;p>AAV are versatile tools used by neuroscientists for expression and manipulation of neurons. Many scientists have benefited from the high-quality, ready-to-use AAV prep service from Addgene, a nonprofit plasmid repository. However, it can be challenging to determine which AAV tool and techniques are best to use for an experiment. Scientists also may have questions about how much virus to inject or which serotype or promoter should be used to target the desired neuron or brain region. To help scientists answer these questions, Addgene launched an open platform called the AAV Data Hub (&lt;a href="https://datahub.addgene.org/aav/">https://datahub.addgene.org/aav/&lt;/a>) which allows researchers to easily share practical experimental details with the scientific community (AAV used, in vivo model used, injection site, injection volumes, etc.). The goal of this platform is to help scientists find the best AAV tool for their experiments by reviewing combined data from a broad range of research labs. The AAV Data Hub launched in late 2019 and over 100 experiments have since been contributed to this project. The dataset includes details and images from experiments conducted in six different species and several different expression sites.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Addgene&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://datahub.addgene.org/aav/">https://datahub.addgene.org/aav/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/ZPKdr1RdtGI" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;hr>
&lt;p>This post was automatically generated by
Angela Abitua&lt;/p>
&lt;hr></description></item><item><title>Feeding Experimentation Device ver3 (FED3)</title><link>https://open-neuroscience.com/post/feeding_experimentation_device_ver3_fed3_/</link><pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/feeding_experimentation_device_ver3_fed3_/</guid><description>&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/-DinyZiqo3I" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>FED3 is an open-source battery-powered device for home-cage training of mice in operant tasks. FED3 can be 3D printed and the control code is open-source and can be modified. The code is written in the Arduino language and is run on an Adafruit Feather M0 Adalogger microcontroller inside of FED3. Mice interact with FED3 through two nose-pokes and FED3 responds with visual stimuli, auditory stimuli, and by dispensing pellets. FED3 also has an analog output that allows it to synchronize with and control external equipment such as lasers or brain recording systems. A screen provides feedback to the user, and all behavioral events are logged to an on-board microSD card. The default code includes multiple built-in programs but FED3 is open-source and hackable, and can be easily modified to perform other tasks.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Lex Kravitz&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/KravitzLabDevices/FED3">https://github.com/KravitzLabDevices/FED3&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/3kBaFsYON4U" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;hr>
&lt;p>This post was automatically generated by
Lex Kravitz&lt;/p>
&lt;hr></description></item><item><title>Suite2P</title><link>https://open-neuroscience.com/post/suite2p/</link><pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/suite2p/</guid><description>&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/Zxbz2OUnzLY" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>Suite2P is a very modular imaging processing pipeline written in Python which allows you to perform registration of raw data movies, automatic cell detection, extraction of calcium traces and infers spike times. It is a very fast and accurate tool and can work on standard workstations. It also includes a visualization graphical user interface (GUI) that facilitates analysis and manual curation of the cell detection algorithm.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Carsen Stringer and Marius Pachitariu&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.suite2p.org/">https://www.suite2p.org/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>An open-source experimental framework for automation of high-throughput cell biology experiments</title><link>https://open-neuroscience.com/post/an_open_source_experimental_framework_for_automation_of_high_throughput_cell_biology_experiments/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/an_open_source_experimental_framework_for_automation_of_high_throughput_cell_biology_experiments/</guid><description>&lt;p>Modern Biology methods require a large number of high quality experiments to be conducted, which requires a high degree of automation. Our solution is an open-source hardware that allows for automatic high-throughput generation of large amounts of cell biology data. The hardware consists of an automatic XY-stage for moving a multiwell plate containing growing cells; a perfusion manifold allowing application of up to 8 different solutions; and a small epifluorescent microscope. It is extremely cheap (approximately Â£400 without and Â£2500 with a fluorescent microscope) and is easily customizable for individual experimental needs. We demonstrate the usability of this platform with high-throughput Ca2+ imaging and large-scale labelling experiments. All building instructions, software and PCB Gerber file are provided.&lt;/p>
&lt;hr>
&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/lJpWyzvg4Zk" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Pavel Katunin; Anton Nikolaev&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/frescolabs/FrescoM">https://github.com/frescolabs/FrescoM&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/RerEV6oOm4s" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;hr>
&lt;p>This post was automatically generated by
Anton Nikolaev&lt;/p>
&lt;hr></description></item><item><title>YAPiC</title><link>https://open-neuroscience.com/post/yapic/</link><pubDate>Tue, 10 Nov 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/yapic/</guid><description>&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/l_jrJGuzztg" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;br>
With YAPiC you can make your own customized filter (also called model or classifier) to enhance a certain structure of your choice with a simple Python based command line interface, installable with pip. We have used YAPiC so far for analyzing various microscopy image data. Our experiments are mainly related to neurobiology, cell biology, histopathology and drug discovery (high content screening). However, YAPiC is a very generally applicable tool and can be applied to very different domains. It could be used for detecting e.g. forest regions in satellite images, clouds in landscape photographs or fried eggs in food photography.
Pixel classification in YAPiC is based on deep learning wit fully convolutional neural networks. Development of YAPiC started in 2015, when Ronneberger et al. presented a U-shaped fully convolutional neural network that was capable of solving highly challenging pixel classification tasks in bio images, such as tumor classification in histological slides or cell segmentation in brightfield DIC images.
&lt;p>YAPiC was designed to make this new kind of AI powered pixel classification simply applicable, i.e feasible to use for a PhD student in his/her imaging project.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Christoph Moehl; Manuel Schoelling&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://yapic.github.io/yapic/">https://yapic.github.io/yapic/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Christoph Moehl&lt;/p>
&lt;hr></description></item><item><title>NeuroFedora</title><link>https://open-neuroscience.com/post/neurofedora/</link><pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/neurofedora/</guid><description>&lt;p>NeuroFedora is an initiative to provide a ready to use Fedora Linux based Free/Open source software platform for neuroscience. We believe that similar to Free software, science should be free for all to use, share, modify, and study. The use of Free software also aids reproducibility, data sharing, and collaboration in the research community. By making the tools used in the scientific process easier to use, NeuroFedora aims to take a step to enable this ideal.&lt;/p>
&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/2xAK1tY0qro" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>NeuroFedora volunteers @ the Fedora project&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://neuro.fedoraproject.org">https://neuro.fedoraproject.org&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Ankur Sinha (NeuroFedora SIG member)&lt;/p>
&lt;hr></description></item><item><title>PiVR</title><link>https://open-neuroscience.com/post/pivr/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pivr/</guid><description>&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/uG898FL421U" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;br>
PiVR is a system that allows experimenters to immerse small animals into virtual realities. The system tracks the position of the animal and presents light stimulation according to predefined rules, thus creating a virtual landscape in which the animal can behave. By using optogenetics, we have used PiVR to present fruit fly larvae with virtual olfactory realities, adult fruit flies with a virtual gustatory reality and zebrafish larvae with a virtual light gradient.
&lt;p>PiVR operates at high temporal resolution (70Hz) with low latencies (&amp;lt;30 milliseconds) while being affordable (&amp;lt;US$500) and easy to build (&amp;lt;6 hours). Through extensive documentation (&lt;a href="http://www.PiVR.org">www.PiVR.org&lt;/a>), this tool was designed to be accessible to a wide public, from high school students to professional researchers studying systems neuroscience in academia.&lt;/p>
&lt;p>The project is open source (BSD-3) and the documented code written in the freely available programming language Python. We hope that PiVR will be adapted by advanced users for their particular needs, for example to create closed-loop experiments involving other sensory modalities (e.g., sound/vibration) through the use of PWM controllable devices. We envision PiVR to be used as the central module when creating virtual realities for a variety of sensory modalities. This â€˜PiVR moduleâ€™ takes care of detecting the animal and presenting the appropriate PWM signal that is then picked up by the PWM controllable device installed by the user, for example to produce a sound whenever an animal enters a pre-defined region.&lt;/p>
&lt;p>In short, PiVR is a powerful and affordable experimental platform allowing experimenters to create a wide array of virtual reality experiments. Our hope is that PiVR will be adapted by several labs to democratize closed-loop experiments and, by standardizing image quality and the animal detection algorithm, increase reproducibility.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>David Tadres; Matthieu Louis&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://www.PiVR.org">http://www.PiVR.org&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/w5tIG6B6FWo" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;hr>
&lt;p>This post was automatically generated by
David Tadres&lt;/p>
&lt;hr></description></item><item><title>DeepLabStream</title><link>https://open-neuroscience.com/post/deeplabstream/</link><pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/deeplabstream/</guid><description>&lt;p>DeepLabStream is a python based multi-purpose tool that enables the realtime tracking of animals and manipulation of experiments. Our toolbox is adapted from the previously published DeepLabCut (Mathis et al., 2018) and expands on its core capabilities. DeepLabStreams core feature is the real-time analysis using any type of camera-based video stream (incl. multiple streams). Building onto that, we designed a full experimental closed-loop toolkit. It enables running experimental protocols that are dependent on a constant stream of bodypart positions and feedback activation of several input/output devices. It&amp;rsquo;s capabilities range from simple region of interest (ROI) based triggers to headdirection or behavior dependent stimulation.&lt;/p>
&lt;hr>
&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/ZspLDZb_kMI" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Schwarz Neurocon Lab&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/SchwarzNeuroconLab/DeepLabStream">https://github.com/SchwarzNeuroconLab/DeepLabStream&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>SpikeInterface</title><link>https://open-neuroscience.com/post/spikeinterface/</link><pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/spikeinterface/</guid><description>&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/p_dd52IzOGo" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>SpikeInterface is a unified Python framework for spike sorting. With its high-level API, it is designed to be accessible and easy to use, allowing users to build full analysis pipelines for spike sorting (reading-writing (IO) / preprocessing / spike sorting / postprocessing / validation / curation / comparison / visualization) with a few lines of code.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Alessio Buccino*; Cole Hurwitz*; Samuel Garcia; Jeremy Magland; Josh Siegle; Matthias Hennig&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/SpikeInterface/spikeinterface">https://github.com/SpikeInterface/spikeinterface&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/nWJGwFB7oII" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;hr>
&lt;p>This post was automatically generated by
Alessio Buccino&lt;/p>
&lt;hr></description></item><item><title>OpenFlexure</title><link>https://open-neuroscience.com/post/openflexure/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/openflexure/</guid><description>&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/__mND9BFOts" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;br>
OpenFlexure is a 3D printed flexure translation stage, developed by a group at the Bath University. The stage is capable of sub-micron-scale motion, with very small drift over time. Which makes it quite good, among other things, for time-lapse protocols that need to be done over days/weeks time, and under space restricted areas, such as fume hoods. A paper describing it in detail can be found [here](http://arxiv.org/abs/1509.05394).
&lt;p>Adding a camera and servo motors, turns the stage into an automated microscope. More details about the project can be found &lt;a href="https://openflexure.org/" target="_blank" rel="noopener">here&lt;/a>.&lt;/p></description></item><item><title>Open Ephys</title><link>https://open-neuroscience.com/post/open_ephys/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open_ephys/</guid><description>&lt;h2 id="world-wide-series-seminar">World Wide Series Seminar&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/DmK2CtsjqHc" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>Open Ephys is a great initiative to create a suite that encompasses hardware for LFP and spiking recording, optogenetics combined with custom written software for microstimulation, environmental stimuli, extracellular recording and optogen. perturbations. Their ultimate goal is to create a system optimized for tetrodes and optogenetics where one is able to record and analyse data in real-time. On the &lt;a href="http://www.open-ephys.org/" target="_blank" rel="noopener">projectâ€™s website&lt;/a> one can download plans on how to build the devices and estimate on part cost (which is much, much lower than commercially available systems out there).&lt;/p></description></item></channel></rss>
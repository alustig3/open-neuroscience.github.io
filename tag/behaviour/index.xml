<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Behaviour | Open Neuroscience</title><link>https://open-neuroscience.com/tag/behaviour/</link><atom:link href="https://open-neuroscience.com/tag/behaviour/index.xml" rel="self" type="application/rss+xml"/><description>Behaviour</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>CC BY SA 4.0</copyright><lastBuildDate>Fri, 22 Jan 2021 00:00:00 +0000</lastBuildDate><image><url>https://open-neuroscience.com/media/openneuroscience_logo_dark.svg</url><title>Behaviour</title><link>https://open-neuroscience.com/tag/behaviour/</link></image><item><title>Efficient training of mice on the 5-choice serial reaction time task in an automated rodent training system</title><link>https://open-neuroscience.com/post/efficient_training_of_mice_on_the_5_choice_serial_reaction_time_task_in_an_automated_rodent_training_system/</link><pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/efficient_training_of_mice_on_the_5_choice_serial_reaction_time_task_in_an_automated_rodent_training_system/</guid><description>&lt;p>Experiments aiming to understand sensory-motor systems, cognition and behavior necessitate training animals to perform complex tasks. Traditional training protocols require lab personnel to move the animals between home cages and training chambers, to start and end training sessions, and in some cases, to hand-control each training trial. Human labor not only limits the amount of training per day, but also introduces several sources of variability and may increase animal stress. Here we present an automated training system for the 5-choice serial reaction time task (5CSRTT), a classic rodent task often used to test sensory detection, sustained attention and impulsivity. We found that full automation without human intervention allowed rapid, cost-efficient training, and decreased stress as measured by corticosterone levels. Training breaks introduced only a transient drop in performance, and mice readily generalized across training systems when transferred from automated to manual protocols. We further validated our automated training system with wireless optogenetics and pharmacology experiments, expanding the breadth of experimental needs our system may fulfill. Our automated 5CSRTT system can serve as a prototype for fully automated behavioral training, with methods and principles transferrable to a range of rodent tasks.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Eszter Birtalan; Anita Bánhidi; Joshua I Sanders; Diána Balázsfi; Balázs Hangya;&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/hangyabalazs/ATS">https://github.com/hangyabalazs/ATS&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Balazs Hangya&lt;/p>
&lt;hr></description></item><item><title>jsPsych</title><link>https://open-neuroscience.com/post/jspsych/</link><pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/jspsych/</guid><description>&lt;p>jsPsych is a JavaScript library for running behavioral experiments in a web browser. The library provides a flexible framework for building a wide range of laboratory-like experiments that can be run online. To use jsPsych, you provide a description of the experiment in the form of a timeline. jsPsych handles things like determining which trial to run next, storing data, and randomization. jsPsych uses plugins to define what to do at each point on the timeline. Plugins are ready-made templates for simple experimental tasks like displaying instructions or displaying a stimulus and collecting a keyboard response. Plugins are very flexible to support a wide variety of experiments. It is easy to create your own plugin if you have experience with JavaScript programming.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Josh de Leeuw&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.jspsych.org">https://www.jspsych.org&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
anonymous&lt;/p>
&lt;hr></description></item><item><title>Open Source Tools for Temporally Controlled Rodent Behavior Suitable for Electrophysiology and Optogenetic Manipulations</title><link>https://open-neuroscience.com/post/open_source_tools_for_temporally_controlled_rodent_behavior_suitable_for_electrophysiology_and_optogenetic_manipulations/</link><pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open_source_tools_for_temporally_controlled_rodent_behavior_suitable_for_electrophysiology_and_optogenetic_manipulations/</guid><description>&lt;p>Understanding how the brain controls behavior requires observing and manipulating neural activity in awake behaving animals. Neuronal firing is timed at millisecond precision. Therefore, to decipher temporal coding, it is necessary to monitor and control animal behavior at the same level of temporal accuracy. However, it is technically challenging to deliver sensory stimuli and reinforcers as well as to read the behavioral responses they elicit with millisecond precision. Presently available commercial systems often excel in specific aspects of behavior control, but they do not provide a customizable environment allowing flexible experimental design while maintaining high standards for temporal control necessary for interpreting neuronal activity. Moreover, delay measurements of stimulus and reinforcement delivery are largely unavailable. We combined microcontroller-based behavior control with a sound delivery system for playing complex acoustic stimuli, fast solenoid valves for precisely timed reinforcement delivery and a custom-built sound attenuated chamber using high-end industrial insulation materials. Together this setup provides a physical environment to train head-fixed animals, enables calibrated sound stimuli and precisely timed fluid and air puff presentation as reinforcers. We provide latency measurements for stimulus and reinforcement delivery and an algorithm to perform such measurements on other behavior control systems. Combined with electrophysiology and optogenetic manipulations, the millisecond timing accuracy will help interpret temporally precise neural signals and behavioral changes. Additionally, since software and hardware provided here can be readily customized to achieve a large variety of paradigms, these solutions enable an unusually flexible design of rodent behavioral experiments.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Nicola Solari; Katalin Sviatkó; Tamás Laszlovszky; Panna Hegedüs; Balázs Hangya&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/hangyabalazs/Rodent_behavior_setup">https://github.com/hangyabalazs/Rodent_behavior_setup&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Balazs Hangya&lt;/p>
&lt;hr></description></item><item><title>OPETH: Open Source Solution for Real-Time Peri-Event Time Histogram Based on Open Ephys</title><link>https://open-neuroscience.com/post/opeth_open_source_solution_for_real_time_peri_event_time_histogram_based_on_open_ephys/</link><pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/opeth_open_source_solution_for_real_time_peri_event_time_histogram_based_on_open_ephys/</guid><description>&lt;p>Single cell electrophysiology remains one of the most widely used approaches of systems neuroscience. Decisions made by the experimenter during electrophysiology recording largely determine recording quality, duration of the project and value of the collected data. Therefore, online feedback aiding these decisions can lower monetary and time investment, and substantially speed up projects as well as allow novel studies otherwise not possible due to prohibitively low throughput. Real-time feedback is especially important in studies that involve optogenetic cell type identification by enabling a systematic search for neurons of interest. However, such tools are scarce and limited to costly commercial systems with high degree of specialization, which hitherto prevented wide-ranging benefits for the community. To address this, we present an open-source tool that enables online feedback during electrophysiology experiments and provides a Python interface for the widely used Open Ephys open source data acquisition system. Specifically, our software allows flexible online visualization of spike alignment to external events, called the online peri-event time histogram (OPETH). These external events, conveyed by digital logic signals, may indicate photostimulation time stamps for in vivo optogenetic cell type identification or the times of behaviorally relevant events during in vivo behavioral neurophysiology experiments. Therefore, OPETH allows real-time identification of genetically defined neuron types or behaviorally responsive populations. By allowing &amp;ldquo;hunting&amp;rdquo; for neurons of interest, OPETH significantly reduces experiment time and thus increases the efficiency of experiments that combine in vivo electrophysiology with behavior or optogenetic tagging of neurons.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>András Széll; Sergio Martínez-Bellver; Panna Hegedüs; Balázs Hangya&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/hangyabalazs/opeth">https://github.com/hangyabalazs/opeth&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Balazs Hangya&lt;/p>
&lt;hr></description></item><item><title>Psygo</title><link>https://open-neuroscience.com/post/psygo/</link><pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/psygo/</guid><description>&lt;p>The easiest way to get started creating online behavioural experiments! psygo is a CLI tool that streamlines the development of custom jsPsych plugins, allowing you to create custom behavioural experiments that can be run online. psygo also helps you test your experiment locally. All the hard work is done for you, from setting up a project, to preparing it for administration.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Henry Burgess&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/henry-burgess/psygo">https://github.com/henry-burgess/psygo&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Henry Burgess&lt;/p>
&lt;hr></description></item><item><title>Computer-controlled dog treat dispenser</title><link>https://open-neuroscience.com/post/computer_controlled_dog_treat_dispenser/</link><pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/computer_controlled_dog_treat_dispenser/</guid><description>&lt;p>When performing canine operant conditioning studies, the delivery of the reward can be a limiting factor of the study. While there are a few commercially available options for automatically delivering rewards, they generally require manual input, such as using a remote control, in accordance with the experiment script. This means that human reaction times and transmission distances can cause interruptions to the flow of the experiment. The potential for development of non-supervised conditioning studies is limited by this same factor. To remedy this, we retrofitted an off-the-shelf treat dispenser with new electronics that allow it to be remotely controllable as well as act as an experiment computation, data storage, and networking center. We present a fully integrated dispenser driver board with a complementary Raspberry Pi. With rather simple modifications, the commercial treat dispenser can be modified into a computer-controlled dispenser for canine cognition experiments or for other forms of canine training or games.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Jeffrey R. Stevens; Walker Arce&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/unl-cchil/canine_treat_dispenser">https://github.com/unl-cchil/canine_treat_dispenser&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=veKvqE5ipu4">https://www.youtube.com/watch?v=veKvqE5ipu4&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Jeffrey R. Stevens&lt;/p>
&lt;hr></description></item><item><title>openEyeTrack - An open source high-speed eyetracker</title><link>https://open-neuroscience.com/post/openeyetrack_an_open_source_high_speed_eyetracker/</link><pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/openeyetrack_an_open_source_high_speed_eyetracker/</guid><description>&lt;p>Vision is one of the primary senses, and tracking eye gaze can offer insight into the cues that affect decision-making behavior. Thus, to study decision-making and other cognitive processes, it is fundamentally necessary to track eye position accurately. However, commercial eye trackers are 1) often very expensive, and 2) incorporate proprietary software to detect the movement of the eye. Closed source solutions limit the researcher’s ability to be fully informed regarding the algorithms used to track the eye and to incorporate modifications tailored to their needs. Here, we present our software solution, openEyeTrack, a low-cost, high-speed, low-latency, open-source video-based eye tracker. Video-based eye trackers can perform nearly as well as classical scleral search coil methods and are suitable for most applications.&lt;/p>
&lt;p>openEyeTrack is a video-based eye-tracker that takes advantage of OpenCV, a low-cost, high-speed infrared camera and GigE-V APIs for Linux provided by Teledyne DALSA, the graphical user interface toolkit QT5 and cvui, the OpenCV based GUI. All of the software components are freely available. The only costs are from the hardware components such as the camera (Genie Nano M640 NIR, Teledyne DALSA, ~$450, ~730 frames per second) and infrared light source, an articulated arm to position the camera (Manfrotto: $130), a computer with one or more gigabit network interface cards, and a power over ethernet switch to power and receive data from the camera.&lt;/p>
&lt;p>By using the GigE-V Framework to capture the frames from the DALSA camera and the OpenCV simple blob detector, openEyeTrack can accurately estimate the position and area of the pupil. We include pupil size calculations because of its putative link to arousal levels and emotions of the subject.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Chandramouli Chandrasekaran; Jorge Paolo Casas&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/chand-lab/openEyeTrack">https://github.com/chand-lab/openEyeTrack&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Chandramouli Chandrasekaran&lt;/p>
&lt;hr></description></item><item><title>Mouse VR</title><link>https://open-neuroscience.com/post/mouse_vr/</link><pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/mouse_vr/</guid><description>&lt;p>Harvey Lab miniaturized mouse VR rig for head-fixed virtual navigation and decision-making tasks.&lt;/p>
&lt;p>The VR setup is comprised of several independent assemblies:&lt;/p>
&lt;p>The screen assembly: a laser projector projects onto a parabolic screen surrounding the mouse. This is the basis for the visual virtual reality.&lt;/p>
&lt;p>Ball cup assembly: an air-supported 8&amp;quot; styrofoam ball that the mouse can run on, with associated ball cup, sensors, and electronics&lt;/p>
&lt;p>Reward delivery system and lick sensor: lick spout, liquid reward reservoir, solenoid, and associated electronics&lt;/p>
&lt;p>Enclosure: A box surrounding the behavioral setup.&lt;/p>
&lt;p>Each of these components is independent of the others: i.e. just the screen could be used in combination with a different treadmill and reward delivery system. The electronics for the ball sensors, reward delivery, and lick detection are all mounted on the same PCB. If only one or two of these functions are needed, you do not need to populate the entire PCB.&lt;/p>
&lt;p>The screen assembly is designed to be small enough to be mounted within a standard 19&amp;quot; server rack, which could easily fit 3 rigs stacked vertically (or two + monitor and keyboard station).&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Noah Pettit; Matthias Minderer; Selmaan Chettih; Charlotte Arlt; Jim Bohnslav; Pavel Gorelick; Ofer Mazor; Christopher Harvey&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/HarveyLab/mouseVR">https://github.com/HarveyLab/mouseVR&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Noah Pettit&lt;/p>
&lt;hr></description></item><item><title>FastTrack</title><link>https://open-neuroscience.com/post/fasttrack/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/fasttrack/</guid><description>&lt;p>FastTrack is an open-source cross-platform tracking software. Easy to install and easy to use, it can track a large variety of systems from active particles to animals, with a known or unknown number of objects. It can process movies from any quality on low-end to high-end computers.&lt;/p>
&lt;p>Two main features are implemented in the software:
- A fast and automatic tracking algorithm that can detect and track objects, conserving the objects' identities across the video recording.
- A manual tool to review the tracking where errors can be corrected rapidly and easily to achieve 100% accuracy with a minimum of efforts.&lt;/p>
&lt;p>FastTrack do not require coding abilities to be used. A developer documentation is available for users who want to embed FastTrack tracking algorithm directly inside their projects.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Benjamin Gallois&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/FastTrackOrg/FastTrack">https://github.com/FastTrackOrg/FastTrack&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="http://www.fasttrack.sh/UserManual/docs/assets/example_vid.webm">http://www.fasttrack.sh/UserManual/docs/assets/example_vid.webm&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Benjamin Gallois&lt;/p>
&lt;hr></description></item><item><title>PiDose</title><link>https://open-neuroscience.com/post/pidose/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pidose/</guid><description>&lt;p>PiDose is an open-source tool for scientists performing drug administration experiments with mice. It allows for automated daily oral dosing of mice over long time periods (weeks to months) without the need for experimenter interaction and handling. To accomplish this, a small 3D-printed chamber is mounted adjacent to a regular mouse home-cage, with an opening in the cage to allow animals to freely access the chamber.&lt;/p>
&lt;p>The chamber is supported by a load cell, and does not contact the cage but sits directly next to the entrance opening. Prior to treatment, mice have a small RFID capsule implanted subcutaneously, and when they enter the chamber they are detected by an RFID reader. While the mouse is in the chamber, readings are taken from the load cell in order to determine the mouse&amp;rsquo;s bodyweight. At the opposite end of the chamber from the entrance, a nose-poke port accesses a spout which dispenses drops from two separate liquid reservoirs. This spout is wired to a capacitive touch sensor controller in order to detect licks, and delivers liquid drops in response to licking.&lt;/p>
&lt;p>Each day, an average weight is calculated for each mouse and a drug dosage is determined based on this. When a mouse licks at the spout it dispenses either regular drinking water or a drop of drug solution depending on if they have received their daily dosage or not. All components are controlled by a Python script running on a Raspberry Pi.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Cameron Woodard; Wissam Nasrallah; Bahram Samiei; Tim Murphy; Lynn Raymond&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://osf.io/rpyfm/">https://osf.io/rpyfm/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Cameron Woodard&lt;/p>
&lt;hr></description></item><item><title>pyControl</title><link>https://open-neuroscience.com/post/pycontrol/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pycontrol/</guid><description>&lt;p>pyControl is a system of open source hardware and software for controlling behavioural experiments, built around the Micropython microcontroller.&lt;/p>
&lt;p>pyControl makes it easy to program complex behavioural tasks using a clean, intuitive, and flexible syntax for specifying tasks as state machines. User created task definition files, written in Python, run directly on the microcontroller, supported by pyControl framework code. This gives users the power and simplicity of Python for specifying task behaviour, while allowing advanced users low-level access to the microcontroller hardware.&lt;/p>
&lt;p>pyControl hardware consists of a breakout board and a set of devices such as nose-pokes, audio boards, LED drivers, rotary encoders and stepper motor controllers that are connected to the breakout board to create behavioural setups.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Thomas Akam&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://pycontrol.readthedocs.io">https://pycontrol.readthedocs.io&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Thomas Akam&lt;/p>
&lt;hr></description></item><item><title>SLEAP</title><link>https://open-neuroscience.com/post/sleap/</link><pubDate>Sun, 26 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/sleap/</guid><description>&lt;p>SLEAP (Social LEAP Estimates Animal Poses) is a multi-animal pose tracker based on deep learning. It is the successor of LEAP (Pereira et al., Nature Methods, 2019) and was designed to deal with the problem of tracking body landmarks of multiple freely interacting animals.&lt;/p>
&lt;p>Using deep learning, SLEAP trains neural network models from few user annotations to enable highly accurate body part localization, grouping and tracking. It supports multiple neural network architectures, including pretrained state-of-the-art models and lightweight customizable architectures. SLEAP has been used successfully to track mice, fruit flies, bees and other species of animals under a variety of experimental and imaging conditions.&lt;/p>
&lt;p>The software was designed to make it easy for users with no experience with deep learning through a fully featured GUI, as well as providing a rich functionality for advanced users seeking to develop a custom solution for their project. Tutorials and guides are available on our website (&lt;a href="https://sleap.ai">https://sleap.ai&lt;/a>) detailing steps for easy installation (Windows/Mac/Linux), labeling a new project, training on the locally or on the cloud, and tracking new data.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Talmo Pereira; Joshua Shaevitz; Mala Murthy&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://sleap.ai">https://sleap.ai&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=zwCf1pGnBUw">https://www.youtube.com/watch?v=zwCf1pGnBUw&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Talmo Pereira&lt;/p>
&lt;hr></description></item><item><title>PiVR</title><link>https://open-neuroscience.com/post/pivr/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pivr/</guid><description>&lt;p>PiVR is a system that allows experimenters to immerse small animals into virtual realities. The system tracks the position of the animal and presents light stimulation according to predefined rules, thus creating a virtual landscape in which the animal can behave. By using optogenetics, we have used PiVR to present fruit fly larvae with virtual olfactory realities, adult fruit flies with a virtual gustatory reality and zebrafish larvae with a virtual light gradient.&lt;/p>
&lt;p>PiVR operates at high temporal resolution (70Hz) with low latencies (&amp;lt;30 milliseconds) while being affordable (&amp;lt;US$500) and easy to build (&amp;lt;6 hours). Through extensive documentation (&lt;a href="http://www.PiVR.org">www.PiVR.org&lt;/a>), this tool was designed to be accessible to a wide public, from high school students to professional researchers studying systems neuroscience in academia.&lt;/p>
&lt;p>The project is open source (BSD-3) and the documented code written in the freely available programming language Python. We hope that PiVR will be adapted by advanced users for their particular needs, for example to create closed-loop experiments involving other sensory modalities (e.g., sound/vibration) through the use of PWM controllable devices. We envision PiVR to be used as the central module when creating virtual realities for a variety of sensory modalities. This ‘PiVR module’ takes care of detecting the animal and presenting the appropriate PWM signal that is then picked up by the PWM controllable device installed by the user, for example to produce a sound whenever an animal enters a pre-defined region.&lt;/p>
&lt;p>In short, PiVR is a powerful and affordable experimental platform allowing experimenters to create a wide array of virtual reality experiments. Our hope is that PiVR will be adapted by several labs to democratize closed-loop experiments and, by standardizing image quality and the animal detection algorithm, increase reproducibility.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>David Tadres; Matthieu Louis&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://www.PiVR.org">http://www.PiVR.org&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=w5tIG6B6FWo">https://www.youtube.com/watch?v=w5tIG6B6FWo&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
David Tadres&lt;/p>
&lt;hr></description></item><item><title>Simple Behavioral Analysis (SimBA)</title><link>https://open-neuroscience.com/post/simple_behavioral_analysis_simba/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/simple_behavioral_analysis_simba/</guid><description>&lt;p>Several excellent computational frameworks exist that enable high-throughput and consistent tracking of freely moving unmarked animals. SimBA introduce and distribute a plug-and play pipeline that enables users to use these pose-estimation approaches in combination with behavioral annotation for the generation of supervised machine-learning behavioral predictive classifiers.&lt;/p>
&lt;p>SimBA was developed for the analysis of complex social behaviors, but includes the flexibility for users to generate predictive classifiers across other behavioral modalities with minimal effort and no specialized computational background.&lt;/p>
&lt;p>SimBA has a variety of extended functions for large scale batch video pre-processing, generating descriptive statistics from movement features, and interactive modules for user-defined regions of interest and visualizing classification probabilities and movement patterns.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Simon Nilsson: Jia Jie Chhong; Sophia Hwang; Nastacia Goodwin; Sam A Golden&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/sgoldenlab/simba">https://github.com/sgoldenlab/simba&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=Frq6mMcaHBc&amp;amp;list=PLi5Vwf0hhy1R6NDQJ3U28MOUJPfl2YWYl&amp;amp;index=2&amp;amp;t=0s">https://www.youtube.com/watch?v=Frq6mMcaHBc&amp;amp;list=PLi5Vwf0hhy1R6NDQJ3U28MOUJPfl2YWYl&amp;amp;index=2&amp;amp;t=0s&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Simon Nilsson&lt;/p>
&lt;hr></description></item><item><title>Stytra</title><link>https://open-neuroscience.com/post/stytra/</link><pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/stytra/</guid><description>&lt;p>Stytra, a flexible, open-source software package, written in Python and designed to cover all the general requirements involved in larval zebrafish behavioral experiments.&lt;/p>
&lt;p>It provides timed stimulus presentation, interfacing with external devices and simultaneous real-time tracking of behavioral parameters such as position, orientation, tail and eye motion in both freely-swimming and head-restrained preparations.&lt;/p>
&lt;p>Stytra logs all recorded quantities, metadata, and code version in standardized formats to allow full provenance tracking, from data acquisition through analysis to publication.&lt;/p>
&lt;p>The package is modular and expandable for different experimental protocols and setups. We also provide complete documentation with examples for extending the package to new stimuli and hardware, as well as a schema and parts list for behavioural setups.&lt;/p>
&lt;p>The software can be used in the context of calcium imaging experiments by interfacing with other acquisition devices.&lt;/p>
&lt;p>Our aims are to enable more laboratories to easily implement behavioral experiments, as well as to provide a platform for sharing stimulus protocols that permits easy reproduction of experiments and straightforward validation.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Vilim Stih; Luigi Petrucco&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/portugueslab/stytra">https://github.com/portugueslab/stytra&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Anonymous&lt;/p>
&lt;hr></description></item><item><title>DeepLabCut</title><link>https://open-neuroscience.com/post/deeplabcut/</link><pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/deeplabcut/</guid><description>&lt;p>DeepLabCut™ is an efficient method for 3D markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results (i.e. you can match human labeling accuracy) with minimal training data (typically 50-200 frames). We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors.&lt;/p>
&lt;p>The package is open source, fast, robust, and can be used to compute 3D pose estimates. Please see the original paper and the latest work below. This package is collaboratively developed by the Mathis Group &amp;amp; Mathis Lab at EPFL/Harvard.&lt;/p>
&lt;p>The code is freely available and easy to install in a few clicks with Anaconda (and pypi). Please see instructions on deeplabcut.org. We also provide a very easy to use GUI interface, and a step-by-step user guide!&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Mackenzie Mathis, Alexander Mathis &amp;amp; contributors&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://deeplabcut.org/">http://deeplabcut.org/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/channel/UC2HEbWpC_1v6i9RnDMy-dfA">https://www.youtube.com/channel/UC2HEbWpC_1v6i9RnDMy-dfA&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Mackenzie Mathis&lt;/p>
&lt;hr></description></item><item><title>Bonsai</title><link>https://open-neuroscience.com/post/bonsai/</link><pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/bonsai/</guid><description>&lt;p>Bonsai is a high-performance, easy to use, and flexible visual programming language for designing closed-loop neuroscience experiments combining physiology and behaviour data.&lt;/p>
&lt;p>Bonsai has allowed scientists with no previous programming experience to quickly develop their own experimental rigs and is also being increasingly used as a platform to integrate new open-source hardware and software from the experimental neuroscience community.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Gonçalo Lopes&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://bonsai-rx.org/">https://bonsai-rx.org/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Gonçalo Lopes&lt;/p>
&lt;hr></description></item><item><title>Ethoscopes</title><link>https://open-neuroscience.com/post/ethoscopes/</link><pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/ethoscopes/</guid><description>&lt;p>Ethoscopes are machines for high-throughput analysis of behavior in Drosophila and other animals.&lt;/p>
&lt;p>Ethoscopes provide a software and hardware solution that is reproducible and easily scalable.&lt;/p>
&lt;p>They perform, in real-time, tracking and profiling of behavior by using a supervised machine learning algorithm, are able to deliver behaviorally triggered stimuli to flies in a feedback-loop mode, and are highly customizable and open source.&lt;/p>
&lt;p>Ethoscopes can be built easily by using 3D printing technology and rely on Raspberry Pi microcomputers and Arduino boards to provide affordable and flexible hardware.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Quentin Geissmann; Luis Garcia; Giorgio Gilestro&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://lab.gilest.ro/ethoscope">http://lab.gilest.ro/ethoscope&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=5oWGBUMJON8&amp;amp;feature=emb_title">https://www.youtube.com/watch?v=5oWGBUMJON8&amp;amp;feature=emb_title&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Giorgio Gilestro&lt;/p>
&lt;hr></description></item><item><title>Bonvision</title><link>https://open-neuroscience.com/post/bonvision/</link><pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/bonvision/</guid><description>&lt;p>BonVision is an open-source closed-loop visual environment generator developed by the Saleem Lab and Solomon Lab at the UCL Institute of Behavioural Neuroscience in collaboration with NeuroGEARS.&lt;/p>
&lt;p>BonVision’s key features include:&lt;/p>
&lt;pre>&lt;code>Naturally closed-loop system based on reactive coding of the Bonsai framework
Handles 2D and 3D stimuli with equal ease
Visual environment generated independent of display configuration
Graphical programming language of the Bonsai framework
Can be used for Augmented Reality, Virtual Reality or 2D visual stimuli
Does not require the observer to be in a fixed position
&lt;/code>&lt;/pre>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Bonvision&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://bonvision.github.io">http://bonvision.github.io&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>DeepLabStream</title><link>https://open-neuroscience.com/post/deeplabstream/</link><pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/deeplabstream/</guid><description>&lt;p>DeepLabStream is a python based multi-purpose tool that enables the realtime tracking of animals and manipulation of experiments. Our toolbox is adapted from the previously published DeepLabCut (Mathis et al., 2018) and expands on its core capabilities. DeepLabStreams core feature is the real-time analysis using any type of camera-based video stream (incl. multiple streams). Building onto that, we designed a full experimental closed-loop toolkit. It enables running experimental protocols that are dependent on a constant stream of bodypart positions and feedback activation of several input/output devices. It&amp;rsquo;s capabilities range from simple region of interest (ROI) based triggers to headdirection or behavior dependent stimulation.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Schwarz Neurocon Lab&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/SchwarzNeuroconLab/DeepLabStream">https://github.com/SchwarzNeuroconLab/DeepLabStream&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>FishCam</title><link>https://open-neuroscience.com/post/fishcam/</link><pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/fishcam/</guid><description>&lt;p>We describe the “FishCam”, a low-cost (500 USD) autonomous camera package to record videos and images underwater. The system is composed of easily accessible components and can be programmed to turn ON and OFF on customizable schedules. Its 8-megapixel camera module is capable of taking 3280 × 2464-pixel images and videos. An optional buzzer circuit inside the pressure housing allows synchronization of the video data from the FishCam with passive acoustic recorders. Ten FishCam deployments were performed along the east coast of Vancouver Island, British Columbia, Canada, from January to December 2019. Field tests demonstrate that the proposed system can record up to 212 h of video data over a period of at least 14 days. The FishCam data collected allowed us to identify fish species and observe species interactions and behaviors. The FishCam is an operational, easily-reproduced and inexpensive camera system that can help expand both the temporal and spatial coverage of underwater observations in ecological research. With its low cost and simple design, it has the potential to be integrated into educational and citizen science projects, and to facilitate learning the basics of electronics and programming.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Xavier Mouy&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.sciencedirect.com/science/article/pii/S2468067220300195">https://www.sciencedirect.com/science/article/pii/S2468067220300195&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Deep Cinac</title><link>https://open-neuroscience.com/post/deep_cinac/</link><pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/deep_cinac/</guid><description>&lt;p>Two-photon calcium imaging is now widely used to infer neuronal dynamics from changes in fluorescence of an indicator. However, state of the art computational tools are not optimized for the reliable detection of fluorescence transients from highly synchronous neurons located in densely packed regions such as the CA1 pyramidal layer of the hippocampus during early postnatal stages of development. Indeed,the latest analytical tools often lack proper benchmark measurements. To meet this challenge, we first developed a graphical user interface allowing for a precise manual detection of all calcium transients from imaged neurons based on the visualization of the calcium imaging movie. Then, we analyzed the movies using a convolutional neural network with an attention process and a bidirectional long-short term memory network. This method is able to reach human performance and offers a better F1 score (harmonic mean of sensitivity and precision) than CaImAn to infer neural activity in the developingCA1 without any user intervention. It also enables automatically identifying activity originating from GABAergic neurons. Overall, DeepCINAC offers a simple, fast and flexible open-source toolbox for processing a wide variety of calcium imaging datasets while providing the tools to evaluate its performance.&lt;/p>
&lt;p>See full text at &lt;a href="https://www.biorxiv.org/content/10.1101/803726v2.full.pdf">https://www.biorxiv.org/content/10.1101/803726v2.full.pdf&lt;/a>&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Julien Denis&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://gitlab.com/cossartlab/deepcinac">https://gitlab.com/cossartlab/deepcinac&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Head-Mounted Mesoscope</title><link>https://open-neuroscience.com/post/head-mounted_mesoscope/</link><pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/head-mounted_mesoscope/</guid><description>&lt;p>The advent of genetically encoded calcium indicators, along with surgical preparations such as thinned skulls or refractive index matched skulls, have enabled mesoscale cortical activity imaging in head-fixed mice. Such imaging studies have revealed complex patterns of coordinated activity across the cortex during spontaneous behaviors, goal-directed behavior, locomotion, motor learning,and perceptual decision making. However, neural activity during unrestrained behavior significantly differs from neural activity in head-fixed animals. Whole-cortex imaging in freely behaving mice will enable the study of neural activity in a larger, more complex repertoire of behaviors not possible in head-fixed animals. Here we present the “Mesoscope,” a wide-field miniaturized, head-mounted fluorescence microscope compatible with transparent polymer skulls recently developed by our group. With afield of view of 8 mm x 10 mm and weighing less than 4 g, the Mesoscope can image most of the mouse dorsal cortex with resolution ranging from 39 to 56μm. Stroboscopic illumination with blue and green LEDs allows fort he measurement of both fluorescence changes due to calcium activity and reflectance signals to capture hemodynamic changes. We have used the Mesoscope to successfully record mesoscale calcium activity across the dorsal cortex during sensory-evoked stimuli, open field behaviors, and social interactions.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Biosensing and Biorobotics Lab&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.biorxiv.org/content/10.1101/2020.05.25.114892v1.full.pdf">https://www.biorxiv.org/content/10.1101/2020.05.25.114892v1.full.pdf&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Open Source Syringe Pump Controller</title><link>https://open-neuroscience.com/post/open_source_syringe_pump_controller/</link><pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open_source_syringe_pump_controller/</guid><description>&lt;p>Syringe pumps are a necessary piece of laboratory equipment that are used for fluid delivery in behavioral neuroscience laboratories. Many experiments provide rodents and primates with fluid rewards such as juice, water, or liquid sucrose. Current commercialized syringe pumps are not customizable and do not have the ability to deliver multiple volumes of fluid based on different inputs to the pump. Additionally, many syringe pumps are expensive and cannot be used in experiments with paired neurophysiological recordings due to electrical noise. We developed an open source syringe pump controller using commonly available parts. The controller adjusts the acceleration and speed of the motor to deliver three different volumes of fluid reward within one common time epoch. This syringe pump controller is cost effective and has been successfully implemented in rodent behavioral experiments with paired neurophysiological recordings in the rat frontal cortex while rats lick for different volumes of liquid sucrose rewards. Our syringe pump controller will enable new experiments to address the potential confound of temporal information in studies of reward signaling by fluid magnitude.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Laubach Lab&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/LaubachLab/OpenSourceSyringePump">https://github.com/LaubachLab/OpenSourceSyringePump&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Automated Operant Conditioning</title><link>https://open-neuroscience.com/post/automated_operant_conditioning/</link><pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/automated_operant_conditioning/</guid><description>&lt;p>Operant conditioning (OC) is a classical paradigm and a standard technique used in experimental psychology in which animals learn to perform an action to achieve a reward. By using this paradigm, it is possible to extract learning curves and measure accurately reaction times (RTs). Both these measurements are proxy of cognitive capabilities and can be used to evaluate the effectiveness of therapeutic interventions in mouse models of disease. Here, we describe a fully 3D printable device that is able to perform OC on freely moving mice, while performing real-time tracking of the animal position. We successfully trained six mice, showing stereotyped learning curves that are highly reproducible across mice and reaching &amp;gt;70% of accuracy after 2 d of conditioning. Different products for OC are commercially available, though most of them do not provide customizable features and are relatively expensive. This data demonstrate that this system is a valuable alternative to available state-of-the-art commercial devices, representing a good balance between performance, cost, and versatility in its use.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Raffaele Mazziotti, Giulia Sagona, Leonardo Lupori, Virginia Martini and Tommaso Pizzorusso&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/raffaelemazziotti/oc_chamber">https://github.com/raffaelemazziotti/oc_chamber&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>VocalMat</title><link>https://open-neuroscience.com/post/vocalmat/</link><pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/vocalmat/</guid><description>&lt;p>Mice emit ultrasonic vocalizations (USV) to transmit socially-relevant information. To detect and classify these USVs, here we describe the development of VocalMat. VocalMat is a software that uses image-processing and differential geometry approaches to detect USVs in audio files, eliminating the need for user-defined parameter tuning. VocalMat also uses computational vision and machine learning methods to classify USVs into distinct categories. In a dataset of &amp;gt;4,000 USVs emitted by mice, VocalMat detected more than &amp;gt;98% of the USVs and accurately classified ≈86% of USVs when considering the most likely label out of 11 different USV types. We then used Diffusion Maps and Manifold Alignment to analyze the probability distribution of USV classification among different experimental groups, providing a robust method to quantify and qualify the vocal repertoire of mice. Thus, VocalMat allows accurate and highly quantitative analysis of USVs, opening the opportunity for detailed and high-throughput analysis of this behavior.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Antonio H. O. Fonseca, Gustavo M. Santana, Sergio Bampi, Marcelo O Dietrich&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.dietrich-lab.org/vocalmat">https://www.dietrich-lab.org/vocalmat&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>Autoreward 2</title><link>https://open-neuroscience.com/post/autoreward2/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/autoreward2/</guid><description>&lt;p>The &lt;strong>motivation&lt;/strong> to start this project arises when we started to include a new behavioral paradigm in the lab, an alternation T-mace with return arms (like the one in Wood e_t al._ 2000). We wanted a clean performance, as well as a clean video record, so we consider necessary to interfere neither with the animal attention (mice, how they are!) nor the camera’s field of view. I decided then to give a try to the new hobby I was getting into, “Do-It-Yourself” (DIY) stuff.&lt;/p>
&lt;p>In my head, it was pictured very simple. At the end of the day, I just needed a) something to detect the animal passing by, b) something to deliver a drop of water and c) something to make it happen in a coordinated way. And that’s what Autoreward2 is, no more, no less.&lt;/p>
&lt;p>Well perhaps it is a bit more. &lt;strong>So far, the project can&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Detect&lt;/strong> when the animal reaches the end of any of the two arms.&lt;/li>
&lt;li>&lt;strong>Deliver&lt;/strong> a small drop of fluid through the corresponding licking port (easy to make it happen in the opposite, if wanted).&lt;/li>
&lt;li>Give visual cues to the experimenter, indicating which arm has been reached.&lt;/li>
&lt;li>Allow to &lt;strong>select&lt;/strong> different modes of working for different working protocols: ‘Waiting for selection’, ‘Habituation’, ‘Training’, ‘Experimental’ and “Filling and cleaning” modes (and is ready to include more!).&lt;/li>
&lt;/ul>
&lt;p>To achieve it, I decided for very &lt;strong>simple approach&lt;/strong>. A couple of cheap infrared emitters are continuously read by an UNO R3 board. Breaking any of the beams triggers the signal to open the corresponding solenoid valve, connected to the fluid tank. That lets the liquid flow by gravity for around 75 milliseconds, resulting in a single drop at the tip of the licking port.&lt;/p>
&lt;div align="center">
&lt;p>&lt;img src="./featured2.jpg" alt="">&lt;/p>
&lt;/div>
&lt;p>There is a delay after each detection, to avoid repetitive delivery if animals don’t leave the area. A couple LEDs mounted in the bare-board (out of animal sight) light up when the process is triggered, one for each side. They also work as indicators for the ‘Waiting for selection’ mode, when they are continuously on, meanwhile no option is choose or the ‘return to waiting mode action’ is pressed.&lt;/p>
&lt;p>&lt;strong>The selection&lt;/strong> is made through a 4×4 membrane keypad. Right now, only options 1 to 4 are programmed, making up to 12 more programs available! When any section is made, the in-built LED blinks the corresponding times and the system is ready to work. At any moment, pressing any key makes the system reset to the waiting mode. As easy as that.&lt;/p>
&lt;p>Everything is &lt;strong>powered&lt;/strong> by a regular 9V wall adapter, giving 3.3V to the LEDs and Infrared detectors, and 9V to the solenoids. Of course, it is possible to use a 9V batterie to power it. To avoid damage coming from the solenoid discharges, the circuit is protected by a couple of diodes at this level.&lt;/p>
&lt;p>And that’s all, &lt;strong>it’s simple&lt;/strong>. The most important thing: it &lt;strong>works&lt;/strong>. The other most important thing: it costs around &lt;strong>80€&lt;/strong>. Here is the to-buy list (or equivalent):&lt;/p>
&lt;ul>
&lt;li>Elegoo UNO R3 (I found them for &lt;strong>10€&lt;/strong>, with USB cable)&lt;/li>
&lt;li>BreadBoard + Acrylic base (&lt;strong>7€&lt;/strong>)&lt;/li>
&lt;li>9V 1A Wall power supply (&lt;strong>9€&lt;/strong>)&lt;/li>
&lt;li>2x InfraRed beams, 5mm (&lt;strong>15€&lt;/strong> both, the 3mm ones are even cheaper)&lt;/li>
&lt;li>2x Mini-Solenoid valves (&lt;strong>10€&lt;/strong> both)&lt;/li>
&lt;li>2x red LEDs&lt;/li>
&lt;li>4x 1 KΩ resistors&lt;/li>
&lt;li>2x TIP120 Transistors&lt;/li>
&lt;li>2x 1N4001 diodes&lt;/li>
&lt;li>Wiring (set of jumpers for less than &lt;strong>10€&lt;/strong>)&lt;/li>
&lt;li>‘Velcro’ to attach the acrylic base where the boards are mounted.&lt;/li>
&lt;li>Plastic tubing and laboratory sample tubes, modified with turning siringe tips to attach/deattach the tubing easily.&lt;/li>
&lt;li>2x or 4x weak magnets to fix the tubes to the walls.&lt;/li>
&lt;/ul>
&lt;p> &lt;/p>
&lt;p>Feel free to access the &lt;a href="https://github.com/jjballesteros/Arduino-AutoReward" target="_blank" rel="noopener">Github&lt;/a> page or the &lt;a href="http://forum.arduino.cc/index.php?topic=476643.0" target="_blank" rel="noopener">Arduino forum post&lt;/a> to obtain the &lt;strong>code&lt;/strong>, check for the circuit &lt;strong>sketch&lt;/strong>, and see some &lt;strong>pictures&lt;/strong>.&lt;/p>
&lt;p>PD: If someone is scandalized by the code, I am getting better on it, it is not my main strength. Please, improve it! Of course, I have in mind many possible upgrades such as a screen, a SD card port, to change the Keypad for a wireless interface (tactile?) … Did someone say smartphone plus Bluetooth? Going fancy, a barcode reader to easily introduce subjects’ data… And here is where I relay in the open-access idea, I offer it and hopefully someone implement any of the ideas. If so, remember to share!&lt;/p>
&lt;p>Jesús J. Ballesteros&lt;/p>
&lt;p>Contact me:&lt;/p>
&lt;p>&lt;a href="https://twitter.com/jjballesterosc" target="_blank" rel="noopener">Twitter&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.researchgate.net/profile/J_J_Ballesteros" target="_blank" rel="noopener">ResearchGate&lt;/a>&lt;/p></description></item><item><title>Skinner Box with RPi+Python</title><link>https://open-neuroscience.com/post/skinnerbox_rpi_python/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/skinnerbox_rpi_python/</guid><description>&lt;p>This project was developed by &lt;a href="http://www.kscottz.com/about/" target="_blank" rel="noopener">Katherine Scott&lt;/a> to be presented at the PyCon 2014. She developed a skinner box for her pet rats using a raspberry pi and some 3D printed parts. The setup contain a food dispenser, a buzzer, levers, a camera to observe the animals and it is hooked in a way that everything can be controlled over the internet!&lt;/p>
&lt;p>You can find the files for 3D parts &lt;a href="http://www.thingiverse.com/thing:296335" target="_blank" rel="noopener">here&lt;/a> and a better description of the project &lt;a href="http://www.kscottz.com/open-skinner-box-pycon-2014/" target="_blank" rel="noopener">here&lt;/a>&lt;/p>
&lt;p> &lt;/p>
&lt;iframe width="790" height="444" src="https://www.youtube.com/embed/grMfIoDgn9M" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe>
&lt;p> &lt;/p></description></item></channel></rss>